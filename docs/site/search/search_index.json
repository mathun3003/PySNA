{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"PySNA Documentation Contents for users: Installation Quick Start User Guide TwitterAPI Utility Functions CLI Tool Contents for maintainers: Implementation Details TwitterAPI TwitterDataFetcher TwitterDataProcessor BaseDataProcessor Utility Functions CLI Functions Software Tests Repository Information Links: Tweepy Docs Twitter API Docs","title":"Home"},{"location":"#pysna-documentation","text":"Contents for users: Installation Quick Start User Guide TwitterAPI Utility Functions CLI Tool Contents for maintainers: Implementation Details TwitterAPI TwitterDataFetcher TwitterDataProcessor BaseDataProcessor Utility Functions CLI Functions Software Tests Repository Information Links: Tweepy Docs Twitter API Docs","title":"PySNA Documentation"},{"location":"maintenance/BaseDataProcessor/","text":"BaseDataProcessor The BaseDataProcessor class contains general operations for data processing. It forms the basis for the TwitterDataProcessor class. The BaseDataProcessor class is used within the TwitterAPI class for processing data that is not specific to any social media platform. Methods exist for calculating descriptive metrics for numeric and datetime values as well as calculating the intersection and difference of sets. This class can also be used to process previously collected data. Initialization If you want to use this class for data processing or other package components, follow the steps below. Import the BaseDataProcessor class from the process module: from pysna.process import BaseDataProcessor # init instance data_processor = BaseDataProcessor() and start invoking a function: sets = [set(1,2,4), set(1,3,5), set(1,8,9)] # calculate intersection of all sets data_processor.intersection(sets) Methods The methods of this class use functions from Numpy and default Python operations. calc_descriptive_metrics Calculates descriptive metrics of a given data set. Returns the given data set with appended statistical metrics. Input data set must be a dictionary containing numeric values. Function: BaseDataProcessor.calc_descriptive_metrics(data: Dict[str | int, Number]) The following metrics are calculated: Max Value Min Value Mean Value Median Standard Deviation Sample Variance Range (Max - Min) Interquartiles Range Mean Absolute Deviation These metrics might help to interpret the data more accurately and saves time to evaluate data manually. All metrics are calculated and appended to a new key 'metrics' in the input dictionary. This implementation enables to enrich input data with statistical metrics without the need to append the metrics to the dictionary after calculation. Source Code import numpy as np def calc_descriptive_metrics(self, data: Dict[str | int, Number]) -> dict: \"\"\"Calculates descriptive metrics of a given data set. Args: data (Dict[str | int, Number]): Data dictionary containing numeric values. Raises: ValueError: If non-numeric values are contained in the data dictionary. Returns: dict: Input data dictionary containing descriptive metrics. Metrics: - Max Value - Min Value - Mean Value - Median - Standard Deviation - Sample Variance - Range (Max - Min) - Interquartiles Range - Mean Absolute Deviation \"\"\" if not any(isinstance(value, Number) for value in data.values()): raise ValueError(\"Only numeric values are allowed.\") # extract numeric values by iterating over data dict with iterable items numerics = list(data.values()) # init empty dict to store descriptive metrics metrics = dict() # calc max metrics[\"max\"] = max(numerics) # calc min metrics[\"min\"] = min(numerics) # calc mean metrics[\"mean\"] = np.array(numerics).mean() # calc median metrics[\"median\"] = np.median(numerics) # calc standard deviation metrics[\"std\"] = np.std(numerics) # calc variance metrics[\"var\"] = np.var(numerics) # calc range metrics[\"range\"] = max(numerics) - min(numerics) # calc interquarile range metrics[\"IQR\"] = np.subtract(*np.percentile(numerics, [75, 25])) # calc absolute mean deviation metrics[\"mad\"] = np.mean(np.absolute(numerics - np.mean(numerics))) # add metrics data[\"metrics\"] = metrics return data calc_datetime_metrics Calculates descriptive metrics on datetime objects. The function takes in a dictionary with datetime values. The function will return the input dictionary with appended metrics. Function: BaseDataProcessor.calc_datetime_metrics(dates: Dict[str, datetime]) The following metrics are calculated: Mean Median Max Min Time Span (in days, seconds, and microseconds) Deviation from mean (in days and seconds). Negative values indicate below average, positive ones above average. Deviation from median (in days and seconds). Negative values indicate below median, positive ones above average. The metrics will help to analyze creation dates of social media accounts or posts. The metrics are choosed based on typical behaviors of social bots as they are often created within a short period. These metrics will help to figure out if it is likely that the investigated account is a social bot. All metrics are calculated and appended to a new key 'metrics' in the input dictionary. This implementation enables to enrich input data with statistical metrics without the need to append the metrics to the dictionary after calculation. Source Code def calc_datetime_metrics(self, dates: Dict[str, datetime]) -> dict(): \"\"\"Calculates descriptive metrics on datetime objects. Args: dates (Dict[str, datetime]): Dictionary containing identifiers as keys and datetime objects as values. Returns: dict: Input dates with added datetime metrics. Metrics: - Mean - Median - Max - Min - Time Span (in days, seconds, and microseconds) - Deviation from mean (in days and seconds). Negative values indicate below average, positive ones above average. - Deviation from median (in days and seconds). Negative values indicate below median, positive ones above average. \"\"\" # use the datetime's timestamp to make them comparable timestamps = [dt.timestamp() for dt in dates.values()] # calc mean of creation dates total_time = sum(timestamps) mean_timestamp = total_time / len(timestamps) # convert mean timestamp back to datetime object with timezone information mean_datetime = datetime.fromtimestamp(mean_timestamp, tz=timezone.utc) # calculate time differences to mean datetime of every creation date time_diffs_mean = {key: {\"days\": (dt - mean_datetime).days, \"seconds\": (dt - mean_datetime).seconds} for key, dt in dates.items()} # find the median of the timestamps median_timestamp = np.median(timestamps) # Convert median timestamp back to datetime object median_datetime = datetime.fromtimestamp(median_timestamp, tz=timezone.utc) # calculate time differences to median timestamp of every creation date time_diffs_median = {key: {\"days\": (dt - median_datetime).days, \"seconds\": (dt - median_datetime).seconds} for key, dt in dates.items()} # calc range of creation dates max_date, min_date = max(dates.values()), min(dates.values()) time_span = max_date - min_date # convert creation dates to isoformat for readability dates = {key: dt.isoformat() for key, dt in dates.items()} # add metrics to output dates[\"metrics\"] = dict() dates[\"metrics\"][\"deviation_from_mean\"] = time_diffs_mean dates[\"metrics\"][\"deviation_from_median\"] = time_diffs_median dates[\"metrics\"][\"time_span\"] = {\"days\": time_span.days, \"seconds\": time_span.seconds, \"microseconds\": time_span.microseconds} dates[\"metrics\"][\"mean\"] = mean_datetime.isoformat() dates[\"metrics\"][\"median\"] = median_datetime.isoformat() dates[\"metrics\"][\"max\"] = max_date.isoformat() dates[\"metrics\"][\"min\"] = min_date.isoformat() return dates intersection Calculates the intersection of multiple sets. This function takes in a list of sets and returns their intersection. Function: BaseDataProcessor.intersection(iterable: List[set]) This function is used, for example, to get the follower IDs of multiple social media accounts. The sets contain the individual follower IDs of the social media accounts. Source Code def intersection(self, iterable: List[set]) -> list: \"\"\"Calculates the intersection of multiple sets. Args: iterable (List[set]): List containing sets. Returns: list: intersection set casted to list. \"\"\" intersection = set.intersection(*map(set, iterable)) return list(intersection) difference Calculates the difference of multiple sets. The function takes in a list of dictionaries containing identifiers (e.g., account IDs) as keys and the sets as values. This function will return for each key in the dictionary the individual difference of each set. Function: BaseDataProcessor.difference(sets: Dict[int | str, set]) This function is used to calculate the difference of followers of the specified social media accounts. In this context, the account IDs are stored as dictionary keys and their follower IDs as values. Source Code def difference(self, sets: Dict[int | str, set]) -> dict: \"\"\"Calculates the difference of multiple sets. Args: sets (Dict[set]): Dictionary containing sets where keys are identifiers. Returns: dict: Individual difference of each set that was provided. \"\"\" # init empty dict to store individual differences for each set differences = dict() for key, values in sets.items(): differences[key] = list(set(values)) for other_key, other_values in sets.items(): if key != other_key: differences[key] = list(set(differences[key]) - set(other_values)) return differences","title":"BaseDataProcessor"},{"location":"maintenance/BaseDataProcessor/#basedataprocessor","text":"The BaseDataProcessor class contains general operations for data processing. It forms the basis for the TwitterDataProcessor class. The BaseDataProcessor class is used within the TwitterAPI class for processing data that is not specific to any social media platform. Methods exist for calculating descriptive metrics for numeric and datetime values as well as calculating the intersection and difference of sets. This class can also be used to process previously collected data.","title":"BaseDataProcessor"},{"location":"maintenance/BaseDataProcessor/#initialization","text":"If you want to use this class for data processing or other package components, follow the steps below. Import the BaseDataProcessor class from the process module: from pysna.process import BaseDataProcessor # init instance data_processor = BaseDataProcessor() and start invoking a function: sets = [set(1,2,4), set(1,3,5), set(1,8,9)] # calculate intersection of all sets data_processor.intersection(sets)","title":"Initialization"},{"location":"maintenance/BaseDataProcessor/#methods","text":"The methods of this class use functions from Numpy and default Python operations.","title":"Methods"},{"location":"maintenance/BaseDataProcessor/#calc_descriptive_metrics","text":"Calculates descriptive metrics of a given data set. Returns the given data set with appended statistical metrics. Input data set must be a dictionary containing numeric values. Function: BaseDataProcessor.calc_descriptive_metrics(data: Dict[str | int, Number]) The following metrics are calculated: Max Value Min Value Mean Value Median Standard Deviation Sample Variance Range (Max - Min) Interquartiles Range Mean Absolute Deviation These metrics might help to interpret the data more accurately and saves time to evaluate data manually. All metrics are calculated and appended to a new key 'metrics' in the input dictionary. This implementation enables to enrich input data with statistical metrics without the need to append the metrics to the dictionary after calculation. Source Code import numpy as np def calc_descriptive_metrics(self, data: Dict[str | int, Number]) -> dict: \"\"\"Calculates descriptive metrics of a given data set. Args: data (Dict[str | int, Number]): Data dictionary containing numeric values. Raises: ValueError: If non-numeric values are contained in the data dictionary. Returns: dict: Input data dictionary containing descriptive metrics. Metrics: - Max Value - Min Value - Mean Value - Median - Standard Deviation - Sample Variance - Range (Max - Min) - Interquartiles Range - Mean Absolute Deviation \"\"\" if not any(isinstance(value, Number) for value in data.values()): raise ValueError(\"Only numeric values are allowed.\") # extract numeric values by iterating over data dict with iterable items numerics = list(data.values()) # init empty dict to store descriptive metrics metrics = dict() # calc max metrics[\"max\"] = max(numerics) # calc min metrics[\"min\"] = min(numerics) # calc mean metrics[\"mean\"] = np.array(numerics).mean() # calc median metrics[\"median\"] = np.median(numerics) # calc standard deviation metrics[\"std\"] = np.std(numerics) # calc variance metrics[\"var\"] = np.var(numerics) # calc range metrics[\"range\"] = max(numerics) - min(numerics) # calc interquarile range metrics[\"IQR\"] = np.subtract(*np.percentile(numerics, [75, 25])) # calc absolute mean deviation metrics[\"mad\"] = np.mean(np.absolute(numerics - np.mean(numerics))) # add metrics data[\"metrics\"] = metrics return data","title":"calc_descriptive_metrics"},{"location":"maintenance/BaseDataProcessor/#calc_datetime_metrics","text":"Calculates descriptive metrics on datetime objects. The function takes in a dictionary with datetime values. The function will return the input dictionary with appended metrics. Function: BaseDataProcessor.calc_datetime_metrics(dates: Dict[str, datetime]) The following metrics are calculated: Mean Median Max Min Time Span (in days, seconds, and microseconds) Deviation from mean (in days and seconds). Negative values indicate below average, positive ones above average. Deviation from median (in days and seconds). Negative values indicate below median, positive ones above average. The metrics will help to analyze creation dates of social media accounts or posts. The metrics are choosed based on typical behaviors of social bots as they are often created within a short period. These metrics will help to figure out if it is likely that the investigated account is a social bot. All metrics are calculated and appended to a new key 'metrics' in the input dictionary. This implementation enables to enrich input data with statistical metrics without the need to append the metrics to the dictionary after calculation. Source Code def calc_datetime_metrics(self, dates: Dict[str, datetime]) -> dict(): \"\"\"Calculates descriptive metrics on datetime objects. Args: dates (Dict[str, datetime]): Dictionary containing identifiers as keys and datetime objects as values. Returns: dict: Input dates with added datetime metrics. Metrics: - Mean - Median - Max - Min - Time Span (in days, seconds, and microseconds) - Deviation from mean (in days and seconds). Negative values indicate below average, positive ones above average. - Deviation from median (in days and seconds). Negative values indicate below median, positive ones above average. \"\"\" # use the datetime's timestamp to make them comparable timestamps = [dt.timestamp() for dt in dates.values()] # calc mean of creation dates total_time = sum(timestamps) mean_timestamp = total_time / len(timestamps) # convert mean timestamp back to datetime object with timezone information mean_datetime = datetime.fromtimestamp(mean_timestamp, tz=timezone.utc) # calculate time differences to mean datetime of every creation date time_diffs_mean = {key: {\"days\": (dt - mean_datetime).days, \"seconds\": (dt - mean_datetime).seconds} for key, dt in dates.items()} # find the median of the timestamps median_timestamp = np.median(timestamps) # Convert median timestamp back to datetime object median_datetime = datetime.fromtimestamp(median_timestamp, tz=timezone.utc) # calculate time differences to median timestamp of every creation date time_diffs_median = {key: {\"days\": (dt - median_datetime).days, \"seconds\": (dt - median_datetime).seconds} for key, dt in dates.items()} # calc range of creation dates max_date, min_date = max(dates.values()), min(dates.values()) time_span = max_date - min_date # convert creation dates to isoformat for readability dates = {key: dt.isoformat() for key, dt in dates.items()} # add metrics to output dates[\"metrics\"] = dict() dates[\"metrics\"][\"deviation_from_mean\"] = time_diffs_mean dates[\"metrics\"][\"deviation_from_median\"] = time_diffs_median dates[\"metrics\"][\"time_span\"] = {\"days\": time_span.days, \"seconds\": time_span.seconds, \"microseconds\": time_span.microseconds} dates[\"metrics\"][\"mean\"] = mean_datetime.isoformat() dates[\"metrics\"][\"median\"] = median_datetime.isoformat() dates[\"metrics\"][\"max\"] = max_date.isoformat() dates[\"metrics\"][\"min\"] = min_date.isoformat() return dates","title":"calc_datetime_metrics"},{"location":"maintenance/BaseDataProcessor/#intersection","text":"Calculates the intersection of multiple sets. This function takes in a list of sets and returns their intersection. Function: BaseDataProcessor.intersection(iterable: List[set]) This function is used, for example, to get the follower IDs of multiple social media accounts. The sets contain the individual follower IDs of the social media accounts. Source Code def intersection(self, iterable: List[set]) -> list: \"\"\"Calculates the intersection of multiple sets. Args: iterable (List[set]): List containing sets. Returns: list: intersection set casted to list. \"\"\" intersection = set.intersection(*map(set, iterable)) return list(intersection)","title":"intersection"},{"location":"maintenance/BaseDataProcessor/#difference","text":"Calculates the difference of multiple sets. The function takes in a list of dictionaries containing identifiers (e.g., account IDs) as keys and the sets as values. This function will return for each key in the dictionary the individual difference of each set. Function: BaseDataProcessor.difference(sets: Dict[int | str, set]) This function is used to calculate the difference of followers of the specified social media accounts. In this context, the account IDs are stored as dictionary keys and their follower IDs as values. Source Code def difference(self, sets: Dict[int | str, set]) -> dict: \"\"\"Calculates the difference of multiple sets. Args: sets (Dict[set]): Dictionary containing sets where keys are identifiers. Returns: dict: Individual difference of each set that was provided. \"\"\" # init empty dict to store individual differences for each set differences = dict() for key, values in sets.items(): differences[key] = list(set(values)) for other_key, other_values in sets.items(): if key != other_key: differences[key] = list(set(differences[key]) - set(other_values)) return differences","title":"difference"},{"location":"maintenance/TwitterAPI/","text":"TwitterAPI The TwitterAPI class forms the main class for user interaction and serves as a simple interface through encapsulation since the user can use all the functionalities of the package via the four main functions. All functionalities (except the utility function from the utils module) can be used via this class. The TwitterAPI class combines compatible data processing mechanisms with the respective data queries. This is achieved through composition whereby instances of the TwitterDataFetcher and TwitterDataProcessor are created within the class\u2019s constructor. Data processing mechanisms are outsourced to the functions of the TwitterDataProcessor instance whereas the data querying mechanisms are outsourced to the TwitterDataFetcher instance. This design decision ensures hiding implementation details from users or developers during usage or implementation and makes the code more structured. Thus, whenever any developer wants to make adjustments/fixes to the code, he or she has not to be aware of the code from the TwitterDataProcessor and TwitterDataFetcher instances. Contributing developers are able to understand this class's implementation by understanding the seperation of classes' concerns. This class serves the purpose of handling the input and output of user interactions through an interface. No further data processing or fetching is performed by this class. This class is built on top of the tweepy.Client class. The TwitterAPI class inherits the tweepy.Client class. Thus, every function from the inherited class will also be available in the TwitterAPI class, resulting in an package that extends the official Twitter API. The default behavior of the tweepy.Client class is overwritten but retained in order to extend this class. See the package architecture on the overview section of this chapter . Initialization Import the TwitterAPI class from the api module or use the shortcut. from pysna.api import TwitterAPI # full path from pysna import TwitterAPI # shortcut api = TwitterAPI( bearer_token: Any | None = None, consumer_key: Any | None = None, consumer_secret: Any | None = None, access_token: Any | None = None, access_token_secret: Any | None = None, x_rapidapi_key: Any | None = None, x_rapidapi_host: Any | None = None, wait_on_rate_limit: bool = True ) and invoke a function: user_id = 123450897612 api.user_info(user_id, [\"screen_name\", \"followers_count\"]) Find the necessary secrets on the user guide instructions . Methods All functions have pre-defined input parameters (either for the attributes or compare arguments). They are stored inside the literal objects: TwitterAPI.LITERALS_USER_INFO : available attributes for a Twitter account. For more information, see here . TwitterAPI.LITERALS_COMPARE_USERS : available comparison attributes for Twitter accounts. For more information, see here . TwitterAPI.LITERALS_TWEET_INFO : available attributes for a tweet. For more information, see here . TwitterAPI.LITERALS_COMPARE_TWEETS : available comparison attributes for tweets. For more information, see here . Each (comparison) attribute is covered in the corresponding main function by iterations of a for loop. Whenever one of the four main functions is called, the required data resources are fetched by making API calls via the TwitterDataFetcher class for each specified (comparison) attribute and are processed by the TwitterDataProcessor class if necessary. For instance, when comparing multiple users on their common followers, individual followers for each user are fetched first and then processed by calculating the intersection set of all individual followers. If any new functionality is added to this class (e.g., get the length of a tweet by a length attribute), the new attribute has to be added to the corresponding literal object. In addition, the comparison functions also have a features argument which is used to define a feature vector for comparison. The available similarity features for users and tweets are also specified in separate literal objects: TwitterAPI.SIMILARITY_FEATURES_COMPARE_USERS : available similarity features for the TwitterAPI.compare_users function. For more information, see here . TwitterAPI.SIMILARITY_FEATURES_COMPARE_TWEETS : available similarity features for the TwitterAPI.compare_tweets function. For more information, see here . handle_output (private) This function is designed to handle the output of the four main functions accordingly. To avoid accessing a returned dictionary with only one available key, this function returns either the single value from the one-key dictionary or the full dictionary itself if multiple keys are available. Function: TwitterAPI._handle_output(output: dict) This function is private and, thus, not intended for external use. This function was designed to facilitate the use of the package without the need for accessing a returned one-key dictionary. user_info This function allows to request user information from a Twitter account. Function: TwitterAPI.user_info(user: str | int, attributes: List[LITERALS_USER_INFO] | str, return_timestamp: bool = False) Args: user (str | int): Twitter User either specified by corresponding ID or screen name. attributes (List[str] | str): Attributes of the User object. These must be from this list . return_timestamp (bool, optional): Add UTC Timestamp to results. Defaults to False. This function takes in a Twitter user identifier (i.e., an ID or unique screen name). The attributes are passed in by a list object or by a single string. For a single provided attribute, only the corresponding value is returned. For multiple attributes, a dictionary with the key-value pairs of the requested attributes is returned. If the requested attribute for the objet is not available, None will be returned. Source Code def user_info(self, user: str | int, attributes: List[LITERALS_USER_INFO] | str, return_timestamp: bool = False) -> Any: \"\"\"Receive requested user information from Twitter User Object. For one attribute, only the corresponding value is returned. For multiple attributes, a dictionary with the key-value pairs of the requested attributes is returned. Args: user (str | int): Twitter User either specified by corresponding ID or screen name. attributes (List[str] | str): Attributes of the User object. These must be from: id, id_str, name, screen_name, followers, followees, location, description, url, entities, protected, followers_count, friends_count, listed_count, created_at, latest_activity, last_active, liked_tweets, composed_tweets, favourites_count, verified, statuses_count, status, contributors_enabled, profile_image_url_https, profile_banner_url, default_profile, default_profile_image, withheld_in_countries, bot_scores return_timestamp (bool, optional): Add UTC Timestamp to results. Defaults to False. Raises: KeyError: If invalid attribute was provided. ValueError: If Botometer secrets were not provided. Returns: dict: Requested user information. References: https://mathun3003.github.io/PySNA/user-guide/overview/TwitterAPI/#user_info \"\"\" # catch Botometer API secrets before iteration over attributes. if \"bot_scores\" in attributes: if (self._x_rapidapi_key is None) or (self._x_rapidapi_host is None): raise ValueError(\"'X_RAPIDAPI_KEY' and 'X_RAPIDAPI_HOST' secrets for Botometer API need to be provided.\") # initialize empty dict to store requested attributes user_info = dict() # if single string was provided if isinstance(attributes, str): # convert to list for iteration attributes = [attributes] # get user object user_obj = self.fetcher.get_user_object(user) # loop through the list of attributes and add them to the dictionary for attr in attributes: # if invalid attribute was provided if attr not in get_args(self.LITERALS_USER_INFO): raise ValueError(\"Invalid attribute for '{}'\".format(attr)) # if the desired attribute is in default user object returned by the v1 Search API elif attr in user_obj._json.keys(): user_info[attr] = user_obj._json[attr] # get information about user's followers elif attr == \"followers\": user_info[attr] = self.data_processor.extract_followers(user_obj) # get information about user's followees elif attr == \"followees\": user_info[attr] = self.data_processor.extract_followees(user_obj) # get all liked tweets of user elif attr == \"liked_tweets\": # get page results first liked_tweets = self.fetcher.get_liked_tweets_ids(user) user_info[attr] = liked_tweets # get all composed tweets elif attr == \"composed_tweets\": # get page results first composed_tweets = self.fetcher.get_composed_tweets_ids(user) user_info[attr] = composed_tweets # get user's latest activity elif attr == \"latest_activity\": user_info[attr] = self.fetcher.get_latest_activity(user) # get user's latest activity date elif attr == \"last_active\": user_info[attr] = self.fetcher.get_latest_activity_date(user) # get user's botometer scores elif attr == \"bot_scores\": user_info[attr] = self.fetcher.get_botometer_scores(user) # if attribute was not found else: user_info[attr] = None # if timestamp should be returned if return_timestamp: user_info[\"utc_timestamp\"] = strf_datetime(datetime.utcnow(), format=\"%Y-%m-%d %H:%M:%S.%f\") return self._handle_output(user_info) compare_users This function allows a comparison of multiple Twitter accounts. Function: TwitterAPI.compare_users(users: List[str | int], compare: str | List[LITERALS_COMPARE_USERS], return_timestamp: bool = False, features: List[str] | None = None) Args: users (List[str | int]): User IDs or screen names compare (str): Comparison attribute. Must be from this list . return_timestamp (bool, optional): Add UTC Timestamp to results. Defaults to False. features (List[str] | None, optional): Defined features of Twitter User Object on which similarity will be computed. Must be from the features list . Defaults to None. This function takes in multiple Twitter user identifiers (i.e., IDs or unique screen names). The comparison attributes are passed in by a list object or by a single string. For a single attribute, only the corresponding value is returned. For multiple attributes, a dictionary with the key-value pairs of the requested attributes is returned. Source Code def compare_users(self, users: List[str | int], compare: str | List[LITERALS_COMPARE_USERS], return_timestamp: bool = False, features: List[str] | None = None) -> Any: \"\"\"Compare two or more users with the specified comparison attribute(s). For one attribute, only the corresponding value is returned. For multiple attributes, a dictionary with the key-value pairs of the requested attributes is returned. Args: users (List[str | int]): User IDs or screen names compare (str): Comparison attribute. Must be from: relationship, followers_count, followees_count, tweets_count, favourites_count, common_followers, distinct_followers, common_followees, distinct_followees, commonly_liked_tweets, distinctly_liked_tweets, similarity, created_at, protected, verified. return_timestamp (bool, optional): Add UTC Timestamp to results. Defaults to False. features (List[str] | None, optional): Defined features of Twitter User Object on which similarity will be computed. Must be from: followers_count, friends_count, listed_count, favourites_count, statuses_count. Defaults to None. Raises: ValueError: If invalid comparison attribute was provided. Returns: dict | list: Results of requested comparison attribute(s). Referencs: https://mathun3003.github.io/PySNA/user-guide/overview/TwitterAPI/#compare_users \"\"\" # users list must contain at least two elements assert len(users) > 1, \"'users' list must contain at least two elements, {} was/were provided\".format(len(users)) # catch if feature vector contains only numeric values, and contains at least two elements if features: assert len(features) > 1, \"'features' list must have at least two elements. {} was/were given\".format(len(features)) for feat in features: if feat not in get_args(self.SIMILARITY_FEATURES_COMPARE_USERS): raise ValueError(f\"Only numeric features are supported. Must be from: {', '.join(get_args(self.SIMILARITY_FEATURES_COMPARE_USERS))}. You passed in {feat}\") # if single comparison attribute was provided as string if isinstance(compare, str): # change to list object compare = [compare] # init empty dict to store results results = dict() # iterate over comparison attributes for attr in compare: # if invalid attribute was provided if attr not in get_args(self.LITERALS_COMPARE_USERS): raise ValueError(\"Invalid attribute for '{}'\".format(attr)) # match comparison attributes match attr: # compare relationships between two users case \"relationship\": results[attr] = self.fetcher.get_relationship_pairs(users) # compare number of followers case \"followers_count\": # get individual followers followers = {user: self.fetcher.get_user_object(user).followers_count for user in users} # add descriptive metrics followers_with_metrics = self.data_processor.calc_descriptive_metrics(followers) results[attr] = followers_with_metrics # compare number of friends case \"followees_count\": # get individual followees followees = {user: self.fetcher.get_user_object(user).friends_count for user in users} # add descriptive metrics followees = self.data_processor.calc_descriptive_metrics(followees) results[attr] = followees # compare number of Tweets issued by each user case \"tweets_count\": # get individual statuses counts tweets = {user: self.fetcher.get_user_object(user).statuses_count for user in users} # add descriptive metrics tweets = self.data_processor.calc_descriptive_metrics(tweets) results[attr] = tweets # compare number of likes issued by each user case \"favourites_count\": # get individual likes likes = {user: self.fetcher.get_user_object(user).favourites_count for user in users} # add descriptive metrics likes = self.data_processor.calc_descriptive_metrics(likes) results[attr] = likes # compare protected attribute of users case \"protected\": results[attr] = {user: self.fetcher.get_user_object(user).protected for user in users} # compare verified attribute for users case \"verified\": results[attr] = {user: self.fetcher.get_user_object(user).verified for user in users} # get common followers case \"common_followers\": # get individual followers first individual_followers = [self.fetcher.get_user_follower_ids(user) for user in users] # get common followers by calculating the intersection common_followers = self.data_processor.intersection(individual_followers) results[attr] = common_followers # get distinct followers case \"distinct_followers\": # get individual followers first individual_followers = {user: self.fetcher.get_user_follower_ids(user) for user in users} # get distinct followers by calculating the difference of each set distinct_followers = self.data_processor.difference(individual_followers) results[attr] = distinct_followers # get common followees case \"common_followees\": # get individual followees first individual_followees = [self.fetcher.get_user_followee_ids(user) for user in users] # get common followees by calculating the intersection common_followees = self.data_processor.intersection(individual_followees) results[attr] = common_followees # get distinct followees case \"distinct_followees\": # get individual followees first individual_followees = {user: self.fetcher.get_user_followee_ids(user) for user in users} # get distinct followees by calculating the difference of each set distinct_followees = self.data_processor.difference(individual_followees) results[attr] = distinct_followees # get common liked tweets case \"commonly_liked_tweets\": # get individual liked tweets first individual_likes = [self.fetcher.get_liked_tweets_ids(user) for user in users] # get common liked tweets by calculating the intersection common_likes = self.data_processor.intersection(individual_likes) results[attr] = common_likes # get distinct liked tweets case \"distinctly_liked_tweets\": # get individual liked tweets first individual_likes = {user: self.fetcher.get_liked_tweets_ids(user) for user in users} # get distinct liked tweets by calculating the difference for each set distinct_likes = self.data_processor.difference(individual_likes) results[attr] = distinct_likes # compute similarity between two users basd on the defined features case \"similarity\": # feature list object must be defined if features is None: raise ValueError(\"'features' list must be provided.\") # get serialized user objects first user_objs = [self.fetcher.get_user_object(user)._json for user in users] # calculate similarity based on defined feature vector results[attr] = self.data_processor.calc_similarity(user_objs=user_objs, features=features) # compare creaation dates case \"created_at\": # get individual creation dates first creation_dates = {user: self.fetcher.get_user_object(user).created_at for user in users} # add datetime metrics creation_dates = self.data_processor.calc_datetime_metrics(creation_dates) results[attr] = creation_dates # if comparison attribute was not found case _: results[attr] = None # if timestamp should be returned if return_timestamp: results[\"utc_timestamp\"] = strf_datetime(datetime.utcnow(), format=\"%Y-%m-%d %H:%M:%S.%f\") return self._handle_output(results) tweet_info This function allows to request tweet information from a tweet object. Function: TwitterAPI.tweet_info(tweet_id: str | int, attributes: List[LITERALS_TWEET_INFO] | str, return_timestamp: bool = False) Args: tweet_id (str | int): Tweet ID attributes (List[LITERALS_TWEET_INFO] | str): Attributes of the Tweet object. These must be from this list . return_timestamp (bool, optional): Add UTC Timestamp to results. Defaults to False. This function takes in a tweet ID as string or integer representation. The attributes are passed in by a list object or by a single string. For a single provided attribute, only the corresponding value is returned. For multiple attributes, a dictionary with the key-value pairs of the requested attributes is returned. If the requested attribute for the objet is not available, None will be returned. Source Code def tweet_info(self, tweet_id: str | int, attributes: List[LITERALS_TWEET_INFO] | str, return_timestamp: bool = False) -> Any: \"\"\"Receive requested Tweet information from Tweet Object. For one attribute, only the corresponding value is returned. For multiple attributes, a dictionary with the key-value pairs of the requested attributes is returned. Args: tweet_id (str | int): Tweet ID attributes (List[LITERALS_TWEET_INFO] | str): Attributes of the Tweet object. These must be from: id, id_str, full_text, display_text_range, truncated, created_at, entities, tweet_annotations, source, retweeters, in_reply_to_status_id, in_reply_to_status_id_str, in_reply_to_user_id, in_reply_to_user_id_str, in_reply_to_screen_name, user, contributors, coordinates, place, is_quote_status, public_metrics, quoting_users, liking_users, favorited, retweeted, retweeted_status, possibly_sensitive, lang, sentiment. return_timestamp (bool, optional): Add UTC Timestamp to results. Defaults to False. Raises: ValueError: If invalid attribute was provided. Returns: dict: Requested Tweet information. References: https://mathun3003.github.io/PySNA/user-guide/overview/TwitterAPI/#tweet_info \"\"\" # get tweet object tweet_obj = self.fetcher.get_tweet_object(tweet_id) # initialize empty dict to store request information tweet_info = dict() # if single string was provided if isinstance(attributes, str): # convert to list for iteration attributes = [attributes] for attr in attributes: # if invalid attribute was provided if attr not in get_args(self.LITERALS_TWEET_INFO): raise ValueError(\"Invalid attribute for '{}'\".format(attr)) # get default attributes from tweepy Status model elif attr in tweet_obj._json.keys(): tweet_info[attr] = tweet_obj._json[attr] # get all quoting users elif attr == \"quoting_users\": quoting_users = self.fetcher.get_quoting_users_ids(tweet_id) tweet_info[attr] = quoting_users # get all liking users elif attr == \"liking_users\": liking_users = self.fetcher.get_liking_users_ids(tweet_id) tweet_info[attr] = liking_users # get all retweeters elif attr == \"retweeters\": retweeters = self.fetcher.get_retweeters_ids(tweet_id) tweet_info[attr] = retweeters # get public metrics elif attr == \"public_metrics\": tweet_info[attr] = self.fetcher.get_public_metrics(tweet_id) # get context annotations elif attr == \"tweet_annotations\": tweet_info[attr] = self.fetcher.get_context_annotations_and_entities(tweet_id) # get tweet sentiment elif attr == \"sentiment\": tweet_info[attr] = self.data_processor.detect_tweet_sentiment(tweet_obj.full_text) # if attribute was not found else: tweet_info[attr] = None # if timestamp should be returned if return_timestamp: tweet_info[\"utc_timestamp\"] = strf_datetime(datetime.utcnow(), format=\"%Y-%m-%d %H:%M:%S.%f\") return self._handle_output(tweet_info) compare_tweets This function allows a comparison of multiple tweets. Function: TwitterAPI.compare_tweets(tweet_ids: List[str | int], compare: str | List[LITERALS_COMPARE_TWEETS], return_timestamp: bool = False, features: List[str] | None = None) Args: tweets (List[str | int]): List of Tweet IDs. compare (str | List[LITERALS_COMPARE_TWEETS]): Comparison attribute. Needs to be from the this list . return_timestamp (bool, optional): Add UTC Timestamp to results. Defaults to False. features (List[str] | None, optional): Defined features of Twitter User Object on which similarity will be computed. Must be from the features list . Defaults to None. This function takes in multiple tweet IDs as string or integer representation. The comparison attributes are passed in by a list object or by a single string. For a single attribute, only the corresponding value is returned. For multiple attributes, a dictionary with the key-value pairs of the requested attributes is returned. Source Code def compare_tweets(self, tweet_ids: List[str | int], compare: str | List[LITERALS_COMPARE_TWEETS], return_timestamp: bool = False, features: List[str] | None = None) -> Any: \"\"\"Compare two or more Tweets with the specified comparison attribute. For one attribute, only the corresponding value is returned. For multiple attributes, a dictionary with the key-value pairs of the requested attributes is returned. Args: tweets (List[str | int]): List of Tweet IDs. compare (str | List[LITERALS_COMPARE_TWEETS]): Comparison attribute. Needs to be from the following: view_count, like_count, retweet_count, quote_count, reply_count, common_quoting_users, distinct_quoting_users, common_liking_users, distinct_liking_users, common_retweeters, distinct_retweeters, similarity, created_at. return_timestamp (bool, optional): Add UTC Timestamp to results. Defaults to False. features (List[str] | None, optional): Defined features of Twitter User Object on which similarity will be computed. Must be from: retweet_count, reply_count, like_count, quote_count, impression_count. Defaults to None. Raises: AssertionError: If a list of one Tweet ID was provided. ValueError: If invalid comparison attribute was provided. Returns: dict: Requested results for comparison attribute. References: https://mathun3003.github.io/PySNA/user-guide/overview/TwitterAPI/#compare_tweets \"\"\" # tweets list must contain at least two IDs assert len(tweet_ids) > 1, \"'tweets' list object needs at least two entries, not {}\".format(len(tweet_ids)) # catch if feature vector contains only numeric values, and contains at least two elements if features: assert len(features) > 1, \"'features' list must have at least two elements. {} was/were given\".format(len(features)) for feat in features: if feat not in get_args(self.SIMILARITY_FEATURES_COMPARE_TWEETS): raise ValueError(f\"Only numeric features are supported. Must be from: {', '.join(get_args(self.SIMILARITY_FEATURES_COMPARE_TWEETS))}. You passed in {feat}.\") # if single comparison attribute was provided as string if isinstance(compare, str): # change to list object compare = [compare] # init empty dict to store results results = dict() # iterate over every given comparison atttribute for attr in compare: # if invalid attribute was provided if attr not in get_args(self.LITERALS_COMPARE_TWEETS): raise ValueError(\"Invalid attribute for '{}'\".format(attr)) # match comparison attribute match attr: # compare numer of views / impressions case \"view_count\": # get individual view_counts view_counts = {tweet_id: self.fetcher.get_public_metrics(tweet_id)[\"impression_count\"] for tweet_id in tweet_ids} # add descriptive metrics view_counts = self.data_processor.calc_descriptive_metrics(view_counts) results[attr] = view_counts # compare number of likes case \"like_count\": # get individual like_counts like_counts = {tweet_id: self.fetcher.get_public_metrics(tweet_id)[\"like_count\"] for tweet_id in tweet_ids} # add descriptive metrics like_counts = self.data_processor.calc_descriptive_metrics(like_counts) results[attr] = like_counts # compare number or retweets case \"retweet_count\": # get individual number of retweets retweet_counts = {tweet_id: self.fetcher.get_public_metrics(tweet_id)[\"retweet_count\"] for tweet_id in tweet_ids} # add descriptive metrics retweet_counts = self.data_processor.calc_descriptive_metrics(retweet_counts) results[attr] = retweet_counts # compare number of quotes case \"quote_count\": # get individual number of quotes quote_counts = {tweet_id: self.fetcher.get_public_metrics(tweet_id)[\"quote_count\"] for tweet_id in tweet_ids} # add descriptive metrics quote_counts = self.data_processor.calc_descriptive_metrics(quote_counts) results[attr] = quote_counts # compare number of commonts case \"reply_count\": # get individual number of replies first reply_counts = {tweet_id: self.fetcher.get_public_metrics(tweet_id)[\"reply_count\"] for tweet_id in tweet_ids} # add descriptive metrics reply_counts = self.data_processor.calc_descriptive_metrics(reply_counts) results[attr] = reply_counts # get all quoting users all Tweets have in common case \"common_quoting_users\": # get individual quoting users first quoting_users = [self.fetcher.get_quoting_users_ids(tweet_id) for tweet_id in tweet_ids] # get common quoting users by calculating the intersection common_quoting_users = self.data_processor.intersection(quoting_users) # return quoting users results[attr] = common_quoting_users # get distinct quoting users for each tweet case \"distinct_quoting_users\": # get individual quoting users first quoting_users = {tweet_id: self.fetcher.get_quoting_users_ids(tweet_id) for tweet_id in tweet_ids} # get distinct quoting users for each tweet by calculating the difference for each set distinct_quoting_users = self.data_processor.difference(quoting_users) results[attr] = distinct_quoting_users # get all liking users that all tweets have in common case \"common_liking_users\": # get individual liking users first liking_users = [self.fetcher.get_liking_users_ids(tweet_id) for tweet_id in tweet_ids] # get common liking users by calculating the intersection common_liking_users = self.data_processor.intersection(liking_users) # return common liking users results[attr] = common_liking_users # get distinct liking users of all tweets case \"distinct_liking_users\": # get individual liking users first liking_users = {tweet_id: self.fetcher.get_liking_users_ids(tweet_id) for tweet_id in tweet_ids} # get distinct liking users for each tweet by calculating the difference for each set distinct_liking_users = self.data_processor.difference(liking_users) results[attr] = distinct_liking_users # get all retweeters all tweets have in common case \"common_retweeters\": # get individual retweeters first retweeters = [self.fetcher.get_retweeters_ids(tweet_id) for tweet_id in tweet_ids] # get common retweeters by calculating the intersection common_retweeters = self.data_processor.intersection(retweeters) # return common retweeters results[attr] = common_retweeters # get distinct retweeters of all tweets case \"distinct_retweeters\": # get individual retweeters first retweeters = {tweet_id: self.fetcher.get_retweeters_ids(tweet_id) for tweet_id in tweet_ids} # get distinct retweeters by calculating the difference for each set distinct_retweeters = self.data_processor.difference(retweeters) results[attr] = distinct_retweeters # compute similarity between two tweets basd on the defined features case \"similarity\": # feature list object must be defined if features is None: raise ValueError(\"'features' list must be provided.\") # get public metrics for Tweet objects first public_metrics = {tweet_id: self.fetcher.get_public_metrics(tweet_id) for tweet_id in tweet_ids} # calculate similarity based on defined feature vector results[attr] = self.data_processor.calc_similarity(tweet_metrics=public_metrics, features=features) # compare creation dates of tweets case \"created_at\": # get individual creation dates first creation_dates = {tweet_id: self.fetcher.get_tweet_object(tweet_id).created_at for tweet_id in tweet_ids} # add datetime metrics creation_dates = self.data_processor.calc_datetime_metrics(creation_dates) results[attr] = creation_dates # if attribute was not found case _: results[attr] = None # if UTC timestamp should be returned if return_timestamp: results[\"utc_timestamp\"] = strf_datetime(datetime.utcnow(), format=\"%Y-%m-%d %H:%M:%S.%f\") return self._handle_output(results)","title":"TwitterAPI"},{"location":"maintenance/TwitterAPI/#twitterapi","text":"The TwitterAPI class forms the main class for user interaction and serves as a simple interface through encapsulation since the user can use all the functionalities of the package via the four main functions. All functionalities (except the utility function from the utils module) can be used via this class. The TwitterAPI class combines compatible data processing mechanisms with the respective data queries. This is achieved through composition whereby instances of the TwitterDataFetcher and TwitterDataProcessor are created within the class\u2019s constructor. Data processing mechanisms are outsourced to the functions of the TwitterDataProcessor instance whereas the data querying mechanisms are outsourced to the TwitterDataFetcher instance. This design decision ensures hiding implementation details from users or developers during usage or implementation and makes the code more structured. Thus, whenever any developer wants to make adjustments/fixes to the code, he or she has not to be aware of the code from the TwitterDataProcessor and TwitterDataFetcher instances. Contributing developers are able to understand this class's implementation by understanding the seperation of classes' concerns. This class serves the purpose of handling the input and output of user interactions through an interface. No further data processing or fetching is performed by this class. This class is built on top of the tweepy.Client class. The TwitterAPI class inherits the tweepy.Client class. Thus, every function from the inherited class will also be available in the TwitterAPI class, resulting in an package that extends the official Twitter API. The default behavior of the tweepy.Client class is overwritten but retained in order to extend this class. See the package architecture on the overview section of this chapter .","title":"TwitterAPI"},{"location":"maintenance/TwitterAPI/#initialization","text":"Import the TwitterAPI class from the api module or use the shortcut. from pysna.api import TwitterAPI # full path from pysna import TwitterAPI # shortcut api = TwitterAPI( bearer_token: Any | None = None, consumer_key: Any | None = None, consumer_secret: Any | None = None, access_token: Any | None = None, access_token_secret: Any | None = None, x_rapidapi_key: Any | None = None, x_rapidapi_host: Any | None = None, wait_on_rate_limit: bool = True ) and invoke a function: user_id = 123450897612 api.user_info(user_id, [\"screen_name\", \"followers_count\"]) Find the necessary secrets on the user guide instructions .","title":"Initialization"},{"location":"maintenance/TwitterAPI/#methods","text":"All functions have pre-defined input parameters (either for the attributes or compare arguments). They are stored inside the literal objects: TwitterAPI.LITERALS_USER_INFO : available attributes for a Twitter account. For more information, see here . TwitterAPI.LITERALS_COMPARE_USERS : available comparison attributes for Twitter accounts. For more information, see here . TwitterAPI.LITERALS_TWEET_INFO : available attributes for a tweet. For more information, see here . TwitterAPI.LITERALS_COMPARE_TWEETS : available comparison attributes for tweets. For more information, see here . Each (comparison) attribute is covered in the corresponding main function by iterations of a for loop. Whenever one of the four main functions is called, the required data resources are fetched by making API calls via the TwitterDataFetcher class for each specified (comparison) attribute and are processed by the TwitterDataProcessor class if necessary. For instance, when comparing multiple users on their common followers, individual followers for each user are fetched first and then processed by calculating the intersection set of all individual followers. If any new functionality is added to this class (e.g., get the length of a tweet by a length attribute), the new attribute has to be added to the corresponding literal object. In addition, the comparison functions also have a features argument which is used to define a feature vector for comparison. The available similarity features for users and tweets are also specified in separate literal objects: TwitterAPI.SIMILARITY_FEATURES_COMPARE_USERS : available similarity features for the TwitterAPI.compare_users function. For more information, see here . TwitterAPI.SIMILARITY_FEATURES_COMPARE_TWEETS : available similarity features for the TwitterAPI.compare_tweets function. For more information, see here .","title":"Methods"},{"location":"maintenance/TwitterAPI/#handle_output-private","text":"This function is designed to handle the output of the four main functions accordingly. To avoid accessing a returned dictionary with only one available key, this function returns either the single value from the one-key dictionary or the full dictionary itself if multiple keys are available. Function: TwitterAPI._handle_output(output: dict) This function is private and, thus, not intended for external use. This function was designed to facilitate the use of the package without the need for accessing a returned one-key dictionary.","title":"handle_output (private)"},{"location":"maintenance/TwitterAPI/#user_info","text":"This function allows to request user information from a Twitter account. Function: TwitterAPI.user_info(user: str | int, attributes: List[LITERALS_USER_INFO] | str, return_timestamp: bool = False) Args: user (str | int): Twitter User either specified by corresponding ID or screen name. attributes (List[str] | str): Attributes of the User object. These must be from this list . return_timestamp (bool, optional): Add UTC Timestamp to results. Defaults to False. This function takes in a Twitter user identifier (i.e., an ID or unique screen name). The attributes are passed in by a list object or by a single string. For a single provided attribute, only the corresponding value is returned. For multiple attributes, a dictionary with the key-value pairs of the requested attributes is returned. If the requested attribute for the objet is not available, None will be returned. Source Code def user_info(self, user: str | int, attributes: List[LITERALS_USER_INFO] | str, return_timestamp: bool = False) -> Any: \"\"\"Receive requested user information from Twitter User Object. For one attribute, only the corresponding value is returned. For multiple attributes, a dictionary with the key-value pairs of the requested attributes is returned. Args: user (str | int): Twitter User either specified by corresponding ID or screen name. attributes (List[str] | str): Attributes of the User object. These must be from: id, id_str, name, screen_name, followers, followees, location, description, url, entities, protected, followers_count, friends_count, listed_count, created_at, latest_activity, last_active, liked_tweets, composed_tweets, favourites_count, verified, statuses_count, status, contributors_enabled, profile_image_url_https, profile_banner_url, default_profile, default_profile_image, withheld_in_countries, bot_scores return_timestamp (bool, optional): Add UTC Timestamp to results. Defaults to False. Raises: KeyError: If invalid attribute was provided. ValueError: If Botometer secrets were not provided. Returns: dict: Requested user information. References: https://mathun3003.github.io/PySNA/user-guide/overview/TwitterAPI/#user_info \"\"\" # catch Botometer API secrets before iteration over attributes. if \"bot_scores\" in attributes: if (self._x_rapidapi_key is None) or (self._x_rapidapi_host is None): raise ValueError(\"'X_RAPIDAPI_KEY' and 'X_RAPIDAPI_HOST' secrets for Botometer API need to be provided.\") # initialize empty dict to store requested attributes user_info = dict() # if single string was provided if isinstance(attributes, str): # convert to list for iteration attributes = [attributes] # get user object user_obj = self.fetcher.get_user_object(user) # loop through the list of attributes and add them to the dictionary for attr in attributes: # if invalid attribute was provided if attr not in get_args(self.LITERALS_USER_INFO): raise ValueError(\"Invalid attribute for '{}'\".format(attr)) # if the desired attribute is in default user object returned by the v1 Search API elif attr in user_obj._json.keys(): user_info[attr] = user_obj._json[attr] # get information about user's followers elif attr == \"followers\": user_info[attr] = self.data_processor.extract_followers(user_obj) # get information about user's followees elif attr == \"followees\": user_info[attr] = self.data_processor.extract_followees(user_obj) # get all liked tweets of user elif attr == \"liked_tweets\": # get page results first liked_tweets = self.fetcher.get_liked_tweets_ids(user) user_info[attr] = liked_tweets # get all composed tweets elif attr == \"composed_tweets\": # get page results first composed_tweets = self.fetcher.get_composed_tweets_ids(user) user_info[attr] = composed_tweets # get user's latest activity elif attr == \"latest_activity\": user_info[attr] = self.fetcher.get_latest_activity(user) # get user's latest activity date elif attr == \"last_active\": user_info[attr] = self.fetcher.get_latest_activity_date(user) # get user's botometer scores elif attr == \"bot_scores\": user_info[attr] = self.fetcher.get_botometer_scores(user) # if attribute was not found else: user_info[attr] = None # if timestamp should be returned if return_timestamp: user_info[\"utc_timestamp\"] = strf_datetime(datetime.utcnow(), format=\"%Y-%m-%d %H:%M:%S.%f\") return self._handle_output(user_info)","title":"user_info"},{"location":"maintenance/TwitterAPI/#compare_users","text":"This function allows a comparison of multiple Twitter accounts. Function: TwitterAPI.compare_users(users: List[str | int], compare: str | List[LITERALS_COMPARE_USERS], return_timestamp: bool = False, features: List[str] | None = None) Args: users (List[str | int]): User IDs or screen names compare (str): Comparison attribute. Must be from this list . return_timestamp (bool, optional): Add UTC Timestamp to results. Defaults to False. features (List[str] | None, optional): Defined features of Twitter User Object on which similarity will be computed. Must be from the features list . Defaults to None. This function takes in multiple Twitter user identifiers (i.e., IDs or unique screen names). The comparison attributes are passed in by a list object or by a single string. For a single attribute, only the corresponding value is returned. For multiple attributes, a dictionary with the key-value pairs of the requested attributes is returned. Source Code def compare_users(self, users: List[str | int], compare: str | List[LITERALS_COMPARE_USERS], return_timestamp: bool = False, features: List[str] | None = None) -> Any: \"\"\"Compare two or more users with the specified comparison attribute(s). For one attribute, only the corresponding value is returned. For multiple attributes, a dictionary with the key-value pairs of the requested attributes is returned. Args: users (List[str | int]): User IDs or screen names compare (str): Comparison attribute. Must be from: relationship, followers_count, followees_count, tweets_count, favourites_count, common_followers, distinct_followers, common_followees, distinct_followees, commonly_liked_tweets, distinctly_liked_tweets, similarity, created_at, protected, verified. return_timestamp (bool, optional): Add UTC Timestamp to results. Defaults to False. features (List[str] | None, optional): Defined features of Twitter User Object on which similarity will be computed. Must be from: followers_count, friends_count, listed_count, favourites_count, statuses_count. Defaults to None. Raises: ValueError: If invalid comparison attribute was provided. Returns: dict | list: Results of requested comparison attribute(s). Referencs: https://mathun3003.github.io/PySNA/user-guide/overview/TwitterAPI/#compare_users \"\"\" # users list must contain at least two elements assert len(users) > 1, \"'users' list must contain at least two elements, {} was/were provided\".format(len(users)) # catch if feature vector contains only numeric values, and contains at least two elements if features: assert len(features) > 1, \"'features' list must have at least two elements. {} was/were given\".format(len(features)) for feat in features: if feat not in get_args(self.SIMILARITY_FEATURES_COMPARE_USERS): raise ValueError(f\"Only numeric features are supported. Must be from: {', '.join(get_args(self.SIMILARITY_FEATURES_COMPARE_USERS))}. You passed in {feat}\") # if single comparison attribute was provided as string if isinstance(compare, str): # change to list object compare = [compare] # init empty dict to store results results = dict() # iterate over comparison attributes for attr in compare: # if invalid attribute was provided if attr not in get_args(self.LITERALS_COMPARE_USERS): raise ValueError(\"Invalid attribute for '{}'\".format(attr)) # match comparison attributes match attr: # compare relationships between two users case \"relationship\": results[attr] = self.fetcher.get_relationship_pairs(users) # compare number of followers case \"followers_count\": # get individual followers followers = {user: self.fetcher.get_user_object(user).followers_count for user in users} # add descriptive metrics followers_with_metrics = self.data_processor.calc_descriptive_metrics(followers) results[attr] = followers_with_metrics # compare number of friends case \"followees_count\": # get individual followees followees = {user: self.fetcher.get_user_object(user).friends_count for user in users} # add descriptive metrics followees = self.data_processor.calc_descriptive_metrics(followees) results[attr] = followees # compare number of Tweets issued by each user case \"tweets_count\": # get individual statuses counts tweets = {user: self.fetcher.get_user_object(user).statuses_count for user in users} # add descriptive metrics tweets = self.data_processor.calc_descriptive_metrics(tweets) results[attr] = tweets # compare number of likes issued by each user case \"favourites_count\": # get individual likes likes = {user: self.fetcher.get_user_object(user).favourites_count for user in users} # add descriptive metrics likes = self.data_processor.calc_descriptive_metrics(likes) results[attr] = likes # compare protected attribute of users case \"protected\": results[attr] = {user: self.fetcher.get_user_object(user).protected for user in users} # compare verified attribute for users case \"verified\": results[attr] = {user: self.fetcher.get_user_object(user).verified for user in users} # get common followers case \"common_followers\": # get individual followers first individual_followers = [self.fetcher.get_user_follower_ids(user) for user in users] # get common followers by calculating the intersection common_followers = self.data_processor.intersection(individual_followers) results[attr] = common_followers # get distinct followers case \"distinct_followers\": # get individual followers first individual_followers = {user: self.fetcher.get_user_follower_ids(user) for user in users} # get distinct followers by calculating the difference of each set distinct_followers = self.data_processor.difference(individual_followers) results[attr] = distinct_followers # get common followees case \"common_followees\": # get individual followees first individual_followees = [self.fetcher.get_user_followee_ids(user) for user in users] # get common followees by calculating the intersection common_followees = self.data_processor.intersection(individual_followees) results[attr] = common_followees # get distinct followees case \"distinct_followees\": # get individual followees first individual_followees = {user: self.fetcher.get_user_followee_ids(user) for user in users} # get distinct followees by calculating the difference of each set distinct_followees = self.data_processor.difference(individual_followees) results[attr] = distinct_followees # get common liked tweets case \"commonly_liked_tweets\": # get individual liked tweets first individual_likes = [self.fetcher.get_liked_tweets_ids(user) for user in users] # get common liked tweets by calculating the intersection common_likes = self.data_processor.intersection(individual_likes) results[attr] = common_likes # get distinct liked tweets case \"distinctly_liked_tweets\": # get individual liked tweets first individual_likes = {user: self.fetcher.get_liked_tweets_ids(user) for user in users} # get distinct liked tweets by calculating the difference for each set distinct_likes = self.data_processor.difference(individual_likes) results[attr] = distinct_likes # compute similarity between two users basd on the defined features case \"similarity\": # feature list object must be defined if features is None: raise ValueError(\"'features' list must be provided.\") # get serialized user objects first user_objs = [self.fetcher.get_user_object(user)._json for user in users] # calculate similarity based on defined feature vector results[attr] = self.data_processor.calc_similarity(user_objs=user_objs, features=features) # compare creaation dates case \"created_at\": # get individual creation dates first creation_dates = {user: self.fetcher.get_user_object(user).created_at for user in users} # add datetime metrics creation_dates = self.data_processor.calc_datetime_metrics(creation_dates) results[attr] = creation_dates # if comparison attribute was not found case _: results[attr] = None # if timestamp should be returned if return_timestamp: results[\"utc_timestamp\"] = strf_datetime(datetime.utcnow(), format=\"%Y-%m-%d %H:%M:%S.%f\") return self._handle_output(results)","title":"compare_users"},{"location":"maintenance/TwitterAPI/#tweet_info","text":"This function allows to request tweet information from a tweet object. Function: TwitterAPI.tweet_info(tweet_id: str | int, attributes: List[LITERALS_TWEET_INFO] | str, return_timestamp: bool = False) Args: tweet_id (str | int): Tweet ID attributes (List[LITERALS_TWEET_INFO] | str): Attributes of the Tweet object. These must be from this list . return_timestamp (bool, optional): Add UTC Timestamp to results. Defaults to False. This function takes in a tweet ID as string or integer representation. The attributes are passed in by a list object or by a single string. For a single provided attribute, only the corresponding value is returned. For multiple attributes, a dictionary with the key-value pairs of the requested attributes is returned. If the requested attribute for the objet is not available, None will be returned. Source Code def tweet_info(self, tweet_id: str | int, attributes: List[LITERALS_TWEET_INFO] | str, return_timestamp: bool = False) -> Any: \"\"\"Receive requested Tweet information from Tweet Object. For one attribute, only the corresponding value is returned. For multiple attributes, a dictionary with the key-value pairs of the requested attributes is returned. Args: tweet_id (str | int): Tweet ID attributes (List[LITERALS_TWEET_INFO] | str): Attributes of the Tweet object. These must be from: id, id_str, full_text, display_text_range, truncated, created_at, entities, tweet_annotations, source, retweeters, in_reply_to_status_id, in_reply_to_status_id_str, in_reply_to_user_id, in_reply_to_user_id_str, in_reply_to_screen_name, user, contributors, coordinates, place, is_quote_status, public_metrics, quoting_users, liking_users, favorited, retweeted, retweeted_status, possibly_sensitive, lang, sentiment. return_timestamp (bool, optional): Add UTC Timestamp to results. Defaults to False. Raises: ValueError: If invalid attribute was provided. Returns: dict: Requested Tweet information. References: https://mathun3003.github.io/PySNA/user-guide/overview/TwitterAPI/#tweet_info \"\"\" # get tweet object tweet_obj = self.fetcher.get_tweet_object(tweet_id) # initialize empty dict to store request information tweet_info = dict() # if single string was provided if isinstance(attributes, str): # convert to list for iteration attributes = [attributes] for attr in attributes: # if invalid attribute was provided if attr not in get_args(self.LITERALS_TWEET_INFO): raise ValueError(\"Invalid attribute for '{}'\".format(attr)) # get default attributes from tweepy Status model elif attr in tweet_obj._json.keys(): tweet_info[attr] = tweet_obj._json[attr] # get all quoting users elif attr == \"quoting_users\": quoting_users = self.fetcher.get_quoting_users_ids(tweet_id) tweet_info[attr] = quoting_users # get all liking users elif attr == \"liking_users\": liking_users = self.fetcher.get_liking_users_ids(tweet_id) tweet_info[attr] = liking_users # get all retweeters elif attr == \"retweeters\": retweeters = self.fetcher.get_retweeters_ids(tweet_id) tweet_info[attr] = retweeters # get public metrics elif attr == \"public_metrics\": tweet_info[attr] = self.fetcher.get_public_metrics(tweet_id) # get context annotations elif attr == \"tweet_annotations\": tweet_info[attr] = self.fetcher.get_context_annotations_and_entities(tweet_id) # get tweet sentiment elif attr == \"sentiment\": tweet_info[attr] = self.data_processor.detect_tweet_sentiment(tweet_obj.full_text) # if attribute was not found else: tweet_info[attr] = None # if timestamp should be returned if return_timestamp: tweet_info[\"utc_timestamp\"] = strf_datetime(datetime.utcnow(), format=\"%Y-%m-%d %H:%M:%S.%f\") return self._handle_output(tweet_info)","title":"tweet_info"},{"location":"maintenance/TwitterAPI/#compare_tweets","text":"This function allows a comparison of multiple tweets. Function: TwitterAPI.compare_tweets(tweet_ids: List[str | int], compare: str | List[LITERALS_COMPARE_TWEETS], return_timestamp: bool = False, features: List[str] | None = None) Args: tweets (List[str | int]): List of Tweet IDs. compare (str | List[LITERALS_COMPARE_TWEETS]): Comparison attribute. Needs to be from the this list . return_timestamp (bool, optional): Add UTC Timestamp to results. Defaults to False. features (List[str] | None, optional): Defined features of Twitter User Object on which similarity will be computed. Must be from the features list . Defaults to None. This function takes in multiple tweet IDs as string or integer representation. The comparison attributes are passed in by a list object or by a single string. For a single attribute, only the corresponding value is returned. For multiple attributes, a dictionary with the key-value pairs of the requested attributes is returned. Source Code def compare_tweets(self, tweet_ids: List[str | int], compare: str | List[LITERALS_COMPARE_TWEETS], return_timestamp: bool = False, features: List[str] | None = None) -> Any: \"\"\"Compare two or more Tweets with the specified comparison attribute. For one attribute, only the corresponding value is returned. For multiple attributes, a dictionary with the key-value pairs of the requested attributes is returned. Args: tweets (List[str | int]): List of Tweet IDs. compare (str | List[LITERALS_COMPARE_TWEETS]): Comparison attribute. Needs to be from the following: view_count, like_count, retweet_count, quote_count, reply_count, common_quoting_users, distinct_quoting_users, common_liking_users, distinct_liking_users, common_retweeters, distinct_retweeters, similarity, created_at. return_timestamp (bool, optional): Add UTC Timestamp to results. Defaults to False. features (List[str] | None, optional): Defined features of Twitter User Object on which similarity will be computed. Must be from: retweet_count, reply_count, like_count, quote_count, impression_count. Defaults to None. Raises: AssertionError: If a list of one Tweet ID was provided. ValueError: If invalid comparison attribute was provided. Returns: dict: Requested results for comparison attribute. References: https://mathun3003.github.io/PySNA/user-guide/overview/TwitterAPI/#compare_tweets \"\"\" # tweets list must contain at least two IDs assert len(tweet_ids) > 1, \"'tweets' list object needs at least two entries, not {}\".format(len(tweet_ids)) # catch if feature vector contains only numeric values, and contains at least two elements if features: assert len(features) > 1, \"'features' list must have at least two elements. {} was/were given\".format(len(features)) for feat in features: if feat not in get_args(self.SIMILARITY_FEATURES_COMPARE_TWEETS): raise ValueError(f\"Only numeric features are supported. Must be from: {', '.join(get_args(self.SIMILARITY_FEATURES_COMPARE_TWEETS))}. You passed in {feat}.\") # if single comparison attribute was provided as string if isinstance(compare, str): # change to list object compare = [compare] # init empty dict to store results results = dict() # iterate over every given comparison atttribute for attr in compare: # if invalid attribute was provided if attr not in get_args(self.LITERALS_COMPARE_TWEETS): raise ValueError(\"Invalid attribute for '{}'\".format(attr)) # match comparison attribute match attr: # compare numer of views / impressions case \"view_count\": # get individual view_counts view_counts = {tweet_id: self.fetcher.get_public_metrics(tweet_id)[\"impression_count\"] for tweet_id in tweet_ids} # add descriptive metrics view_counts = self.data_processor.calc_descriptive_metrics(view_counts) results[attr] = view_counts # compare number of likes case \"like_count\": # get individual like_counts like_counts = {tweet_id: self.fetcher.get_public_metrics(tweet_id)[\"like_count\"] for tweet_id in tweet_ids} # add descriptive metrics like_counts = self.data_processor.calc_descriptive_metrics(like_counts) results[attr] = like_counts # compare number or retweets case \"retweet_count\": # get individual number of retweets retweet_counts = {tweet_id: self.fetcher.get_public_metrics(tweet_id)[\"retweet_count\"] for tweet_id in tweet_ids} # add descriptive metrics retweet_counts = self.data_processor.calc_descriptive_metrics(retweet_counts) results[attr] = retweet_counts # compare number of quotes case \"quote_count\": # get individual number of quotes quote_counts = {tweet_id: self.fetcher.get_public_metrics(tweet_id)[\"quote_count\"] for tweet_id in tweet_ids} # add descriptive metrics quote_counts = self.data_processor.calc_descriptive_metrics(quote_counts) results[attr] = quote_counts # compare number of commonts case \"reply_count\": # get individual number of replies first reply_counts = {tweet_id: self.fetcher.get_public_metrics(tweet_id)[\"reply_count\"] for tweet_id in tweet_ids} # add descriptive metrics reply_counts = self.data_processor.calc_descriptive_metrics(reply_counts) results[attr] = reply_counts # get all quoting users all Tweets have in common case \"common_quoting_users\": # get individual quoting users first quoting_users = [self.fetcher.get_quoting_users_ids(tweet_id) for tweet_id in tweet_ids] # get common quoting users by calculating the intersection common_quoting_users = self.data_processor.intersection(quoting_users) # return quoting users results[attr] = common_quoting_users # get distinct quoting users for each tweet case \"distinct_quoting_users\": # get individual quoting users first quoting_users = {tweet_id: self.fetcher.get_quoting_users_ids(tweet_id) for tweet_id in tweet_ids} # get distinct quoting users for each tweet by calculating the difference for each set distinct_quoting_users = self.data_processor.difference(quoting_users) results[attr] = distinct_quoting_users # get all liking users that all tweets have in common case \"common_liking_users\": # get individual liking users first liking_users = [self.fetcher.get_liking_users_ids(tweet_id) for tweet_id in tweet_ids] # get common liking users by calculating the intersection common_liking_users = self.data_processor.intersection(liking_users) # return common liking users results[attr] = common_liking_users # get distinct liking users of all tweets case \"distinct_liking_users\": # get individual liking users first liking_users = {tweet_id: self.fetcher.get_liking_users_ids(tweet_id) for tweet_id in tweet_ids} # get distinct liking users for each tweet by calculating the difference for each set distinct_liking_users = self.data_processor.difference(liking_users) results[attr] = distinct_liking_users # get all retweeters all tweets have in common case \"common_retweeters\": # get individual retweeters first retweeters = [self.fetcher.get_retweeters_ids(tweet_id) for tweet_id in tweet_ids] # get common retweeters by calculating the intersection common_retweeters = self.data_processor.intersection(retweeters) # return common retweeters results[attr] = common_retweeters # get distinct retweeters of all tweets case \"distinct_retweeters\": # get individual retweeters first retweeters = {tweet_id: self.fetcher.get_retweeters_ids(tweet_id) for tweet_id in tweet_ids} # get distinct retweeters by calculating the difference for each set distinct_retweeters = self.data_processor.difference(retweeters) results[attr] = distinct_retweeters # compute similarity between two tweets basd on the defined features case \"similarity\": # feature list object must be defined if features is None: raise ValueError(\"'features' list must be provided.\") # get public metrics for Tweet objects first public_metrics = {tweet_id: self.fetcher.get_public_metrics(tweet_id) for tweet_id in tweet_ids} # calculate similarity based on defined feature vector results[attr] = self.data_processor.calc_similarity(tweet_metrics=public_metrics, features=features) # compare creation dates of tweets case \"created_at\": # get individual creation dates first creation_dates = {tweet_id: self.fetcher.get_tweet_object(tweet_id).created_at for tweet_id in tweet_ids} # add datetime metrics creation_dates = self.data_processor.calc_datetime_metrics(creation_dates) results[attr] = creation_dates # if attribute was not found case _: results[attr] = None # if UTC timestamp should be returned if return_timestamp: results[\"utc_timestamp\"] = strf_datetime(datetime.utcnow(), format=\"%Y-%m-%d %H:%M:%S.%f\") return self._handle_output(results)","title":"compare_tweets"},{"location":"maintenance/TwitterDataFetcher/","text":"TwitterDataFetcher The TwitterDataFetcher class is used to specifically query Twitter data. To simplify the queries to the Twitter API, this software component uses already existing open-source software for interacting with the API, namely the Tweepy Python package . It uses the Tweepy Client class to query the data dictionaries of the Twitter Search API v2 as well as the Tweepy API class to access the data dictionaries based on the Twitter Search API v1. The Twitter Search API v1 is mainly used to query user and tweet objects. Although this API version is partially deprecated, it offers a comparable content to the latest API version and often requires less API calls to receive the same information compared to the v2 API. Additional direct requests to the Twitter Search API v2 are performed, too, using the Python requests library to query endpoints that have been migrated or deprecated in the Tweepy package. This class can also be used to in isolation to collect Twitter data. It requires, therefore, authentication for the Twitter platform. Provide the secrets for the Twitter API and, if desired, for the Botometer API. You can find a list of required secrets in the user guide for the TwitterAPI class. This class has the concern to fetch Twitter data. No further processing is performed within this class. Initialization If you want to use this class for data processing or other package components, follow the steps below. Import the TwitterDataFetcher class from the fetch module. from pysna.fetch import TwitterDataFetcher fetcher = TwitterDataFetcher( bearer_token: Any | None = None, consumer_key: Any | None = None, consumer_secret: Any | None = None, access_token: Any | None = None, access_token_secret: Any | None = None, x_rapidapi_key: Any | None = None, x_rapidapi_host: Any | None = None ) and invoke a function: user_id = 123450897612 fetcher.get_latest_activity(user_id) Find the necessary secrets on the user guide instructions . Methods Private Methods manual_request Performs a manual request to the Twitter API. Returns JSON formatted API response. Function: TwitterDataFetcher._manual_request(url: str, method: str = \"GET\", header: dict | None = None, payload: dict | None = None, additional_fields: Dict[str, List[str]] | None = None) Args: url (str): API URL (without specified fields) method (str): Request method according to REST. Defaults to \"GET\". header : Custom HTTP Header. Defaults to None. payload : JSON data for HTTP requests. Defaults to None. additional_fields (Dict[str, List[str]] | None, optional): Fields can be specified (e.g., tweet.fields) according to the official API reference. Defaults to None. The function will raise an exception if the response status code is unlike 200. With this function, performig manual requests is facilitated as the query string is built by the function based on the provided input arguments. The url argument has to be provided in raw form (i.e., without any parameters or fields). The method argument allows to specify the REST request method (i.e., GET, POST, PUT, DELETE). Defaults to GET. The header argument allows to specify a custom header. This is useful if another API besides the Twitter API is fetched. If no custom header is provided, the default header for the Twitter API authentification is used based on the provided bearer_token during instantiation. The payload argument allows to send data for a POST or PUT request. The data must be provided as a dictionary. The additional_fields argument is used to specify Twitter fields (i.e., user fields or tweet fields) and, thus, enhance the query and return additional information. The argument can be used as follows: {\"tweet.fields\": [\"public_metrics\"]} The function will then build the query string and send it to the API. You can find the full list of Twitter fields in the documentation: https://developer.twitter.com/en/docs/twitter-api/fields Source Code def _manual_request(self, url: str, method: str = \"GET\", header: dict | None = None, payload: dict | None = None, additional_fields: Dict[str, List[str]] | None = None) -> dict: \"\"\"Perform a manual request to the Twitter API. Args: url (str): API URL (without specified fields) method (str): Request method according to REST. Defaults to \"GET\". header (dict | None): Custom HTTP Header. Defaults to None. payload (dict | None): JSON data for HTTP requests. Defaults to None. additional_fields (Dict[str, List[str]] | None, optional): Fields can be specified (e.g., tweet.fields) according to the official API reference. Defaults to None. Raises: Exception: If status code != 200. Returns: dict: JSON formatted response of API request. \"\"\" # if additional_fields were provided if additional_fields: # init empty string fields = \"?\" # create fields string dynamically for every field in additional_fields for field in additional_fields.keys(): # e.g., in format \"tweet.fields=lang,author_id\" fields += f\"{field}={','.join(additional_fields[field])}&\" # append fields to url url += fields[:-1] if header is None: # set header header = {\"Authorization\": f\"Bearer {self._bearer_token}\"} response = requests.request(method=method, url=url, headers=header, json=payload) if response.status_code != 200: raise Exception(\"Request returned an error: {} {}\".format(response.status_code, response.text)) return response.json() paginate Custom pagination function It turns out that the pagination functions from the Tweepy Python packge are considerably slower than doing the pagination manually. For this reason, this function was designed. Function: TwitterDataFetcher._paginate(func, params: Dict[str, str | int], limit: int | None = None, response_attribute: str = \"data\", page_attribute: str | None = None) Args: func : Function used for pagination params (Dict[str, str | int]): Dict containing request parameters. Must be of the form {'id': ..., 'limit': ..., 'pagination_token': ...} limit (int | None, optional): Maximum number of results. Defaults to None, thus, no limit. response_attribute (str, optional): Attribute of the Response object. Defaults to \"data\". Options: [\"data\", \"includes\"] page_attribute (str, optional): The attribute that should be extracted for every entry of a page. Defaults to None. The params argument is used to specify the parameters for the next page. Therefore, an id is needed as well as a key indicating the maximm number of results (i.e., limit ). None indicates that no limit is desired and, thus, all available results will be returned. The pagination_token key can be set to None initially. This pagination token will be reset during iteraion. In case, you wish to start from a different page than the first one, provide a pagination token. All parameters must be provided via a dictionary of the form: {\"id\": 1234456, \"limit\": None, # no limit \"pagination_token\": None} The response_attribute argument specifies where to collect the data from the response. If data is specified, the results are received from the default attribute field of the response. If includes is specified, the results are obtained from the additional information provided by the Twitter fields.` The page_attribute argument specifies what attribute should be extracted for every entry of a page. For instance, if this argument is set to id , then the IDs will be extracted from every entry (e.g., user IDs of user objects). Inside that function, a counter is incremented for every result that has been fetched. If the limit was reached, the function will break out the loop and will return immediately the obtained results. Otherwise, the function will check if last page was reached and will fetch the next page (if available). Source Code def _paginate(self, func, params: Dict[str, str | int], limit: int | None = None, response_attribute: str = \"data\", page_attribute: str | None = None) -> list: \"\"\"Pagination function Args: func: Function used for pagination params (Dict[str, str | int]): Dict containing request parameters. Should be of the form {'id': ..., 'max_results': ..., 'pagination_token': ...} limit (int | None, optional): Maximum number of results. Defaults to None, thus, no limit. response_attribute (str, optional): Attribute of the Response object. Defaults to \"data\". Options: [\"data\", \"includes\"] Raises: KeyError: 'id', 'max_results', and 'pagination_token' should be provided in the params dict. Returns: set: Results \"\"\" # init counter counter = 0 # init empty results set results = list() # set break out var break_out = False while not break_out: # make request response = func(**params) # if any data exists if response.__getattribute__(response_attribute) is not None: # iterate over response results for item in response.__getattribute__(response_attribute): # add result if page_attribute is None: results.append(item) else: results.append(item.__getattribute__(page_attribute)) # increment counter counter += 1 # if limit was reached, break if (limit is not None) and (counter == limit): # set break_out var to true break_out = True break # if last page was reached if \"next_token\" not in response.meta: break # else, set new pagination token for next iteration else: params[\"pagination_token\"] = response.meta[\"next_token\"] # if no data exists, break else: break return results Twitter user related methods get_user_object Request Twitter user object using Tweepy. The user object is fetched from the Twitter Search API v1. For this, the Tweepy API class is used. Function: TwitterDataFetcher.get_user_object(user: str | int) The function takes in either the user ID as string or integer or the user's unique screen name. It returns the requested API v1 user object. The function handles the performed request based on what user identifier was given. If the requested user has been suspended from Twitter, an error will be returned and a messeage will be logged to stdout. Source Code def get_user_object(self, user: str | int) -> tweepy.models.User: \"\"\"Request Twitter User Object via tweepy Args: user (str): Either User ID or screen name Returns: tweepy.User: Twitter User object from tweepy \"\"\" try: # check if string for user1 is convertible to int in order to check for user ID or screen name if (isinstance(user, int)) or (user.isdigit()): # get profile for user by user ID user_obj = self.api.get_user(user_id=user) else: # get profile for user by screen name user_obj = self.api.get_user(screen_name=user) except tweepy.errors.Forbidden as e: # log to stdout log.error(\"403 Forbidden: access refused or access is not allowed.\") # if user ID was provided if user.isdigit() or isinstance(user, int): url = f\"https://api.twitter.com/2/users/{user}\" else: # if screen name was provided url = f\"https://api.twitter.com/2/users/by/username/{user}\" response = self._manual_request(url) # if an error occured that says the user has been suspended if any(\"User has been suspended\" in error[\"detail\"] for error in response[\"errors\"]): log.error(\"User has been suspended from Twitter. Requested user: {}\".format(user)) raise e else: raise e return user_obj get_user_follower_ids Request Twitter follower IDs from user. Function: TwitterDataFetcher.get_user_follower_ids(user: str | int) This function takes in a Twitter user identifier (either ID or unique screen name). It returns all follower user IDs from the specified user as a set. Here, the ``tweepy.Cursor```is used for pagination. The function handles the performed request based on what user identifier was given. Source Code def get_user_follower_ids(self, user: str | int) -> Set[int]: \"\"\"Request Twitter follower IDs from user Args: user (str | int): Either User ID or screen name. Returns: Set[int]: Array containing follower IDs \"\"\" # check if string for user1 is convertible to int in order to check for user ID or screen name if (isinstance(user, int)) or (user.isdigit()): params = {\"user_id\": user} else: params = {\"screen_name\": user} follower_ids = list() for page in tweepy.Cursor(self.api.get_follower_ids, **params).pages(): follower_ids.extend(page) return set(follower_ids) get_user_followee_ids Request Twitter followee IDs from user. Function: TwitterDataFetcher.get_user_followee_ids(user: str | int) This function takes in a Twitter user identifier (i.e., either ID or unique screen name) and returns a set containing all IDs from the user's followees (AKA friends or follows). The function handles the performed request based on what user identifier was given. Source Code def get_user_followee_ids(self, user: str | int) -> Set[int]: \"\"\"Request Twitter followee IDs from user Args: user (str): Either User ID or screen name. Returns: Set[int]: Array containing follow IDs \"\"\" # check if string for user1 is convertible to int in order to check for user ID or screen name if (isinstance(user, int)) or (user.isdigit()): params = {\"user_id\": user} else: params = {\"screen_name\": user} followee_ids = list() for page in tweepy.Cursor(self.api.get_friend_ids, **params).pages(): followee_ids.extend(page) return set(followee_ids) get_latest_activity Returns latest user's activity by fetching the top element from its timeline. Function: TwitterDataFetcher.get_latest_activity(user: str | int) This function takes in a Twitter user identifier (i.e., either ID or unique screen name) and returns the latest activity from the user's timeline. Therefore, the _manual_request function is used to request the corresponding endpoint . Often, this will be a tweet composed by the user itself. Then, all available data of that tweet will be returned as a dictionary. The function handles the performed request based on what user identifier was given. Source Code def get_latest_activity(self, user: str | int) -> dict: \"\"\"Returns latest user's activity by fetching the top element from its timeline. Args: user (str | int): User ID or screen name. Returns: dict: Latest activity. \"\"\" # if screen name was provided if (isinstance(user, str)) and (user.isdigit() is False): url = f\"https://api.twitter.com/1.1/statuses/user_timeline.json?screen_name={user}&include_rts=true&trim_user=true&tweet_mode=extended\" # else go with user ID else: url = f\"https://api.twitter.com/1.1/statuses/user_timeline.json?user_id={user}&include_rts=true&trim_user=true&tweet_mode=extended\" response_json = self._manual_request(url) # return the first item since timeline is sorted descending return response_json[0] get_latest_activity_date Get latest activity date from specified user by fetching the top element from its timeline and extract the creation date. Function: TwitterDataFetcher.get_latest_activity_date(user: str | int) This function takes in a Twitter user identifier (i.e., either ID or unique screen name) and returns the latest activity date from the user's timeline. Therefore, the _manual_request function is used to request the corresponding endpoint . The latest activity date is determined by fetching the latest activity from the user's timeline first, and then extracting the creation date. Usually, this will be a tweet composed by the user. If this is the case, the creation date of that tweet will be returned, representing the latest public available activity date. Source Code def get_latest_activity_date(self, user: str | int) -> str: \"\"\"Get latest activity date from specified user by fetching the top element from its timeline. Args: user (str | int): User ID or screen name. Returns: str: Activity date of latest activity. \"\"\" # if screen name was provided if (isinstance(user, str)) and (user.isdigit() is False): url = f\"https://api.twitter.com/1.1/statuses/user_timeline.json?screen_name={user}&include_rts=true&trim_user=true\" # else go with user ID else: url = f\"https://api.twitter.com/1.1/statuses/user_timeline.json?user_id={user}&include_rts=true&trim_user=true\" response_json = self._manual_request(url) # return the first item since timeline is sorted descending return response_json[0][\"created_at\"] get_relationship Get relationship between two Twitter users. Function: TwitterDataFetcher.get_relationship(source_user: str | int, target_user: str | int) The function takes in a source and a target user identifier. It uses the Tweepy.API.get_friendship function to get the relationship. Therefore, this function handles the performed query based on the provided user identifiers. The function will return the parsed JSON relationship for the source and target user as a dictionary. Source Code def get_relationship(self, source_user: str | int, target_user: str | int) -> dict: \"\"\"Get relationship between two users. Args: user1 (str | int): Source user ID or screen name. user2 (str | int): Target user ID or screen name. Returns: dict: Unpacked tuple of JSON from tweepy Friendship model. Reference: https://developer.twitter.com/en/docs/twitter-api/v1/accounts-and-users/follow-search-get-users/api-reference/get-friendships-show#example-response \"\"\" params = {\"source_id\": None, \"source_screen_name\": None, \"target_id\": None, \"target_screen_name\": None} # if source_user is int or a digit if (isinstance(source_user, int)) or (isinstance(source_user, str) and (source_user.isdigit())): params[\"source_id\"] = source_user # else if screen name was provided elif (isinstance(source_user, str)) and (not source_user.isdigit()): params[\"source_screen_name\"] = source_user else: log.error(\"No ID or username provided for {}\".format(source_user)) # if target_user is int or a digit if (isinstance(target_user, int)) or (isinstance(target_user, str) and (target_user.isdigit())): params[\"target_id\"] = target_user # else if screen name was provided elif (isinstance(target_user, str)) and (not target_user.isdigit()): params[\"target_screen_name\"] = target_user else: log.error(\"No ID or username provided for {}\".format(target_user)) relationship = self.api.get_friendship(**params) return {\"source\": relationship[0]._json, \"target\": relationship[1]._json} get_relationship_pairs Creates pairs for each uniqie combination of provided users based on their relationship. Function: TwitterDataFetcher.get_relationship_pairs(users: List[str | int]) This function takes in a list of user identifiers (i.e., IDs or unique screen names). It will create a pair of each combination of the provided users and returns their individual relationships. For instance, if three users WWU_Muenster , goetheuni , UniKonstanz were provided, the pairs are determined as follows: ( WWU_Muenster , goetheuni ) ( WWU_Muenster , UniKonstanz ) ( goetheuni , WWU_Muenster ) ( goetheuni , UniKonstanz ) ( UniKonstanz , WWU_Muenster ) ( UniKonstanz , goehteuni ) These pairs are set as dictionary keys. The respective relationships are stored as dictionary values. Source Code def get_relationship_pairs(self, users: List[str | int]) -> dict: \"\"\"Creates pairs for each unique combination of provided users based on their relationship. Args: users (List[str | int]): List of user IDs or screen names. Returns: dict: Pairs of users containing their relationship to each other. \"\"\" # init emtpy relationships dict relationships = dict() # iterate over every pair combination of provided users for user in users: for other_user in users: if user != other_user: relationships[(user, other_user)] = self.get_relationship(source_user=user, target_user=other_user) return relationships get_liked_tweets_ids Get (all) liked tweet IDs of the provided user. Function: TwitterDataFetcher.get_liked_tweets_ids(user: str | int, limit: int | None = None) Args: user (str | int): User ID or screen name. limit (int | None): The maximum number of results to be returned. By default, each page will return the maximum number of results available. This function uses the custom TwitterDataFetcher._paginate function to get the specified number of results. To get the tweet IDs, the tweepy.Client.get_liked_tweets function is used. The function wil return a Python set of the IDs of the liked tweets by the user. The function handles the performed request based on what user identifier was given. Source Code def get_liked_tweets_ids(self, user: str | int, limit: int | None = None) -> list(): \"\"\"Get (all) liked Tweets of provided user. Args: user (str | int): User ID or screen name. limit (int | None): The maximum number of results to be returned. By default, each page will return the maximum number of results available. Returns: Set[int]: Tweet Objects of liked Tweets. \"\"\" # if user ID was provided if (isinstance(user, int)) or (user.isdigit()): params = {\"id\": user, \"max_results\": 100, \"pagination_token\": None} else: user_obj = self.get_user_object(user) params = {\"id\": user_obj.id, \"max_results\": 100, \"pagination_token\": None} page_results = self._paginate(self.client.get_liked_tweets, params, limit=limit, page_attribute=\"id\") return page_results get_composed_tweets_ids Get (all) composed tweet IDs of provided user by pagination. Function: TwitterDataFetcher.get_composed_tweets_ids(user: str | int, limit: int | None = None) Args: user (str | int): User ID or screen name. limit (int | None): The maximum number of results to be returned. By default, each page will return the maximum number of results available. This function uses the custom TwitterDataFetcher._paginate function to get the specified number of results. To get the tweet IDs, the tweepy.Client.get_users_tweets function is used. The function wil return a Python set of the IDs of the composed tweets by the user. The function handles the performed request based on what user identifier was given. Source Code def get_composed_tweets_ids(self, user: str | int, limit: int | None = None) -> list: \"\"\"Get (all) composed Tweets of provided user by pagination. Args: user (str | int): User ID or screen name. limit (int | None): The maximum number of results to be returned. By default, each page will return the maximum number of results available. Returns: list: Tweet Objects of composed Tweets. \"\"\" # user ID is required, if screen name was provided if (isinstance(user, str)) and (not user.isdigit()): user = self.get_user_object(user).id # set params params = {\"id\": user, \"max_results\": 100, \"pagination_token\": None} # get page results page_results = self._paginate(self.client.get_users_tweets, params, limit=limit, page_attribute=\"id\") return page_results get_botometer_scores Returns bot scores from the Botometer API for the specified Twitter account. Function: TwitterDataFetcher.get_botometer_scores(user: str | int) This function takes in a Twitter account identifier (i.e., ID or unique screen name.) This function relies on the external Botometer API . To use this function, the corresponding RapidAPI secrets need to be provided. See the secrets overview for more details. The function gets the user's timeline first and takes the latest 100 tweets from its timeline. Then, this data is send via the `payload argument of the TwitterDataFetcher._manual_request function using a POST request. Then, the JSON response is returned. Source Code def get_botometer_scores(self, user: str | int) -> dict: \"\"\"Returns bot scores from the Botometer API for the specified Twitter user. Args: user (str | int): User ID or screen name. Returns: dict: The raw Botometer scores for the specified user. Reference: https://rapidapi.com/OSoMe/api/botometer-pro/details \"\"\" if (self._x_rapidapi_key is None) or (self._x_rapidapi_host is None): raise ValueError(\"'X_RAPIDAPI_KEY' and 'X_RAPIDAPI_HOST' secrets for Botometer API need to be provided.\") # get user object user_obj = self.get_user_object(user) # get user timeline timeline = list(map(lambda x: x._json, self.api.user_timeline(user_id=user_obj.id, count=200))) # get user data if timeline: user_data = timeline[0][\"user\"] else: user_data = user_obj._json screen_name = \"@\" + user_data[\"screen_name\"] # get latest 100 Tweets tweets = list(map(lambda x: x._json, self.api.search_tweets(screen_name, count=100))) # set payload payload = {\"mentions\": tweets, \"timeline\": timeline, \"user\": user_data} # set header headers = {\"content-type\": \"application/json\", \"X-RapidAPI-Key\": self._x_rapidapi_key, \"X-RapidAPI-Host\": self._x_rapidapi_host} # set url url = \"https://botometer-pro.p.rapidapi.com/4/check_account\" # get results response = self._manual_request(url, \"POST\", headers, payload) return response Tweet related methods get_tweet_object Request Twitter tweet object via tweepy. Function: TwitterDataFetcher.get_tweet_object(tweet: str | int) The function takes in either the tweet ID as string or integer. It returns the extended tweet object requested via the API v1 using the tweepy.API.get_status function. If the requested tweet object has been deleted, an error will be returned and a messeage will be logged to stdout. Reference: https://developer.twitter.com/en/docs/twitter-api/v1/data-dictionary/object-model/tweet Source Code def get_tweet_object(self, tweet: str | int) -> tweepy.models.Status: \"\"\"Request Twitter Tweet Object via tweepy Args: tweet (int | str): Tweet ID Returns: tweepy.models.Status: tweepy Status Model Reference: https://developer.twitter.com/en/docs/twitter-api/v1/data-dictionary/object-model/tweet \"\"\" try: tweet_obj = self.api.get_status(tweet, include_entities=True, tweet_mode=\"extended\") except tweepy.errors.NotFound as e: log.error(\"404 Not Found: Resource not found.\") raise e except tweepy.errors.Forbidden as e: log.error(\"403 Forbidden: access refused or access is not allowed.\") raise e return tweet_obj get_liking_users_ids Get (all) liking users of provided tweet by pagination. Function: TwitterDataFetcher.get_liking_users_ids(tweet_id: str | int, limit: int | None = None) Args: tweet (str | int): Tweet ID. limit (int | None): The maximum number of results to be returned. By default, each page will return the maximum number of results available. The function takes in the tweet ID as string or integer representation as well as the limit argument. If limit is none, all available results will be returned. It returns the user IDs of the users that liked the specified tweet. This function uses the custom TwitterDataFetcher._paginate function to get the specified number of results. To get the user IDs, the tweepy.Client.get_liking_users function is used. Source Code def get_liking_users_ids(self, tweet_id: str | int, limit: int | None = None) -> list: \"\"\"Get (all) liking users of provided Tweet by pagination. Args: tweet (str | int): Tweet ID. limit (int | None): The maximum number of results to be returned. By default, each page will return the maximum number of results available. Returns: Set[int]: User Objects as list. \"\"\" # set params params = {\"id\": tweet_id, \"max_results\": 100, \"pagination_token\": None} # get page results page_results = self._paginate(self.client.get_liking_users, params, limit=limit, page_attribute=\"id\") return page_results get_retweeters_ids Get (all) retweeting users of provided tweet by pagination. Function: TwitterDataFetcher.get_retweeters_ids(tweet_id: str | int, limit: int | None = None) Args: tweet (str | int): Tweet ID. limit (int | None): The maximum number of results to be returned. By default, each page will return the maximum number of results available. The function takes in the tweet ID as string or integer representation as well as the limit argument. If limit is none, all available results will be returned. It returns the user IDs of the users that retweeted the specified tweet. This function uses the custom TwitterDataFetcher._paginate function to get the specified number of results. To get the user IDs, the tweepy.Client.get_retweeters function is used. Source Code def get_retweeters_ids(self, tweet_id: str | int, limit: int | None = None) -> list: \"\"\"Get (all) retweeting users of provided Tweet by pagination. Args: tweet (str | int): Tweet ID. limit (int | None): The maximum number of results to be returned. By default, each page will return the maximum number of results available. Returns: Set[int]: User Objects of retweeting users. \"\"\" params = {\"id\": tweet_id, \"max_results\": 100, \"pagination_token\": None} # get page results page_results = self._paginate(self.client.get_retweeters, params, limit=limit, page_attribute=\"id\") return page_results get_quoting_users_ids Get (all) quoting users of provided Tweet by pagination. Function: TwitterDataFetcher.get_quoting_users_ids(tweet_id: str | int, limit: int | None = None) Args: tweet_id (str | int): Tweet ID. limit (int | None): The maximum number of results to be returned. By default, each page will return the maximum number of results available. The function takes in the tweet ID as string or integer representation as well as the limit argument. If limit is none, all available results will be returned. It returns the user IDs of the users that quoted the specified tweet. This function uses the custom TwitterDataFetcher._paginate function to get the specified number of results. To get the tweet objects, the tweepy.Client.get_quote_tweets function is used. Then, the quoting users IDs are extracted from the additional information provided within the includes fields of each page. For more details, see the instructions on the TwitterDataFetcher._paginate function Source Code def get_quoting_users_ids(self, tweet_id: str | int, limit: int | None = None) -> list: \"\"\"Get (all) quoting users of provided Tweet by pagination. Args: tweet_id (str | int): Tweet ID. limit (int | None): The maximum number of results to be returned. By default, each page will return the maximum number of results available. Returns: list: User Objects of quoting users. \"\"\" params = {\"id\": tweet_id, \"max_results\": 100, \"pagination_token\": None} # get page results page_results = self._paginate(self.client.get_quote_tweets, params, limit=limit, response_attribute=\"includes\", page_attribute=\"id\") return page_results get_context_annotations_and_entities Get context annotations and entities from a tweet object. Function: TwitterDataFetcher.get_context_annotations_and_entities(tweet_id: str | int) The function takes in the tweet ID as string or integer representation. The function returns the context annotations (e.g., topics) and named entities of the specified tweet. Therefore, it uses the TwitterDataFetcher._manual_request function. The tweet fields for context_annotations and entities are set. If any context annotation or named entity exist, the JSON response of the request is returned, else None . Reference: https://developer.twitter.com/en/docs/twitter-api/annotations/overview Source Code def get_context_annotations_and_entities(self, tweet_id: str | int) -> dict | None: \"\"\"Get context annotations and entities from a Tweet. Args: tweet_id (str | int): Tweet ID Returns: dict | None: context annotations and entities if available, else None. Reference: https://developer.twitter.com/en/docs/twitter-api/annotations/overview \"\"\" url = f\"https://api.twitter.com/2/tweets/{tweet_id}\" response_json = self._manual_request(url, additional_fields={\"tweet.fields\": [\"context_annotations\", \"entities\"]}) # if key is not awailable, return None if \"context_annotations\" or \"entities\" in response_json[\"data\"]: return response_json[\"data\"] else: return None get_public_metrics Get public metrics from tweet object. Function: TwitterDataFetcher.get_public_metrics(tweet_id: str | int) The function takes in the tweet ID as string or integer representation. The following public metrics are returned: impressions_count (=views) quote_count reply_count retweet_count favorite_count (=likes) Here you can find an interpretation of the metrics: https://developer.twitter.com/en/docs/twitter-api/metrics The function returns the public metrics of the tweet. Therefore, it uses the TwitterDataFetcher._manual_request function. The tweet field for public_metrics is set. If any context annotation or named entity exist, the JSON response of the request is returned, else None . Source Code def get_public_metrics(self, tweet_id: str | int) -> dict: \"\"\"Get public metrics from Tweet Object Args: tweet_id (str | int): Tweet ID Returns: dict: Available public metrics for specified Tweet. Metrics: - impressions_count (=views) - quote_count - reply_count - retweet_count - favorite_count (=likes) Reference: https://developer.twitter.com/en/docs/twitter-api/metrics \"\"\" # set URL url = f\"https://api.twitter.com/2/tweets/{tweet_id}\" # make request response_json = self._manual_request(url, additional_fields={\"tweet.fields\": [\"public_metrics\"]}) # get public metrics from JSON response public_metrics = response_json[\"data\"][\"public_metrics\"] return public_metrics","title":"TwitterDataFetcher"},{"location":"maintenance/TwitterDataFetcher/#twitterdatafetcher","text":"The TwitterDataFetcher class is used to specifically query Twitter data. To simplify the queries to the Twitter API, this software component uses already existing open-source software for interacting with the API, namely the Tweepy Python package . It uses the Tweepy Client class to query the data dictionaries of the Twitter Search API v2 as well as the Tweepy API class to access the data dictionaries based on the Twitter Search API v1. The Twitter Search API v1 is mainly used to query user and tweet objects. Although this API version is partially deprecated, it offers a comparable content to the latest API version and often requires less API calls to receive the same information compared to the v2 API. Additional direct requests to the Twitter Search API v2 are performed, too, using the Python requests library to query endpoints that have been migrated or deprecated in the Tweepy package. This class can also be used to in isolation to collect Twitter data. It requires, therefore, authentication for the Twitter platform. Provide the secrets for the Twitter API and, if desired, for the Botometer API. You can find a list of required secrets in the user guide for the TwitterAPI class. This class has the concern to fetch Twitter data. No further processing is performed within this class.","title":"TwitterDataFetcher"},{"location":"maintenance/TwitterDataFetcher/#initialization","text":"If you want to use this class for data processing or other package components, follow the steps below. Import the TwitterDataFetcher class from the fetch module. from pysna.fetch import TwitterDataFetcher fetcher = TwitterDataFetcher( bearer_token: Any | None = None, consumer_key: Any | None = None, consumer_secret: Any | None = None, access_token: Any | None = None, access_token_secret: Any | None = None, x_rapidapi_key: Any | None = None, x_rapidapi_host: Any | None = None ) and invoke a function: user_id = 123450897612 fetcher.get_latest_activity(user_id) Find the necessary secrets on the user guide instructions .","title":"Initialization"},{"location":"maintenance/TwitterDataFetcher/#methods","text":"","title":"Methods"},{"location":"maintenance/TwitterDataFetcher/#private-methods","text":"","title":"Private Methods"},{"location":"maintenance/TwitterDataFetcher/#manual_request","text":"Performs a manual request to the Twitter API. Returns JSON formatted API response. Function: TwitterDataFetcher._manual_request(url: str, method: str = \"GET\", header: dict | None = None, payload: dict | None = None, additional_fields: Dict[str, List[str]] | None = None) Args: url (str): API URL (without specified fields) method (str): Request method according to REST. Defaults to \"GET\". header : Custom HTTP Header. Defaults to None. payload : JSON data for HTTP requests. Defaults to None. additional_fields (Dict[str, List[str]] | None, optional): Fields can be specified (e.g., tweet.fields) according to the official API reference. Defaults to None. The function will raise an exception if the response status code is unlike 200. With this function, performig manual requests is facilitated as the query string is built by the function based on the provided input arguments. The url argument has to be provided in raw form (i.e., without any parameters or fields). The method argument allows to specify the REST request method (i.e., GET, POST, PUT, DELETE). Defaults to GET. The header argument allows to specify a custom header. This is useful if another API besides the Twitter API is fetched. If no custom header is provided, the default header for the Twitter API authentification is used based on the provided bearer_token during instantiation. The payload argument allows to send data for a POST or PUT request. The data must be provided as a dictionary. The additional_fields argument is used to specify Twitter fields (i.e., user fields or tweet fields) and, thus, enhance the query and return additional information. The argument can be used as follows: {\"tweet.fields\": [\"public_metrics\"]} The function will then build the query string and send it to the API. You can find the full list of Twitter fields in the documentation: https://developer.twitter.com/en/docs/twitter-api/fields Source Code def _manual_request(self, url: str, method: str = \"GET\", header: dict | None = None, payload: dict | None = None, additional_fields: Dict[str, List[str]] | None = None) -> dict: \"\"\"Perform a manual request to the Twitter API. Args: url (str): API URL (without specified fields) method (str): Request method according to REST. Defaults to \"GET\". header (dict | None): Custom HTTP Header. Defaults to None. payload (dict | None): JSON data for HTTP requests. Defaults to None. additional_fields (Dict[str, List[str]] | None, optional): Fields can be specified (e.g., tweet.fields) according to the official API reference. Defaults to None. Raises: Exception: If status code != 200. Returns: dict: JSON formatted response of API request. \"\"\" # if additional_fields were provided if additional_fields: # init empty string fields = \"?\" # create fields string dynamically for every field in additional_fields for field in additional_fields.keys(): # e.g., in format \"tweet.fields=lang,author_id\" fields += f\"{field}={','.join(additional_fields[field])}&\" # append fields to url url += fields[:-1] if header is None: # set header header = {\"Authorization\": f\"Bearer {self._bearer_token}\"} response = requests.request(method=method, url=url, headers=header, json=payload) if response.status_code != 200: raise Exception(\"Request returned an error: {} {}\".format(response.status_code, response.text)) return response.json()","title":"manual_request"},{"location":"maintenance/TwitterDataFetcher/#paginate","text":"Custom pagination function It turns out that the pagination functions from the Tweepy Python packge are considerably slower than doing the pagination manually. For this reason, this function was designed. Function: TwitterDataFetcher._paginate(func, params: Dict[str, str | int], limit: int | None = None, response_attribute: str = \"data\", page_attribute: str | None = None) Args: func : Function used for pagination params (Dict[str, str | int]): Dict containing request parameters. Must be of the form {'id': ..., 'limit': ..., 'pagination_token': ...} limit (int | None, optional): Maximum number of results. Defaults to None, thus, no limit. response_attribute (str, optional): Attribute of the Response object. Defaults to \"data\". Options: [\"data\", \"includes\"] page_attribute (str, optional): The attribute that should be extracted for every entry of a page. Defaults to None. The params argument is used to specify the parameters for the next page. Therefore, an id is needed as well as a key indicating the maximm number of results (i.e., limit ). None indicates that no limit is desired and, thus, all available results will be returned. The pagination_token key can be set to None initially. This pagination token will be reset during iteraion. In case, you wish to start from a different page than the first one, provide a pagination token. All parameters must be provided via a dictionary of the form: {\"id\": 1234456, \"limit\": None, # no limit \"pagination_token\": None} The response_attribute argument specifies where to collect the data from the response. If data is specified, the results are received from the default attribute field of the response. If includes is specified, the results are obtained from the additional information provided by the Twitter fields.` The page_attribute argument specifies what attribute should be extracted for every entry of a page. For instance, if this argument is set to id , then the IDs will be extracted from every entry (e.g., user IDs of user objects). Inside that function, a counter is incremented for every result that has been fetched. If the limit was reached, the function will break out the loop and will return immediately the obtained results. Otherwise, the function will check if last page was reached and will fetch the next page (if available). Source Code def _paginate(self, func, params: Dict[str, str | int], limit: int | None = None, response_attribute: str = \"data\", page_attribute: str | None = None) -> list: \"\"\"Pagination function Args: func: Function used for pagination params (Dict[str, str | int]): Dict containing request parameters. Should be of the form {'id': ..., 'max_results': ..., 'pagination_token': ...} limit (int | None, optional): Maximum number of results. Defaults to None, thus, no limit. response_attribute (str, optional): Attribute of the Response object. Defaults to \"data\". Options: [\"data\", \"includes\"] Raises: KeyError: 'id', 'max_results', and 'pagination_token' should be provided in the params dict. Returns: set: Results \"\"\" # init counter counter = 0 # init empty results set results = list() # set break out var break_out = False while not break_out: # make request response = func(**params) # if any data exists if response.__getattribute__(response_attribute) is not None: # iterate over response results for item in response.__getattribute__(response_attribute): # add result if page_attribute is None: results.append(item) else: results.append(item.__getattribute__(page_attribute)) # increment counter counter += 1 # if limit was reached, break if (limit is not None) and (counter == limit): # set break_out var to true break_out = True break # if last page was reached if \"next_token\" not in response.meta: break # else, set new pagination token for next iteration else: params[\"pagination_token\"] = response.meta[\"next_token\"] # if no data exists, break else: break return results","title":"paginate"},{"location":"maintenance/TwitterDataFetcher/#twitter-user-related-methods","text":"","title":"Twitter user related methods"},{"location":"maintenance/TwitterDataFetcher/#get_user_object","text":"Request Twitter user object using Tweepy. The user object is fetched from the Twitter Search API v1. For this, the Tweepy API class is used. Function: TwitterDataFetcher.get_user_object(user: str | int) The function takes in either the user ID as string or integer or the user's unique screen name. It returns the requested API v1 user object. The function handles the performed request based on what user identifier was given. If the requested user has been suspended from Twitter, an error will be returned and a messeage will be logged to stdout. Source Code def get_user_object(self, user: str | int) -> tweepy.models.User: \"\"\"Request Twitter User Object via tweepy Args: user (str): Either User ID or screen name Returns: tweepy.User: Twitter User object from tweepy \"\"\" try: # check if string for user1 is convertible to int in order to check for user ID or screen name if (isinstance(user, int)) or (user.isdigit()): # get profile for user by user ID user_obj = self.api.get_user(user_id=user) else: # get profile for user by screen name user_obj = self.api.get_user(screen_name=user) except tweepy.errors.Forbidden as e: # log to stdout log.error(\"403 Forbidden: access refused or access is not allowed.\") # if user ID was provided if user.isdigit() or isinstance(user, int): url = f\"https://api.twitter.com/2/users/{user}\" else: # if screen name was provided url = f\"https://api.twitter.com/2/users/by/username/{user}\" response = self._manual_request(url) # if an error occured that says the user has been suspended if any(\"User has been suspended\" in error[\"detail\"] for error in response[\"errors\"]): log.error(\"User has been suspended from Twitter. Requested user: {}\".format(user)) raise e else: raise e return user_obj","title":"get_user_object"},{"location":"maintenance/TwitterDataFetcher/#get_user_follower_ids","text":"Request Twitter follower IDs from user. Function: TwitterDataFetcher.get_user_follower_ids(user: str | int) This function takes in a Twitter user identifier (either ID or unique screen name). It returns all follower user IDs from the specified user as a set. Here, the ``tweepy.Cursor```is used for pagination. The function handles the performed request based on what user identifier was given. Source Code def get_user_follower_ids(self, user: str | int) -> Set[int]: \"\"\"Request Twitter follower IDs from user Args: user (str | int): Either User ID or screen name. Returns: Set[int]: Array containing follower IDs \"\"\" # check if string for user1 is convertible to int in order to check for user ID or screen name if (isinstance(user, int)) or (user.isdigit()): params = {\"user_id\": user} else: params = {\"screen_name\": user} follower_ids = list() for page in tweepy.Cursor(self.api.get_follower_ids, **params).pages(): follower_ids.extend(page) return set(follower_ids)","title":"get_user_follower_ids"},{"location":"maintenance/TwitterDataFetcher/#get_user_followee_ids","text":"Request Twitter followee IDs from user. Function: TwitterDataFetcher.get_user_followee_ids(user: str | int) This function takes in a Twitter user identifier (i.e., either ID or unique screen name) and returns a set containing all IDs from the user's followees (AKA friends or follows). The function handles the performed request based on what user identifier was given. Source Code def get_user_followee_ids(self, user: str | int) -> Set[int]: \"\"\"Request Twitter followee IDs from user Args: user (str): Either User ID or screen name. Returns: Set[int]: Array containing follow IDs \"\"\" # check if string for user1 is convertible to int in order to check for user ID or screen name if (isinstance(user, int)) or (user.isdigit()): params = {\"user_id\": user} else: params = {\"screen_name\": user} followee_ids = list() for page in tweepy.Cursor(self.api.get_friend_ids, **params).pages(): followee_ids.extend(page) return set(followee_ids)","title":"get_user_followee_ids"},{"location":"maintenance/TwitterDataFetcher/#get_latest_activity","text":"Returns latest user's activity by fetching the top element from its timeline. Function: TwitterDataFetcher.get_latest_activity(user: str | int) This function takes in a Twitter user identifier (i.e., either ID or unique screen name) and returns the latest activity from the user's timeline. Therefore, the _manual_request function is used to request the corresponding endpoint . Often, this will be a tweet composed by the user itself. Then, all available data of that tweet will be returned as a dictionary. The function handles the performed request based on what user identifier was given. Source Code def get_latest_activity(self, user: str | int) -> dict: \"\"\"Returns latest user's activity by fetching the top element from its timeline. Args: user (str | int): User ID or screen name. Returns: dict: Latest activity. \"\"\" # if screen name was provided if (isinstance(user, str)) and (user.isdigit() is False): url = f\"https://api.twitter.com/1.1/statuses/user_timeline.json?screen_name={user}&include_rts=true&trim_user=true&tweet_mode=extended\" # else go with user ID else: url = f\"https://api.twitter.com/1.1/statuses/user_timeline.json?user_id={user}&include_rts=true&trim_user=true&tweet_mode=extended\" response_json = self._manual_request(url) # return the first item since timeline is sorted descending return response_json[0]","title":"get_latest_activity"},{"location":"maintenance/TwitterDataFetcher/#get_latest_activity_date","text":"Get latest activity date from specified user by fetching the top element from its timeline and extract the creation date. Function: TwitterDataFetcher.get_latest_activity_date(user: str | int) This function takes in a Twitter user identifier (i.e., either ID or unique screen name) and returns the latest activity date from the user's timeline. Therefore, the _manual_request function is used to request the corresponding endpoint . The latest activity date is determined by fetching the latest activity from the user's timeline first, and then extracting the creation date. Usually, this will be a tweet composed by the user. If this is the case, the creation date of that tweet will be returned, representing the latest public available activity date. Source Code def get_latest_activity_date(self, user: str | int) -> str: \"\"\"Get latest activity date from specified user by fetching the top element from its timeline. Args: user (str | int): User ID or screen name. Returns: str: Activity date of latest activity. \"\"\" # if screen name was provided if (isinstance(user, str)) and (user.isdigit() is False): url = f\"https://api.twitter.com/1.1/statuses/user_timeline.json?screen_name={user}&include_rts=true&trim_user=true\" # else go with user ID else: url = f\"https://api.twitter.com/1.1/statuses/user_timeline.json?user_id={user}&include_rts=true&trim_user=true\" response_json = self._manual_request(url) # return the first item since timeline is sorted descending return response_json[0][\"created_at\"]","title":"get_latest_activity_date"},{"location":"maintenance/TwitterDataFetcher/#get_relationship","text":"Get relationship between two Twitter users. Function: TwitterDataFetcher.get_relationship(source_user: str | int, target_user: str | int) The function takes in a source and a target user identifier. It uses the Tweepy.API.get_friendship function to get the relationship. Therefore, this function handles the performed query based on the provided user identifiers. The function will return the parsed JSON relationship for the source and target user as a dictionary. Source Code def get_relationship(self, source_user: str | int, target_user: str | int) -> dict: \"\"\"Get relationship between two users. Args: user1 (str | int): Source user ID or screen name. user2 (str | int): Target user ID or screen name. Returns: dict: Unpacked tuple of JSON from tweepy Friendship model. Reference: https://developer.twitter.com/en/docs/twitter-api/v1/accounts-and-users/follow-search-get-users/api-reference/get-friendships-show#example-response \"\"\" params = {\"source_id\": None, \"source_screen_name\": None, \"target_id\": None, \"target_screen_name\": None} # if source_user is int or a digit if (isinstance(source_user, int)) or (isinstance(source_user, str) and (source_user.isdigit())): params[\"source_id\"] = source_user # else if screen name was provided elif (isinstance(source_user, str)) and (not source_user.isdigit()): params[\"source_screen_name\"] = source_user else: log.error(\"No ID or username provided for {}\".format(source_user)) # if target_user is int or a digit if (isinstance(target_user, int)) or (isinstance(target_user, str) and (target_user.isdigit())): params[\"target_id\"] = target_user # else if screen name was provided elif (isinstance(target_user, str)) and (not target_user.isdigit()): params[\"target_screen_name\"] = target_user else: log.error(\"No ID or username provided for {}\".format(target_user)) relationship = self.api.get_friendship(**params) return {\"source\": relationship[0]._json, \"target\": relationship[1]._json}","title":"get_relationship"},{"location":"maintenance/TwitterDataFetcher/#get_relationship_pairs","text":"Creates pairs for each uniqie combination of provided users based on their relationship. Function: TwitterDataFetcher.get_relationship_pairs(users: List[str | int]) This function takes in a list of user identifiers (i.e., IDs or unique screen names). It will create a pair of each combination of the provided users and returns their individual relationships. For instance, if three users WWU_Muenster , goetheuni , UniKonstanz were provided, the pairs are determined as follows: ( WWU_Muenster , goetheuni ) ( WWU_Muenster , UniKonstanz ) ( goetheuni , WWU_Muenster ) ( goetheuni , UniKonstanz ) ( UniKonstanz , WWU_Muenster ) ( UniKonstanz , goehteuni ) These pairs are set as dictionary keys. The respective relationships are stored as dictionary values. Source Code def get_relationship_pairs(self, users: List[str | int]) -> dict: \"\"\"Creates pairs for each unique combination of provided users based on their relationship. Args: users (List[str | int]): List of user IDs or screen names. Returns: dict: Pairs of users containing their relationship to each other. \"\"\" # init emtpy relationships dict relationships = dict() # iterate over every pair combination of provided users for user in users: for other_user in users: if user != other_user: relationships[(user, other_user)] = self.get_relationship(source_user=user, target_user=other_user) return relationships","title":"get_relationship_pairs"},{"location":"maintenance/TwitterDataFetcher/#get_liked_tweets_ids","text":"Get (all) liked tweet IDs of the provided user. Function: TwitterDataFetcher.get_liked_tweets_ids(user: str | int, limit: int | None = None) Args: user (str | int): User ID or screen name. limit (int | None): The maximum number of results to be returned. By default, each page will return the maximum number of results available. This function uses the custom TwitterDataFetcher._paginate function to get the specified number of results. To get the tweet IDs, the tweepy.Client.get_liked_tweets function is used. The function wil return a Python set of the IDs of the liked tweets by the user. The function handles the performed request based on what user identifier was given. Source Code def get_liked_tweets_ids(self, user: str | int, limit: int | None = None) -> list(): \"\"\"Get (all) liked Tweets of provided user. Args: user (str | int): User ID or screen name. limit (int | None): The maximum number of results to be returned. By default, each page will return the maximum number of results available. Returns: Set[int]: Tweet Objects of liked Tweets. \"\"\" # if user ID was provided if (isinstance(user, int)) or (user.isdigit()): params = {\"id\": user, \"max_results\": 100, \"pagination_token\": None} else: user_obj = self.get_user_object(user) params = {\"id\": user_obj.id, \"max_results\": 100, \"pagination_token\": None} page_results = self._paginate(self.client.get_liked_tweets, params, limit=limit, page_attribute=\"id\") return page_results","title":"get_liked_tweets_ids"},{"location":"maintenance/TwitterDataFetcher/#get_composed_tweets_ids","text":"Get (all) composed tweet IDs of provided user by pagination. Function: TwitterDataFetcher.get_composed_tweets_ids(user: str | int, limit: int | None = None) Args: user (str | int): User ID or screen name. limit (int | None): The maximum number of results to be returned. By default, each page will return the maximum number of results available. This function uses the custom TwitterDataFetcher._paginate function to get the specified number of results. To get the tweet IDs, the tweepy.Client.get_users_tweets function is used. The function wil return a Python set of the IDs of the composed tweets by the user. The function handles the performed request based on what user identifier was given. Source Code def get_composed_tweets_ids(self, user: str | int, limit: int | None = None) -> list: \"\"\"Get (all) composed Tweets of provided user by pagination. Args: user (str | int): User ID or screen name. limit (int | None): The maximum number of results to be returned. By default, each page will return the maximum number of results available. Returns: list: Tweet Objects of composed Tweets. \"\"\" # user ID is required, if screen name was provided if (isinstance(user, str)) and (not user.isdigit()): user = self.get_user_object(user).id # set params params = {\"id\": user, \"max_results\": 100, \"pagination_token\": None} # get page results page_results = self._paginate(self.client.get_users_tweets, params, limit=limit, page_attribute=\"id\") return page_results","title":"get_composed_tweets_ids"},{"location":"maintenance/TwitterDataFetcher/#get_botometer_scores","text":"Returns bot scores from the Botometer API for the specified Twitter account. Function: TwitterDataFetcher.get_botometer_scores(user: str | int) This function takes in a Twitter account identifier (i.e., ID or unique screen name.) This function relies on the external Botometer API . To use this function, the corresponding RapidAPI secrets need to be provided. See the secrets overview for more details. The function gets the user's timeline first and takes the latest 100 tweets from its timeline. Then, this data is send via the `payload argument of the TwitterDataFetcher._manual_request function using a POST request. Then, the JSON response is returned. Source Code def get_botometer_scores(self, user: str | int) -> dict: \"\"\"Returns bot scores from the Botometer API for the specified Twitter user. Args: user (str | int): User ID or screen name. Returns: dict: The raw Botometer scores for the specified user. Reference: https://rapidapi.com/OSoMe/api/botometer-pro/details \"\"\" if (self._x_rapidapi_key is None) or (self._x_rapidapi_host is None): raise ValueError(\"'X_RAPIDAPI_KEY' and 'X_RAPIDAPI_HOST' secrets for Botometer API need to be provided.\") # get user object user_obj = self.get_user_object(user) # get user timeline timeline = list(map(lambda x: x._json, self.api.user_timeline(user_id=user_obj.id, count=200))) # get user data if timeline: user_data = timeline[0][\"user\"] else: user_data = user_obj._json screen_name = \"@\" + user_data[\"screen_name\"] # get latest 100 Tweets tweets = list(map(lambda x: x._json, self.api.search_tweets(screen_name, count=100))) # set payload payload = {\"mentions\": tweets, \"timeline\": timeline, \"user\": user_data} # set header headers = {\"content-type\": \"application/json\", \"X-RapidAPI-Key\": self._x_rapidapi_key, \"X-RapidAPI-Host\": self._x_rapidapi_host} # set url url = \"https://botometer-pro.p.rapidapi.com/4/check_account\" # get results response = self._manual_request(url, \"POST\", headers, payload) return response","title":"get_botometer_scores"},{"location":"maintenance/TwitterDataFetcher/#tweet-related-methods","text":"","title":"Tweet related methods"},{"location":"maintenance/TwitterDataFetcher/#get_tweet_object","text":"Request Twitter tweet object via tweepy. Function: TwitterDataFetcher.get_tweet_object(tweet: str | int) The function takes in either the tweet ID as string or integer. It returns the extended tweet object requested via the API v1 using the tweepy.API.get_status function. If the requested tweet object has been deleted, an error will be returned and a messeage will be logged to stdout. Reference: https://developer.twitter.com/en/docs/twitter-api/v1/data-dictionary/object-model/tweet Source Code def get_tweet_object(self, tweet: str | int) -> tweepy.models.Status: \"\"\"Request Twitter Tweet Object via tweepy Args: tweet (int | str): Tweet ID Returns: tweepy.models.Status: tweepy Status Model Reference: https://developer.twitter.com/en/docs/twitter-api/v1/data-dictionary/object-model/tweet \"\"\" try: tweet_obj = self.api.get_status(tweet, include_entities=True, tweet_mode=\"extended\") except tweepy.errors.NotFound as e: log.error(\"404 Not Found: Resource not found.\") raise e except tweepy.errors.Forbidden as e: log.error(\"403 Forbidden: access refused or access is not allowed.\") raise e return tweet_obj","title":"get_tweet_object"},{"location":"maintenance/TwitterDataFetcher/#get_liking_users_ids","text":"Get (all) liking users of provided tweet by pagination. Function: TwitterDataFetcher.get_liking_users_ids(tweet_id: str | int, limit: int | None = None) Args: tweet (str | int): Tweet ID. limit (int | None): The maximum number of results to be returned. By default, each page will return the maximum number of results available. The function takes in the tweet ID as string or integer representation as well as the limit argument. If limit is none, all available results will be returned. It returns the user IDs of the users that liked the specified tweet. This function uses the custom TwitterDataFetcher._paginate function to get the specified number of results. To get the user IDs, the tweepy.Client.get_liking_users function is used. Source Code def get_liking_users_ids(self, tweet_id: str | int, limit: int | None = None) -> list: \"\"\"Get (all) liking users of provided Tweet by pagination. Args: tweet (str | int): Tweet ID. limit (int | None): The maximum number of results to be returned. By default, each page will return the maximum number of results available. Returns: Set[int]: User Objects as list. \"\"\" # set params params = {\"id\": tweet_id, \"max_results\": 100, \"pagination_token\": None} # get page results page_results = self._paginate(self.client.get_liking_users, params, limit=limit, page_attribute=\"id\") return page_results","title":"get_liking_users_ids"},{"location":"maintenance/TwitterDataFetcher/#get_retweeters_ids","text":"Get (all) retweeting users of provided tweet by pagination. Function: TwitterDataFetcher.get_retweeters_ids(tweet_id: str | int, limit: int | None = None) Args: tweet (str | int): Tweet ID. limit (int | None): The maximum number of results to be returned. By default, each page will return the maximum number of results available. The function takes in the tweet ID as string or integer representation as well as the limit argument. If limit is none, all available results will be returned. It returns the user IDs of the users that retweeted the specified tweet. This function uses the custom TwitterDataFetcher._paginate function to get the specified number of results. To get the user IDs, the tweepy.Client.get_retweeters function is used. Source Code def get_retweeters_ids(self, tweet_id: str | int, limit: int | None = None) -> list: \"\"\"Get (all) retweeting users of provided Tweet by pagination. Args: tweet (str | int): Tweet ID. limit (int | None): The maximum number of results to be returned. By default, each page will return the maximum number of results available. Returns: Set[int]: User Objects of retweeting users. \"\"\" params = {\"id\": tweet_id, \"max_results\": 100, \"pagination_token\": None} # get page results page_results = self._paginate(self.client.get_retweeters, params, limit=limit, page_attribute=\"id\") return page_results","title":"get_retweeters_ids"},{"location":"maintenance/TwitterDataFetcher/#get_quoting_users_ids","text":"Get (all) quoting users of provided Tweet by pagination. Function: TwitterDataFetcher.get_quoting_users_ids(tweet_id: str | int, limit: int | None = None) Args: tweet_id (str | int): Tweet ID. limit (int | None): The maximum number of results to be returned. By default, each page will return the maximum number of results available. The function takes in the tweet ID as string or integer representation as well as the limit argument. If limit is none, all available results will be returned. It returns the user IDs of the users that quoted the specified tweet. This function uses the custom TwitterDataFetcher._paginate function to get the specified number of results. To get the tweet objects, the tweepy.Client.get_quote_tweets function is used. Then, the quoting users IDs are extracted from the additional information provided within the includes fields of each page. For more details, see the instructions on the TwitterDataFetcher._paginate function Source Code def get_quoting_users_ids(self, tweet_id: str | int, limit: int | None = None) -> list: \"\"\"Get (all) quoting users of provided Tweet by pagination. Args: tweet_id (str | int): Tweet ID. limit (int | None): The maximum number of results to be returned. By default, each page will return the maximum number of results available. Returns: list: User Objects of quoting users. \"\"\" params = {\"id\": tweet_id, \"max_results\": 100, \"pagination_token\": None} # get page results page_results = self._paginate(self.client.get_quote_tweets, params, limit=limit, response_attribute=\"includes\", page_attribute=\"id\") return page_results","title":"get_quoting_users_ids"},{"location":"maintenance/TwitterDataFetcher/#get_context_annotations_and_entities","text":"Get context annotations and entities from a tweet object. Function: TwitterDataFetcher.get_context_annotations_and_entities(tweet_id: str | int) The function takes in the tweet ID as string or integer representation. The function returns the context annotations (e.g., topics) and named entities of the specified tweet. Therefore, it uses the TwitterDataFetcher._manual_request function. The tweet fields for context_annotations and entities are set. If any context annotation or named entity exist, the JSON response of the request is returned, else None . Reference: https://developer.twitter.com/en/docs/twitter-api/annotations/overview Source Code def get_context_annotations_and_entities(self, tweet_id: str | int) -> dict | None: \"\"\"Get context annotations and entities from a Tweet. Args: tweet_id (str | int): Tweet ID Returns: dict | None: context annotations and entities if available, else None. Reference: https://developer.twitter.com/en/docs/twitter-api/annotations/overview \"\"\" url = f\"https://api.twitter.com/2/tweets/{tweet_id}\" response_json = self._manual_request(url, additional_fields={\"tweet.fields\": [\"context_annotations\", \"entities\"]}) # if key is not awailable, return None if \"context_annotations\" or \"entities\" in response_json[\"data\"]: return response_json[\"data\"] else: return None","title":"get_context_annotations_and_entities"},{"location":"maintenance/TwitterDataFetcher/#get_public_metrics","text":"Get public metrics from tweet object. Function: TwitterDataFetcher.get_public_metrics(tweet_id: str | int) The function takes in the tweet ID as string or integer representation. The following public metrics are returned: impressions_count (=views) quote_count reply_count retweet_count favorite_count (=likes) Here you can find an interpretation of the metrics: https://developer.twitter.com/en/docs/twitter-api/metrics The function returns the public metrics of the tweet. Therefore, it uses the TwitterDataFetcher._manual_request function. The tweet field for public_metrics is set. If any context annotation or named entity exist, the JSON response of the request is returned, else None . Source Code def get_public_metrics(self, tweet_id: str | int) -> dict: \"\"\"Get public metrics from Tweet Object Args: tweet_id (str | int): Tweet ID Returns: dict: Available public metrics for specified Tweet. Metrics: - impressions_count (=views) - quote_count - reply_count - retweet_count - favorite_count (=likes) Reference: https://developer.twitter.com/en/docs/twitter-api/metrics \"\"\" # set URL url = f\"https://api.twitter.com/2/tweets/{tweet_id}\" # make request response_json = self._manual_request(url, additional_fields={\"tweet.fields\": [\"public_metrics\"]}) # get public metrics from JSON response public_metrics = response_json[\"data\"][\"public_metrics\"] return public_metrics","title":"get_public_metrics"},{"location":"maintenance/TwitterDataProcessor/","text":"TwitterDataProcessor The TwitterDataProcessor has the purpose to process Twitter-specific data and respective data dictionaries (i.e., user or tweet data dictionaries). This class is used inside the TwitterAPI class as a component class through composition. This class can also be used to process previously collected data. It requires no authentication for the Twitter platform and, thus, can be used in isolation. This class has a separated concern compared to the other package's classes, namely to process Twitter-related data. Initialization If you want to use this class for data processing or other package components, follow the steps below. Import the TwitterDataProcessor class from the process module. from pysna.process import TwitterDataProcessor data_processor = TwitterDataProcessor() and invoke a function: tweet = \"Savage Love \ud83c\udfb6 #SavageLoveRemix\" data_processor.clean_tweet(tweet) Methods extract_followers Extract IDs, names, and screen names from a user's followers. This function takes in a Tweepy user object from the v1 API version and returns a dictionary containing the extracted information. Function: TwitterDataProcessor.extract_followers(user_object: tweepy.User) This function will return a dictionary of the form {\"followers_ids\": [], \"followers_names\": [], \"followers_screen_names\": []} NOTE : This function needs a recently fetched Twitter user object from the API v1. Stored user objects (e.g., using the pickle module) that are to be analyzed later will lead to an error. Source Code def extract_followers(self, user_object: tweepy.User) -> Dict[str, str | int]: \"\"\"Extract IDs, names, and screen names from a user's followers. Args: user_object (tweepy.User): Tweepy User Object. Returns: Dict[str, str | int]: Dictionary containing IDs, names, and screen names. \"\"\" info = {\"followers_ids\": list(), \"followers_names\": list(), \"followers_screen_names\": list()} # extract follower IDs info[\"followers_ids\"] = user_object.follower_ids() # extract names and screen names for follower in user_object.followers(): info[\"followers_names\"].append(follower.name) info[\"followers_screen_names\"].append(follower.screen_name) return info extract_followees Extract IDs, names, and screen names from a user's followees (i.e., their follows). This function takes in a Tweepy user object from the v1 API version and returns a dictionary containing the extracted information. Function: TwitterDataProcessor.extract_followees(user_object: tweepy.User) This function will return a dictionary of the form {\"followees_ids\": [], \"followees_names\": [], \"followees_screen_names\": []} NOTE : This function needs a recently fetched Twitter user object from the API v1. Stored user objects (e.g., using the pickle module) that are to be analyzed later will lead to an error. Source Code def extract_followees(self, user_object: tweepy.User) -> Dict[str, str | int]: \"\"\"Extract IDs, names, and screen names from a user's followees. Args: user_object (tweepy.User): Tweepy User Object. Returns: Dict[str, str | int]: Dictionary containing IDs, names, and screen names. \"\"\" info = {\"followees_ids\": list(), \"followees_names\": list(), \"followees_screen_names\": list()} # extract IDs, names and screen names for followee in user_object.friends(): info[\"followees_ids\"].append(followee.id) info[\"followees_names\"].append(followee.name) info[\"followees_screen_names\"].append(followee.screen_name) return info clean_tweet Utility function to clean tweet text by removing links, special characters using simple regex statements. It takes in the raw text of a tweet. Function: TwitterDataProcessor.clean_tweet(tweet: str) This function is used before the detect_tweet_sentiment function. Thus, the tweet is cleaned first and then its sentiment is determined. Both functions are used in combination within the TwitterAPI class. Source Code def clean_tweet(self, tweet: str) -> str: \"\"\"Utility function to clean tweet text by removing links, special characters using simple regex statements. Args: tweet (str): Raw text of the Tweet. Returns: str: Cleaned Tweet \"\"\" return \" \".join(re.sub(r\"(@[A-Za-z0-9]+)|([^0-9A-Za-z \\t])|(\\w+:\\/\\/\\S+)\", \" \", tweet).split()) detect_tweet_sentiment Utility function to classify sentiment of passed tweet using vader sentiment analyzer. English Tweets only. The function takes in the text of a tweet (cleaned from special characters, linkes, emojis, etc.) and will return the tweet sentiment as well as the polarity scores. Function: TwitterDataProcessor.detect_tweet_sentiment(tweet: str) For sentiment detection, the Vader sentiment analyzer is used as this one turned out to be more accurate for tweets compared to NLTK sentiment analyzers. The function will return a dictionary containing the label of the sentiment (i.e., positive, neutral, or negative) and the polarity scores: {\"label\": label, \"polarity_scores\": polarity_score} Source Code def detect_tweet_sentiment(self, tweet: str) -> dict: \"\"\"Utility function to classify sentiment of passed tweet using textblob's sentiment method. English Tweets only. Args: tweet (str): The raw text of the Tweet. Returns: str: the sentiment of the Tweet (either positive, neutral, or negative) and the polarity scores. \"\"\" # create VADER instance analyser = SentimentIntensityAnalyzer() # get polarity scores from cleaned tweet polarity_scores = analyser.polarity_scores(self.clean_tweet(tweet)) # define label if polarity_scores[\"compound\"] >= 0.05: label = \"positive\" elif polarity_scores[\"compound\"] <= -0.05: label = \"negative\" else: label = \"neutral\" # return label and polarity scores return {\"label\": label, \"polarity_scores\": polarity_scores} calc_similarity This function is used to calculate the similarity between multiple user or tweet objects. The function takes in either a list of user objects or a list of public tweet metrics as well as a features list. Either user objects or tweet metrics need to be provided, not both. The user objects must be recently fetched from the Twitter API v1. A stored object (e.g., by using the pickle Python module) will not have the necessary properties to be resolved by this function. Otherwise, an error will be returned. The similarity is calculated based on a feature vector containing numeric values. Thus, for a given set of user or tweet attributes, the features must be provided on which the similarity will be computed. As a distance measure and, thus, the similarity of feature vectors, the vector norm of second order will be calculated which is equivalent to the euclidean distance. Therefore, the numpy.linalg.norm function is used. The smaller the distance, the more similar the two vectors are. The function will determine the distance between a distinct pair of user or tweet objects. For instance, when three user objects for the Twitter accounts 12355 , 734231 , 9083468 are provided, the following output will be generated: {(12355, 734231): 4567.098, (12355, 9083468): 5980.076, (734231, 9083468): 8763.32} The output dictionary contains the distinct pairs of objects as a tuple as dictionary keys. The distances for each distinct pair is given as dictionary value. The output is sorted in ascending order. Hence, the minimal distance and, thus, the most similar pair is provided as first dictionary entry. Function: TwitterDataProcessor.calc_similarity(user_objs: List[dict] | None = None, tweet_metrics: List[Dict[int, dict]] | None = None, *, features: List[str]) Args: user_objs (List[dict] | None, optional): List of serialized Twitter user objects from Twitter Search API v1. Defaults to None. tweet_metrics (List[Dict[int | dict]] | None, optional): List of public Tweet metrics as dictionaries with Tweet IDs as keys. Defaults to None. features (List[str]): Features that should be contained in the feature vector. Features have to be numeric and must belong to the respective object (i.e., user or tweet.) The features that can be provided for the features list can be found in the detailed description of the attributes for the compare_tweets function and the detailed description of the attributes for the compare_users function . The implementation design of this function allows a comparison of Twitter users or tweets based on the available metrics. The implementation was inspired by the characterics of social bots on Twitter as they often have a similar number of followers or followees and their posted tweets often have a similar number of likes. Thus, the calculated similarities might help to identify bot-like behavior of Twitter accounts as well as identify deviations from normal Twitter accounts. If their similarities are small, they are likely to have a similar behavior on Twitter (i.e., a bot could be analyzed). Source Code def calc_similarity(self, user_objs: List[dict] | None = None, tweet_metrics: List[Dict[int, dict]] | None = None, *, features: List[str]) -> dict: \"\"\"Calculates the euclidean distance of users/tweets based on a feature vector. Either user objects or Tweet objects must be specified, not both. Args: user_objs (List[dict] | None, optional): List of serialized Twitter user objects from Twitter Search API v1. Defaults to None. tweet_metrics (List[Dict[int | dict]] | None, optional): List of public Tweet metrics as dictionaries with Tweet IDs as keys. Defaults to None. features (List[str]): Features that should be contained in the feature vector. Features have to be numeric and must belong to the respective object (i.e., user or tweet.) Raises: ValueError: If either 'user_objs' and 'tweet_objs' or none of them were provided. AssertionError: If non-numeric feature was provided in the 'features' list. Returns: dict: Unique pair of users/tweets containing the respective euclidean distance. Sorted in ascending order. \"\"\" # init empty dict to store distances distances = dict() # if users and tweets were provided if user_objs and tweet_metrics: raise ValueError(\"Either 'user_objs' or 'tweet_metrics' must be specified, not both.\") # if only user_objs were provided elif user_objs: # iterate over every uniqe pair for i in range(len(user_objs)): for j in range(i + 1, len(user_objs)): # get user objects for each pair user_i = user_objs[i] user_j = user_objs[j] # build feature vector vec_i = np.array([user_i[feature] for feature in features]) vec_j = np.array([user_j[feature] for feature in features]) # feature vectors have to contain numeric values assert all(isinstance(feat, Number) for feat in vec_i), \"only numeric features are allowed\" assert all(isinstance(feat, Number) for feat in vec_j), \"only numeric features are allowed\" # calc euclidean distance distances[(user_i[\"id\"], user_j[\"id\"])] = np.linalg.norm(vec_i - vec_j, ord=2) elif tweet_metrics: # iterate over every uniqe pair for i in range(len(tweet_metrics)): for j in range(i + 1, len(tweet_metrics)): # get Tweet objects for each pair tweet_i = list(tweet_metrics.values())[i] tweet_j = list(tweet_metrics.values())[j] # build feature vector vec_i = np.array([tweet_i[feature] for feature in features]) vec_j = np.array([tweet_j[feature] for feature in features]) # feature vectors have to contain numeric values assert all(isinstance(feat, Number) for feat in vec_i), \"only numeric features are allowed\" assert all(isinstance(feat, Number) for feat in vec_j), \"only numeric features are allowed\" # calc euclidean distance distances[(list(tweet_metrics.keys())[i], list(tweet_metrics.keys())[j])] = np.linalg.norm(vec_i - vec_j, ord=2) # if none was provided else: raise ValueError(\"Either 'user_objs' or 'tweet_metrics' must be provided.\") # sort dict in ascendin order sorted_values = dict(sorted(distances.items(), key=operator.itemgetter(1))) return sorted_values","title":"TwitterDataProcessor"},{"location":"maintenance/TwitterDataProcessor/#twitterdataprocessor","text":"The TwitterDataProcessor has the purpose to process Twitter-specific data and respective data dictionaries (i.e., user or tweet data dictionaries). This class is used inside the TwitterAPI class as a component class through composition. This class can also be used to process previously collected data. It requires no authentication for the Twitter platform and, thus, can be used in isolation. This class has a separated concern compared to the other package's classes, namely to process Twitter-related data.","title":"TwitterDataProcessor"},{"location":"maintenance/TwitterDataProcessor/#initialization","text":"If you want to use this class for data processing or other package components, follow the steps below. Import the TwitterDataProcessor class from the process module. from pysna.process import TwitterDataProcessor data_processor = TwitterDataProcessor() and invoke a function: tweet = \"Savage Love \ud83c\udfb6 #SavageLoveRemix\" data_processor.clean_tweet(tweet)","title":"Initialization"},{"location":"maintenance/TwitterDataProcessor/#methods","text":"","title":"Methods"},{"location":"maintenance/TwitterDataProcessor/#extract_followers","text":"Extract IDs, names, and screen names from a user's followers. This function takes in a Tweepy user object from the v1 API version and returns a dictionary containing the extracted information. Function: TwitterDataProcessor.extract_followers(user_object: tweepy.User) This function will return a dictionary of the form {\"followers_ids\": [], \"followers_names\": [], \"followers_screen_names\": []} NOTE : This function needs a recently fetched Twitter user object from the API v1. Stored user objects (e.g., using the pickle module) that are to be analyzed later will lead to an error. Source Code def extract_followers(self, user_object: tweepy.User) -> Dict[str, str | int]: \"\"\"Extract IDs, names, and screen names from a user's followers. Args: user_object (tweepy.User): Tweepy User Object. Returns: Dict[str, str | int]: Dictionary containing IDs, names, and screen names. \"\"\" info = {\"followers_ids\": list(), \"followers_names\": list(), \"followers_screen_names\": list()} # extract follower IDs info[\"followers_ids\"] = user_object.follower_ids() # extract names and screen names for follower in user_object.followers(): info[\"followers_names\"].append(follower.name) info[\"followers_screen_names\"].append(follower.screen_name) return info","title":"extract_followers"},{"location":"maintenance/TwitterDataProcessor/#extract_followees","text":"Extract IDs, names, and screen names from a user's followees (i.e., their follows). This function takes in a Tweepy user object from the v1 API version and returns a dictionary containing the extracted information. Function: TwitterDataProcessor.extract_followees(user_object: tweepy.User) This function will return a dictionary of the form {\"followees_ids\": [], \"followees_names\": [], \"followees_screen_names\": []} NOTE : This function needs a recently fetched Twitter user object from the API v1. Stored user objects (e.g., using the pickle module) that are to be analyzed later will lead to an error. Source Code def extract_followees(self, user_object: tweepy.User) -> Dict[str, str | int]: \"\"\"Extract IDs, names, and screen names from a user's followees. Args: user_object (tweepy.User): Tweepy User Object. Returns: Dict[str, str | int]: Dictionary containing IDs, names, and screen names. \"\"\" info = {\"followees_ids\": list(), \"followees_names\": list(), \"followees_screen_names\": list()} # extract IDs, names and screen names for followee in user_object.friends(): info[\"followees_ids\"].append(followee.id) info[\"followees_names\"].append(followee.name) info[\"followees_screen_names\"].append(followee.screen_name) return info","title":"extract_followees"},{"location":"maintenance/TwitterDataProcessor/#clean_tweet","text":"Utility function to clean tweet text by removing links, special characters using simple regex statements. It takes in the raw text of a tweet. Function: TwitterDataProcessor.clean_tweet(tweet: str) This function is used before the detect_tweet_sentiment function. Thus, the tweet is cleaned first and then its sentiment is determined. Both functions are used in combination within the TwitterAPI class. Source Code def clean_tweet(self, tweet: str) -> str: \"\"\"Utility function to clean tweet text by removing links, special characters using simple regex statements. Args: tweet (str): Raw text of the Tweet. Returns: str: Cleaned Tweet \"\"\" return \" \".join(re.sub(r\"(@[A-Za-z0-9]+)|([^0-9A-Za-z \\t])|(\\w+:\\/\\/\\S+)\", \" \", tweet).split())","title":"clean_tweet"},{"location":"maintenance/TwitterDataProcessor/#detect_tweet_sentiment","text":"Utility function to classify sentiment of passed tweet using vader sentiment analyzer. English Tweets only. The function takes in the text of a tweet (cleaned from special characters, linkes, emojis, etc.) and will return the tweet sentiment as well as the polarity scores. Function: TwitterDataProcessor.detect_tweet_sentiment(tweet: str) For sentiment detection, the Vader sentiment analyzer is used as this one turned out to be more accurate for tweets compared to NLTK sentiment analyzers. The function will return a dictionary containing the label of the sentiment (i.e., positive, neutral, or negative) and the polarity scores: {\"label\": label, \"polarity_scores\": polarity_score} Source Code def detect_tweet_sentiment(self, tweet: str) -> dict: \"\"\"Utility function to classify sentiment of passed tweet using textblob's sentiment method. English Tweets only. Args: tweet (str): The raw text of the Tweet. Returns: str: the sentiment of the Tweet (either positive, neutral, or negative) and the polarity scores. \"\"\" # create VADER instance analyser = SentimentIntensityAnalyzer() # get polarity scores from cleaned tweet polarity_scores = analyser.polarity_scores(self.clean_tweet(tweet)) # define label if polarity_scores[\"compound\"] >= 0.05: label = \"positive\" elif polarity_scores[\"compound\"] <= -0.05: label = \"negative\" else: label = \"neutral\" # return label and polarity scores return {\"label\": label, \"polarity_scores\": polarity_scores}","title":"detect_tweet_sentiment"},{"location":"maintenance/TwitterDataProcessor/#calc_similarity","text":"This function is used to calculate the similarity between multiple user or tweet objects. The function takes in either a list of user objects or a list of public tweet metrics as well as a features list. Either user objects or tweet metrics need to be provided, not both. The user objects must be recently fetched from the Twitter API v1. A stored object (e.g., by using the pickle Python module) will not have the necessary properties to be resolved by this function. Otherwise, an error will be returned. The similarity is calculated based on a feature vector containing numeric values. Thus, for a given set of user or tweet attributes, the features must be provided on which the similarity will be computed. As a distance measure and, thus, the similarity of feature vectors, the vector norm of second order will be calculated which is equivalent to the euclidean distance. Therefore, the numpy.linalg.norm function is used. The smaller the distance, the more similar the two vectors are. The function will determine the distance between a distinct pair of user or tweet objects. For instance, when three user objects for the Twitter accounts 12355 , 734231 , 9083468 are provided, the following output will be generated: {(12355, 734231): 4567.098, (12355, 9083468): 5980.076, (734231, 9083468): 8763.32} The output dictionary contains the distinct pairs of objects as a tuple as dictionary keys. The distances for each distinct pair is given as dictionary value. The output is sorted in ascending order. Hence, the minimal distance and, thus, the most similar pair is provided as first dictionary entry. Function: TwitterDataProcessor.calc_similarity(user_objs: List[dict] | None = None, tweet_metrics: List[Dict[int, dict]] | None = None, *, features: List[str]) Args: user_objs (List[dict] | None, optional): List of serialized Twitter user objects from Twitter Search API v1. Defaults to None. tweet_metrics (List[Dict[int | dict]] | None, optional): List of public Tweet metrics as dictionaries with Tweet IDs as keys. Defaults to None. features (List[str]): Features that should be contained in the feature vector. Features have to be numeric and must belong to the respective object (i.e., user or tweet.) The features that can be provided for the features list can be found in the detailed description of the attributes for the compare_tweets function and the detailed description of the attributes for the compare_users function . The implementation design of this function allows a comparison of Twitter users or tweets based on the available metrics. The implementation was inspired by the characterics of social bots on Twitter as they often have a similar number of followers or followees and their posted tweets often have a similar number of likes. Thus, the calculated similarities might help to identify bot-like behavior of Twitter accounts as well as identify deviations from normal Twitter accounts. If their similarities are small, they are likely to have a similar behavior on Twitter (i.e., a bot could be analyzed). Source Code def calc_similarity(self, user_objs: List[dict] | None = None, tweet_metrics: List[Dict[int, dict]] | None = None, *, features: List[str]) -> dict: \"\"\"Calculates the euclidean distance of users/tweets based on a feature vector. Either user objects or Tweet objects must be specified, not both. Args: user_objs (List[dict] | None, optional): List of serialized Twitter user objects from Twitter Search API v1. Defaults to None. tweet_metrics (List[Dict[int | dict]] | None, optional): List of public Tweet metrics as dictionaries with Tweet IDs as keys. Defaults to None. features (List[str]): Features that should be contained in the feature vector. Features have to be numeric and must belong to the respective object (i.e., user or tweet.) Raises: ValueError: If either 'user_objs' and 'tweet_objs' or none of them were provided. AssertionError: If non-numeric feature was provided in the 'features' list. Returns: dict: Unique pair of users/tweets containing the respective euclidean distance. Sorted in ascending order. \"\"\" # init empty dict to store distances distances = dict() # if users and tweets were provided if user_objs and tweet_metrics: raise ValueError(\"Either 'user_objs' or 'tweet_metrics' must be specified, not both.\") # if only user_objs were provided elif user_objs: # iterate over every uniqe pair for i in range(len(user_objs)): for j in range(i + 1, len(user_objs)): # get user objects for each pair user_i = user_objs[i] user_j = user_objs[j] # build feature vector vec_i = np.array([user_i[feature] for feature in features]) vec_j = np.array([user_j[feature] for feature in features]) # feature vectors have to contain numeric values assert all(isinstance(feat, Number) for feat in vec_i), \"only numeric features are allowed\" assert all(isinstance(feat, Number) for feat in vec_j), \"only numeric features are allowed\" # calc euclidean distance distances[(user_i[\"id\"], user_j[\"id\"])] = np.linalg.norm(vec_i - vec_j, ord=2) elif tweet_metrics: # iterate over every uniqe pair for i in range(len(tweet_metrics)): for j in range(i + 1, len(tweet_metrics)): # get Tweet objects for each pair tweet_i = list(tweet_metrics.values())[i] tweet_j = list(tweet_metrics.values())[j] # build feature vector vec_i = np.array([tweet_i[feature] for feature in features]) vec_j = np.array([tweet_j[feature] for feature in features]) # feature vectors have to contain numeric values assert all(isinstance(feat, Number) for feat in vec_i), \"only numeric features are allowed\" assert all(isinstance(feat, Number) for feat in vec_j), \"only numeric features are allowed\" # calc euclidean distance distances[(list(tweet_metrics.keys())[i], list(tweet_metrics.keys())[j])] = np.linalg.norm(vec_i - vec_j, ord=2) # if none was provided else: raise ValueError(\"Either 'user_objs' or 'tweet_metrics' must be provided.\") # sort dict in ascendin order sorted_values = dict(sorted(distances.items(), key=operator.itemgetter(1))) return sorted_values","title":"calc_similarity"},{"location":"maintenance/cli/","text":"Command-line Interface Functions The functions for the CLI are implemented using the argparse Python library. Initially, the usage message is set that users can receive by calling pysna --help . Then, the package version is collected by using a regular expression search for the version specification in the __init__.py . The version is added to the main parser argument for --version . Then, the required secrets (i.e., secrets for the Twitter API. See here for more information) and optional secrets (i.e., Botometer API secrets) are set. Since every function call via the CLI will generate a new CLI session, it is technically infeasible to store the secrets across all CLI sessions and function calls. To avoid passing in the secrets every time the user calls a function, a config file path is defined where the secrets will be stored. Then, the parsers will read the configured secrets from this config file path, so the user does not need to pass in the secrets manually for every function call. The config_file_path is set under the home directory: ~/.pysna/config/secrets.env . The .pysna folder is hidden. For the CLI tool, two parsers were defined: The main parser reacting to the pysna command and the --version and -help flags. The subparser is used to define subcommands for the main parser such that commands/functions can be chained (e.g., pysna user-info ). For every subcommand, the help instructions can be found via the --help flag (e.g., pysna user-info --help ). Internal Functions Internal functions are used to process data, parse file contents, or handle user input. read_secrets This function reads the secrets from the specified environment path. For default, the config_file_path variable is passed in by all user functions. If any other path is provided, the function will read the secrets from this file. Only .env files are supported. Function: read_secrets(env_path: str) The .env file must be of the form: BEARER_TOKEN= CONSUMER_KEY= CONSUMER_SECRET= ACCESS_TOKEN= ACCESS_TOKEN_SECRET= X_RAPIDAPI_KEY= X_RAPIDAPI_HOST= The function will read the secrets and return a dictionary containing the lowered keys and secrets from the .env file. Source Code def read_secrets(env_path: str) -> dict: if not os.path.exists(env_path): raise Exception(\"No config file found for secrets. Use the 'set-secrets' function to create a config file or provide a .env file using the '--env' flag.\") else: load_dotenv(env_path) # catch environmental variables for secret in REQUIRED_SECRETS: if secret not in os.environ: raise KeyError(f\"{secret} must be provided in the environment variables.\") # collect secrets secrets = {secret.lower(): os.getenv(secret) for secret in REQUIRED_SECRETS + OPTIONAL_SECRETS} return secrets output The output function receives the data returned by one of the user functions. Then, this function decides based on the provided arguments if the data should be printed to the console, exported to a file (CSV or JSON) or appended to a file. In case, the user specified a file path using the --output flag but does not specify the --append flag, then the data will be written to a JSON (if the path ends with a .json ) or CSV (if the path ends with a .csv ) file. If the user also specified the --append flag, then the data will be appended to the specified file. Otherwise, the data will be printed to stdout. Function: output(data: dict, encoding: str, path: str | None = None, append: bool = False) Args: data (dict): input data that has been fetched by a function. encoding (str): encoding option of the export. Defaults to UTF-8. path (str | None): the path to the file. Defaults to None. append (bool): wheather to append the data or not. Defaults to false. This function is used for every main user function. It handles the specified flags by the user (i.e., return_timestamp , append , encoding ) Source Code def output(data: dict, encoding: str, path: str | None = None, append: bool = False): # either print results if '--output' arg was provided if (path is not None) and (append is False): if path.endswith(\".json\"): export_to_json(data, path, encoding) elif path.endswith(\".csv\"): export_to_csv(data, path, encoding) # or append to existing file elif (path is not None) and (append is True): if path.endswith(\".json\"): append_to_json(data, path, encoding) elif path.endswith(\".csv\"): append_to_csv(data, path, encoding) # or print them to the CLI in JSON format else: print(json.dumps(data, ensure_ascii=False)) pass argument Convenience function to properly format arguments to pass to the subcommand decorator. Function: argument(*name_or_flags, **kwargs) This function returns a tuple of a list of names or flags and the specified keyword arguments. It is a helper function used to pass in arguments to the decorator subcommand Source Code def argument(*name_or_flags, **kwargs): \"\"\"Convenience function to properly format arguments to pass to the subcommand decorator.\"\"\" return (list(name_or_flags), kwargs) subcommand (decorator) Decorator to define a new subcommand in a sanity-preserving way. The function will be stored in the func variable when the parser parses arguments so that it can be called directly like so: args = cli.parse_args() args.func(args) Usage example: @subcommand([argument(\"-d\", help=\"Enable debug mode\", action=\"store_true\")]) def subcommand(args): print(args) Then on the command line: python cli.py subcommand -d This function is a Python decorator defining a function as a subcommand to the main parser. It adds a new argument to the subparser. Developers can define the function name using the function_name argument (e.g., user-info ) and pass in the function arguments using the args argument in combination with the argument function. Source Code def subcommand(function_name: str, args=[], parent=subparsers): \"\"\"Decorator to define a new subcommand in a sanity-preserving way. The function will be stored in the ``func`` variable when the parser parses arguments so that it can be called directly like so:: args = cli.parse_args() args.func(args) Usage example:: @subcommand([argument(\"-d\", help=\"Enable debug mode\", action=\"store_true\")]) def subcommand(args): print(args) Then on the command line:: $ python cli.py subcommand -d \"\"\" def decorator(func): parser = parent.add_parser(function_name, description=func.__doc__) for arg in args: parser.add_argument(*arg[0], **arg[1]) parser.set_defaults(func=func) return decorator main This function is called when the user runs the pysna command on the console. It parses the provided arguments (such as the subcommands and related arguments) or prints the help or usage instructions to the console. Source Code def main(): args = parser.parse_args() if args.subcommand is None: parser.print_help() else: args.func(args) User Functions User functions are designed to be used by the package user. They form the basis for interaction with the package. set_secrets Since every CLI sessions requires secrets for authentification with the Twitter API, this function serves a way to store the secrets at a hidden place under the config_file_path . The user has to run this function once to set the secrets. Whenever the user wishes to change/overwrite set secrets, he or she can rerun this function. The user has to pass in a file path to a .env file. The .env file must be of the form: BEARER_TOKEN= CONSUMER_KEY= CONSUMER_SECRET= ACCESS_TOKEN= ACCESS_TOKEN_SECRET= X_RAPIDAPI_KEY= X_RAPIDAPI_HOST= In case, any secrets was not provided, an exception will be thrown. If all secrets were collected by the function, it creates the config_file_path under the home directory of the user and copies the .env file to this path. After copying, a message wil be prompted saying the user that the secrets has been set. Source Code @subcommand(\"set-secrets\", args=[argument(\"secrets_file\", type=str, help=\"Path to the secrets file. Only .env files are supported.\")]) def set_secrets(args): \"\"\"CLI function to set or overwrite a config file for storing API secrets. Config file will be set to '~/.pysna/config/secrets.env'.\"\"\" if not args.secrets_file.endswith(\".env\"): raise Exception(\"Only .env files are supported. Please pass in a .env file.\") # check .env file format load_dotenv(args.secrets_file) for secret in REQUIRED_SECRETS: if secret not in os.environ: raise Exception( f\"{secret} must be provided in the {args.secrets_file} file.\" f\"\\nMake sure that your {args.secrets_file} has the following format:\" f\"\\nBEARER_TOKEN=...\\nCONSUMER_KEY=...\\nCONSUMER_SECRET=...\\nACCESS_TOKEN=...\\nACCESS_TOKEN_SECRET=...\" f\"\\nIf you wish to use the Botometer API, also provide the {', '.join(OPTIONAL_SECRETS)} keys in the {args.secrets_file} file.\" ) # create folders if it does not exist yet os.makedirs(os.path.dirname(config_file_path), exist_ok=True) # copy file content from args.secrets_file to config file path shutil.copy2(args.secrets_file, config_file_path) print(f\"Secrets from {args.secrets_file} file were set.\") user_info_cli CLI function equivalent to the TwitterAPI.user_info function. This function is a wrapper around the TwitterAPI.user_info function and handles the inputs and outputs to the console. Args: user : User ID or screen name. attributes (List): User attributes. Must be from his list . env (str, optional): Path to .env file. Defaults to config_file_path . If the user wishes to use different secrets for authentification, he or she can pass in the path to another .env file. This file must also have the same form, as described in the read_secrets function section. return_timestamp (bool, optional): Wheather to return the Unix timestamp of the request. Defaults to false. output (str, optional): Export file path. This argument is also used in combination with the append argument to specify that the data should be added to an existing file. If both arguments were provided, data is appended. encoding (str, optional): Encoding of the data. Defaults to UTF-8. append (bool, optional): Wheather to append the data to an existing file or not. If the flag is provided (i.e., true), the file path needs to be specified with the output argument. First, the secrets are loaded from the environment file path. Then, the authentication to the TwitterAPI using the TwitterAPI class is performed. After that, the TwitterAPI instance calls the user_info function with the specified arguments. The results are either printed to the console, exported to a file, or appended to a file. Source Code @subcommand( \"user-info\", args=[ argument(\"user\", help=\"Twitter User ID or screen name\"), argument(\"attributes\", nargs=\"+\", default=[], help=f\"List or string of desired User attributes. Must be from {', '.join(get_args(TwitterAPI.LITERALS_USER_INFO))}\"), argument(\"--env\", \"-e\", type=str, default=config_file_path, required=False, help=f\"Path to .env file. Defaults to {config_file_path}.\"), argument(\"--return-timestamp\", type=bool, default=False, required=False, action=argparse.BooleanOptionalAction, help=\"Returns the UTC timestamp of the request.\"), argument( \"--output\", \"-o\", type=str, default=None, required=False, help=\"Store results in a JSON or CSV file. Specify output file path (including file name). File extension specifies file export (e.g., '.csv' for CSV file export and '.json' for JSON file export)\", ), argument(\"--encoding\", type=str, default=\"utf-8\", required=False, help=\"Encoding of the output file. Defaults to UTF-8.\"), argument(\"--append\", \"-a\", type=bool, default=False, required=False, action=argparse.BooleanOptionalAction, help=\"Add results to an existing JSON file. File needs to be specified in the --output flag.\"), ], ) def user_info_cli(args): \"\"\"CLI function to request information from the specified Twitter user.\"\"\" # read secrets secrets = read_secrets(args.env) # establish connection to the API api = TwitterAPI(**secrets) # get results result = api.user_info(user=args.user, attributes=args.attributes, return_timestamp=args.return_timestamp) # handle output output(result, path=args.output, encoding=args.encoding, append=args.append) pass tweet_info_cli CLI function equivalent to the TwitterAPI.tweet_info function. This function is a wrapper around the TwitterAPI.tweet_info function and handles the inputs and outputs to the console. Args: tweet_id : The tweet ID attributes (List): Tweet attributes. Must be from this list . env (str, optional): Path to .env file. Defaults to config_file_path . If the user wishes to use different secrets for authentification, he or she can pass in the path to another .env file. This file must also have the same form, as described in the read_secrets function section. return_timestamp (bool, optional): Wheather to return the Unix timestamp of the request. Defaults to false. output (str, optional): Export file path. This argument is also used in combination with the append argument to specify that the data should be added to an existing file. If both arguments were provided, data is appended. encoding (str, optional): Encoding of the data. Defaults to UTF-8. append (bool, optional): Wheather to append the data to an existing file or not. If the flag is provided (i.e., true), the file path needs to be specified with the output argument. First, the secrets are loaded from the environment file path. Then, the authentication to the TwitterAPI using the TwitterAPI class is performed. After that, the TwitterAPI instance calls the tweet_info function with the specified arguments. The results are either printed to the console, exported to a file, or appended to a file. Source Code @subcommand( \"tweet-info\", args=[ argument(\"tweet_id\", help=\"Tweet ID\"), argument(\"attributes\", nargs=\"+\", default=[], help=f\"List or string of desired Tweet attribute. Must be from {', '.join(get_args(TwitterAPI.LITERALS_TWEET_INFO))}\"), argument(\"--env\", \"-e\", type=str, default=config_file_path, required=False, help=f\"Path to .env file. Defaults to {config_file_path}.\"), argument(\"--return-timestamp\", type=bool, default=False, required=False, action=argparse.BooleanOptionalAction, help=\"Returns the UTC timestamp of the request.\"), argument( \"--output\", \"-o\", type=str, default=None, required=False, help=\"Store results in a JSON or CSV file. Specify output file path (including file name). File extension specifies file export (e.g., '.csv' for CSV file export and '.json' for JSON file export)\", ), argument(\"--encoding\", type=str, default=\"utf-8\", required=False, help=\"Encoding of the output file. Defaults to UTF-8.\"), argument(\"--append\", \"-a\", type=bool, default=False, required=False, action=argparse.BooleanOptionalAction, help=\"Add results to an existing JSON file. File needs to be specified in the --output flag.\"), ], ) def tweet_info_cli(args): \"\"\"CLI function to request information from the specified Tweet.\"\"\" # read secrets secrets = read_secrets(args.env) # establish connection to the API api = TwitterAPI(**secrets) # get results result = api.tweet_info(tweet_id=args.tweet_id, attributes=args.attributes, return_timestamp=args.return_timestamp) # handle output output(result, path=args.output, encoding=args.encoding, append=args.append) pass compare_users_cli CLI function equivalent to the TwitterAPI.compare_users function. This function is a wrapper around the TwitterAPI.compare_users function and handles the inputs and outputs to the console. Args: users (List): User IDs or screen names. -- compare (List): Comparison attributes for Twitter users. Must be from this list . Short form: -c . features (List): Features that should be contained within the feature vectors for the similarity comparison attribute. Must be from this list . Short form: -f env (str, optional): Path to .env file. Defaults to config_file_path . If the user wishes to use different secrets for authentification, he or she can pass in the path to another .env file. This file must also have the same form, as described in the read_secrets function section. return_timestamp (bool, optional): Wheather to return the Unix timestamp of the request. Defaults to false. output (str, optional): Export file path. This argument is also used in combination with the append argument to specify that the data should be added to an existing file. If both arguments were provided, data is appended. encoding (str, optional): Encoding of the data. Defaults to UTF-8. append (bool, optional): Wheather to append the data to an existing file or not. If the flag is provided (i.e., true), the file path needs to be specified with the output argument. First, the secrets are loaded from the environment file path. Then, the authentication to the TwitterAPI using the TwitterAPI class is performed. After that, the TwitterAPI instance calls the compare_users function with the specified arguments. The results are either printed to the console, exported to a file, or appended to a file. Source Code @subcommand( \"compare-users\", args=[ argument(\"users\", nargs=\"+\", default=[], help=\"The IDs or screen names of the users.\"), argument(\"--compare\", \"-c\", nargs=\"+\", default=[], required=True, help=f\"The comparison attribute(s). Must be from following: {', '.join(get_args(TwitterAPI.LITERALS_COMPARE_USERS))}.\"), argument( \"--features\", \"-f\", nargs=\"+\", default=[], required=False, help=f\"Features that should be contained in the feature vector for similarity comparison. Must be from: {', '.join(get_args(TwitterAPI.SIMILARITY_FEATURES_COMPARE_USERS))}\", ), argument(\"--env\", \"-e\", type=str, default=config_file_path, required=False, help=f\"Path to .env file. Defaults to {config_file_path}.\"), argument(\"--return-timestamp\", type=bool, default=False, required=False, action=argparse.BooleanOptionalAction, help=\"Returns the UTC timestamp of the request.\"), argument( \"--output\", \"-o\", type=str, default=None, required=False, help=\"Store results in a JSON or CSV file. Specify output file path (including file name). File extension specifies file export (e.g., '.csv' for CSV file export and '.json' for JSON file export)\", ), argument(\"--encoding\", type=str, default=\"utf-8\", required=False, help=\"Encoding of the output file. Defaults to UTF-8.\"), argument(\"--append\", \"-a\", type=bool, default=False, required=False, action=argparse.BooleanOptionalAction, help=\"Add results to an existing JSON file. File needs to be specified in the --output flag.\"), ], ) def compare_users_cli(args): \"\"\"CLI function to compare multiple Twitter users with the specified comparision attribute(s).\"\"\" # read secrets secrets = read_secrets(args.env) # establish connection to the API api = TwitterAPI(**secrets) # get results result = api.compare_users(users=args.users, compare=args.compare, return_timestamp=args.return_timestamp, features=args.features) # handle output output(result, path=args.output, encoding=args.encoding, append=args.append) pass compare_tweets_cli CLI function equivalent to the TwitterAPI.compare_tweets function. This function is a wrapper around the TwitterAPI.compare_tweets function and handles the inputs and outputs to the console. Args: tweets (List): Tweet IDs compare (List): Comparison attributes for Twitter users. Must be from this list . Short form: -c . features (List): Features that should be contained within the feature vectors for the similarity comparison attribute. Must be from this list . Short form: -f env (str, optional): Path to .env file. Defaults to config_file_path . If the user wishes to use different secrets for authentification, he or she can pass in the path to another .env file. This file must also have the same form, as described in the read_secrets function section. return_timestamp (bool, optional): Wheather to return the Unix timestamp of the request. Defaults to false. output (str, optional): Export file path. This argument is also used in combination with the append argument to specify that the data should be added to an existing file. If both arguments were provided, data is appended. encoding (str, optional): Encoding of the data. Defaults to UTF-8. append (bool, optional): Wheather to append the data to an existing file or not. If the flag is provided (i.e., true), the file path needs to be specified with the output argument. First, the secrets are loaded from the environment file path. Then, the authentication to the TwitterAPI using the TwitterAPI class is performed. After that, the TwitterAPI instance calls the compare_tweets function with the specified arguments. The results are either printed to the console, exported to a file, or appended to a file. Source Code @subcommand( \"compare-tweets\", args=[ argument(\"tweets\", nargs=\"+\", default=[], help=\"The IDs of the Tweets.\"), argument(\"--compare\", \"-c\", nargs=\"+\", default=[], required=True, help=f\"The comparison attribute(s). Must be the following: {', '.join(get_args(TwitterAPI.LITERALS_COMPARE_TWEETS))}.\"), argument( \"--features\", \"-f\", nargs=\"+\", default=[], required=False, help=f\"Features that should be contained in the feature vector for similarity comparison. Must be from: {', '.join(get_args(TwitterAPI.SIMILARITY_FEATURES_COMPARE_TWEETS))}\" ), argument(\"--env\", \"-e\", type=str, default=config_file_path, required=False, help=f\"Path to .env file. Defaults to {config_file_path}.\"), argument(\"--return-timestamp\", type=bool, default=False, required=False, action=argparse.BooleanOptionalAction, help=\"Returns the UTC timestamp of the request.\"), argument( \"--output\", \"-o\", type=str, default=None, required=False, help=\"Store results in a JSON or CSV file. Specify output file path (including file name). File extension specifies file export (e.g., '.csv' for CSV file export and '.json' for JSON file export)\", ), argument(\"--encoding\", type=str, default=\"utf-8\", required=False, help=\"Encoding of the output file. Defaults to UTF-8.\"), argument(\"--append\", \"-a\", type=bool, default=False, required=False, action=argparse.BooleanOptionalAction, help=\"Add results to an existing JSON file. File needs to be specified in the --output flag.\"), ], ) def compare_tweets_cli(args): \"\"\"CLI function to compare multiple Tweets with the specified comparision attribute(s).\"\"\" # read secrets secrets = read_secrets(args.env) # establish connection to the API api = TwitterAPI(**secrets) # get results result = api.compare_tweets(tweet_ids=args.tweets, compare=args.compare, return_timestamp=args.return_timestamp, features=args.features) # handle output output(result, path=args.output, encoding=args.encoding, append=args.append) pass","title":"CLI Functions"},{"location":"maintenance/cli/#command-line-interface-functions","text":"The functions for the CLI are implemented using the argparse Python library. Initially, the usage message is set that users can receive by calling pysna --help . Then, the package version is collected by using a regular expression search for the version specification in the __init__.py . The version is added to the main parser argument for --version . Then, the required secrets (i.e., secrets for the Twitter API. See here for more information) and optional secrets (i.e., Botometer API secrets) are set. Since every function call via the CLI will generate a new CLI session, it is technically infeasible to store the secrets across all CLI sessions and function calls. To avoid passing in the secrets every time the user calls a function, a config file path is defined where the secrets will be stored. Then, the parsers will read the configured secrets from this config file path, so the user does not need to pass in the secrets manually for every function call. The config_file_path is set under the home directory: ~/.pysna/config/secrets.env . The .pysna folder is hidden. For the CLI tool, two parsers were defined: The main parser reacting to the pysna command and the --version and -help flags. The subparser is used to define subcommands for the main parser such that commands/functions can be chained (e.g., pysna user-info ). For every subcommand, the help instructions can be found via the --help flag (e.g., pysna user-info --help ).","title":"Command-line Interface Functions"},{"location":"maintenance/cli/#internal-functions","text":"Internal functions are used to process data, parse file contents, or handle user input.","title":"Internal Functions"},{"location":"maintenance/cli/#read_secrets","text":"This function reads the secrets from the specified environment path. For default, the config_file_path variable is passed in by all user functions. If any other path is provided, the function will read the secrets from this file. Only .env files are supported. Function: read_secrets(env_path: str) The .env file must be of the form: BEARER_TOKEN= CONSUMER_KEY= CONSUMER_SECRET= ACCESS_TOKEN= ACCESS_TOKEN_SECRET= X_RAPIDAPI_KEY= X_RAPIDAPI_HOST= The function will read the secrets and return a dictionary containing the lowered keys and secrets from the .env file. Source Code def read_secrets(env_path: str) -> dict: if not os.path.exists(env_path): raise Exception(\"No config file found for secrets. Use the 'set-secrets' function to create a config file or provide a .env file using the '--env' flag.\") else: load_dotenv(env_path) # catch environmental variables for secret in REQUIRED_SECRETS: if secret not in os.environ: raise KeyError(f\"{secret} must be provided in the environment variables.\") # collect secrets secrets = {secret.lower(): os.getenv(secret) for secret in REQUIRED_SECRETS + OPTIONAL_SECRETS} return secrets","title":"read_secrets"},{"location":"maintenance/cli/#output","text":"The output function receives the data returned by one of the user functions. Then, this function decides based on the provided arguments if the data should be printed to the console, exported to a file (CSV or JSON) or appended to a file. In case, the user specified a file path using the --output flag but does not specify the --append flag, then the data will be written to a JSON (if the path ends with a .json ) or CSV (if the path ends with a .csv ) file. If the user also specified the --append flag, then the data will be appended to the specified file. Otherwise, the data will be printed to stdout. Function: output(data: dict, encoding: str, path: str | None = None, append: bool = False) Args: data (dict): input data that has been fetched by a function. encoding (str): encoding option of the export. Defaults to UTF-8. path (str | None): the path to the file. Defaults to None. append (bool): wheather to append the data or not. Defaults to false. This function is used for every main user function. It handles the specified flags by the user (i.e., return_timestamp , append , encoding ) Source Code def output(data: dict, encoding: str, path: str | None = None, append: bool = False): # either print results if '--output' arg was provided if (path is not None) and (append is False): if path.endswith(\".json\"): export_to_json(data, path, encoding) elif path.endswith(\".csv\"): export_to_csv(data, path, encoding) # or append to existing file elif (path is not None) and (append is True): if path.endswith(\".json\"): append_to_json(data, path, encoding) elif path.endswith(\".csv\"): append_to_csv(data, path, encoding) # or print them to the CLI in JSON format else: print(json.dumps(data, ensure_ascii=False)) pass","title":"output"},{"location":"maintenance/cli/#argument","text":"Convenience function to properly format arguments to pass to the subcommand decorator. Function: argument(*name_or_flags, **kwargs) This function returns a tuple of a list of names or flags and the specified keyword arguments. It is a helper function used to pass in arguments to the decorator subcommand Source Code def argument(*name_or_flags, **kwargs): \"\"\"Convenience function to properly format arguments to pass to the subcommand decorator.\"\"\" return (list(name_or_flags), kwargs)","title":"argument"},{"location":"maintenance/cli/#subcommand-decorator","text":"Decorator to define a new subcommand in a sanity-preserving way. The function will be stored in the func variable when the parser parses arguments so that it can be called directly like so: args = cli.parse_args() args.func(args) Usage example: @subcommand([argument(\"-d\", help=\"Enable debug mode\", action=\"store_true\")]) def subcommand(args): print(args) Then on the command line: python cli.py subcommand -d This function is a Python decorator defining a function as a subcommand to the main parser. It adds a new argument to the subparser. Developers can define the function name using the function_name argument (e.g., user-info ) and pass in the function arguments using the args argument in combination with the argument function. Source Code def subcommand(function_name: str, args=[], parent=subparsers): \"\"\"Decorator to define a new subcommand in a sanity-preserving way. The function will be stored in the ``func`` variable when the parser parses arguments so that it can be called directly like so:: args = cli.parse_args() args.func(args) Usage example:: @subcommand([argument(\"-d\", help=\"Enable debug mode\", action=\"store_true\")]) def subcommand(args): print(args) Then on the command line:: $ python cli.py subcommand -d \"\"\" def decorator(func): parser = parent.add_parser(function_name, description=func.__doc__) for arg in args: parser.add_argument(*arg[0], **arg[1]) parser.set_defaults(func=func) return decorator","title":"subcommand (decorator)"},{"location":"maintenance/cli/#main","text":"This function is called when the user runs the pysna command on the console. It parses the provided arguments (such as the subcommands and related arguments) or prints the help or usage instructions to the console. Source Code def main(): args = parser.parse_args() if args.subcommand is None: parser.print_help() else: args.func(args)","title":"main"},{"location":"maintenance/cli/#user-functions","text":"User functions are designed to be used by the package user. They form the basis for interaction with the package.","title":"User Functions"},{"location":"maintenance/cli/#set_secrets","text":"Since every CLI sessions requires secrets for authentification with the Twitter API, this function serves a way to store the secrets at a hidden place under the config_file_path . The user has to run this function once to set the secrets. Whenever the user wishes to change/overwrite set secrets, he or she can rerun this function. The user has to pass in a file path to a .env file. The .env file must be of the form: BEARER_TOKEN= CONSUMER_KEY= CONSUMER_SECRET= ACCESS_TOKEN= ACCESS_TOKEN_SECRET= X_RAPIDAPI_KEY= X_RAPIDAPI_HOST= In case, any secrets was not provided, an exception will be thrown. If all secrets were collected by the function, it creates the config_file_path under the home directory of the user and copies the .env file to this path. After copying, a message wil be prompted saying the user that the secrets has been set. Source Code @subcommand(\"set-secrets\", args=[argument(\"secrets_file\", type=str, help=\"Path to the secrets file. Only .env files are supported.\")]) def set_secrets(args): \"\"\"CLI function to set or overwrite a config file for storing API secrets. Config file will be set to '~/.pysna/config/secrets.env'.\"\"\" if not args.secrets_file.endswith(\".env\"): raise Exception(\"Only .env files are supported. Please pass in a .env file.\") # check .env file format load_dotenv(args.secrets_file) for secret in REQUIRED_SECRETS: if secret not in os.environ: raise Exception( f\"{secret} must be provided in the {args.secrets_file} file.\" f\"\\nMake sure that your {args.secrets_file} has the following format:\" f\"\\nBEARER_TOKEN=...\\nCONSUMER_KEY=...\\nCONSUMER_SECRET=...\\nACCESS_TOKEN=...\\nACCESS_TOKEN_SECRET=...\" f\"\\nIf you wish to use the Botometer API, also provide the {', '.join(OPTIONAL_SECRETS)} keys in the {args.secrets_file} file.\" ) # create folders if it does not exist yet os.makedirs(os.path.dirname(config_file_path), exist_ok=True) # copy file content from args.secrets_file to config file path shutil.copy2(args.secrets_file, config_file_path) print(f\"Secrets from {args.secrets_file} file were set.\")","title":"set_secrets"},{"location":"maintenance/cli/#user_info_cli","text":"CLI function equivalent to the TwitterAPI.user_info function. This function is a wrapper around the TwitterAPI.user_info function and handles the inputs and outputs to the console. Args: user : User ID or screen name. attributes (List): User attributes. Must be from his list . env (str, optional): Path to .env file. Defaults to config_file_path . If the user wishes to use different secrets for authentification, he or she can pass in the path to another .env file. This file must also have the same form, as described in the read_secrets function section. return_timestamp (bool, optional): Wheather to return the Unix timestamp of the request. Defaults to false. output (str, optional): Export file path. This argument is also used in combination with the append argument to specify that the data should be added to an existing file. If both arguments were provided, data is appended. encoding (str, optional): Encoding of the data. Defaults to UTF-8. append (bool, optional): Wheather to append the data to an existing file or not. If the flag is provided (i.e., true), the file path needs to be specified with the output argument. First, the secrets are loaded from the environment file path. Then, the authentication to the TwitterAPI using the TwitterAPI class is performed. After that, the TwitterAPI instance calls the user_info function with the specified arguments. The results are either printed to the console, exported to a file, or appended to a file. Source Code @subcommand( \"user-info\", args=[ argument(\"user\", help=\"Twitter User ID or screen name\"), argument(\"attributes\", nargs=\"+\", default=[], help=f\"List or string of desired User attributes. Must be from {', '.join(get_args(TwitterAPI.LITERALS_USER_INFO))}\"), argument(\"--env\", \"-e\", type=str, default=config_file_path, required=False, help=f\"Path to .env file. Defaults to {config_file_path}.\"), argument(\"--return-timestamp\", type=bool, default=False, required=False, action=argparse.BooleanOptionalAction, help=\"Returns the UTC timestamp of the request.\"), argument( \"--output\", \"-o\", type=str, default=None, required=False, help=\"Store results in a JSON or CSV file. Specify output file path (including file name). File extension specifies file export (e.g., '.csv' for CSV file export and '.json' for JSON file export)\", ), argument(\"--encoding\", type=str, default=\"utf-8\", required=False, help=\"Encoding of the output file. Defaults to UTF-8.\"), argument(\"--append\", \"-a\", type=bool, default=False, required=False, action=argparse.BooleanOptionalAction, help=\"Add results to an existing JSON file. File needs to be specified in the --output flag.\"), ], ) def user_info_cli(args): \"\"\"CLI function to request information from the specified Twitter user.\"\"\" # read secrets secrets = read_secrets(args.env) # establish connection to the API api = TwitterAPI(**secrets) # get results result = api.user_info(user=args.user, attributes=args.attributes, return_timestamp=args.return_timestamp) # handle output output(result, path=args.output, encoding=args.encoding, append=args.append) pass","title":"user_info_cli"},{"location":"maintenance/cli/#tweet_info_cli","text":"CLI function equivalent to the TwitterAPI.tweet_info function. This function is a wrapper around the TwitterAPI.tweet_info function and handles the inputs and outputs to the console. Args: tweet_id : The tweet ID attributes (List): Tweet attributes. Must be from this list . env (str, optional): Path to .env file. Defaults to config_file_path . If the user wishes to use different secrets for authentification, he or she can pass in the path to another .env file. This file must also have the same form, as described in the read_secrets function section. return_timestamp (bool, optional): Wheather to return the Unix timestamp of the request. Defaults to false. output (str, optional): Export file path. This argument is also used in combination with the append argument to specify that the data should be added to an existing file. If both arguments were provided, data is appended. encoding (str, optional): Encoding of the data. Defaults to UTF-8. append (bool, optional): Wheather to append the data to an existing file or not. If the flag is provided (i.e., true), the file path needs to be specified with the output argument. First, the secrets are loaded from the environment file path. Then, the authentication to the TwitterAPI using the TwitterAPI class is performed. After that, the TwitterAPI instance calls the tweet_info function with the specified arguments. The results are either printed to the console, exported to a file, or appended to a file. Source Code @subcommand( \"tweet-info\", args=[ argument(\"tweet_id\", help=\"Tweet ID\"), argument(\"attributes\", nargs=\"+\", default=[], help=f\"List or string of desired Tweet attribute. Must be from {', '.join(get_args(TwitterAPI.LITERALS_TWEET_INFO))}\"), argument(\"--env\", \"-e\", type=str, default=config_file_path, required=False, help=f\"Path to .env file. Defaults to {config_file_path}.\"), argument(\"--return-timestamp\", type=bool, default=False, required=False, action=argparse.BooleanOptionalAction, help=\"Returns the UTC timestamp of the request.\"), argument( \"--output\", \"-o\", type=str, default=None, required=False, help=\"Store results in a JSON or CSV file. Specify output file path (including file name). File extension specifies file export (e.g., '.csv' for CSV file export and '.json' for JSON file export)\", ), argument(\"--encoding\", type=str, default=\"utf-8\", required=False, help=\"Encoding of the output file. Defaults to UTF-8.\"), argument(\"--append\", \"-a\", type=bool, default=False, required=False, action=argparse.BooleanOptionalAction, help=\"Add results to an existing JSON file. File needs to be specified in the --output flag.\"), ], ) def tweet_info_cli(args): \"\"\"CLI function to request information from the specified Tweet.\"\"\" # read secrets secrets = read_secrets(args.env) # establish connection to the API api = TwitterAPI(**secrets) # get results result = api.tweet_info(tweet_id=args.tweet_id, attributes=args.attributes, return_timestamp=args.return_timestamp) # handle output output(result, path=args.output, encoding=args.encoding, append=args.append) pass","title":"tweet_info_cli"},{"location":"maintenance/cli/#compare_users_cli","text":"CLI function equivalent to the TwitterAPI.compare_users function. This function is a wrapper around the TwitterAPI.compare_users function and handles the inputs and outputs to the console. Args: users (List): User IDs or screen names. -- compare (List): Comparison attributes for Twitter users. Must be from this list . Short form: -c . features (List): Features that should be contained within the feature vectors for the similarity comparison attribute. Must be from this list . Short form: -f env (str, optional): Path to .env file. Defaults to config_file_path . If the user wishes to use different secrets for authentification, he or she can pass in the path to another .env file. This file must also have the same form, as described in the read_secrets function section. return_timestamp (bool, optional): Wheather to return the Unix timestamp of the request. Defaults to false. output (str, optional): Export file path. This argument is also used in combination with the append argument to specify that the data should be added to an existing file. If both arguments were provided, data is appended. encoding (str, optional): Encoding of the data. Defaults to UTF-8. append (bool, optional): Wheather to append the data to an existing file or not. If the flag is provided (i.e., true), the file path needs to be specified with the output argument. First, the secrets are loaded from the environment file path. Then, the authentication to the TwitterAPI using the TwitterAPI class is performed. After that, the TwitterAPI instance calls the compare_users function with the specified arguments. The results are either printed to the console, exported to a file, or appended to a file. Source Code @subcommand( \"compare-users\", args=[ argument(\"users\", nargs=\"+\", default=[], help=\"The IDs or screen names of the users.\"), argument(\"--compare\", \"-c\", nargs=\"+\", default=[], required=True, help=f\"The comparison attribute(s). Must be from following: {', '.join(get_args(TwitterAPI.LITERALS_COMPARE_USERS))}.\"), argument( \"--features\", \"-f\", nargs=\"+\", default=[], required=False, help=f\"Features that should be contained in the feature vector for similarity comparison. Must be from: {', '.join(get_args(TwitterAPI.SIMILARITY_FEATURES_COMPARE_USERS))}\", ), argument(\"--env\", \"-e\", type=str, default=config_file_path, required=False, help=f\"Path to .env file. Defaults to {config_file_path}.\"), argument(\"--return-timestamp\", type=bool, default=False, required=False, action=argparse.BooleanOptionalAction, help=\"Returns the UTC timestamp of the request.\"), argument( \"--output\", \"-o\", type=str, default=None, required=False, help=\"Store results in a JSON or CSV file. Specify output file path (including file name). File extension specifies file export (e.g., '.csv' for CSV file export and '.json' for JSON file export)\", ), argument(\"--encoding\", type=str, default=\"utf-8\", required=False, help=\"Encoding of the output file. Defaults to UTF-8.\"), argument(\"--append\", \"-a\", type=bool, default=False, required=False, action=argparse.BooleanOptionalAction, help=\"Add results to an existing JSON file. File needs to be specified in the --output flag.\"), ], ) def compare_users_cli(args): \"\"\"CLI function to compare multiple Twitter users with the specified comparision attribute(s).\"\"\" # read secrets secrets = read_secrets(args.env) # establish connection to the API api = TwitterAPI(**secrets) # get results result = api.compare_users(users=args.users, compare=args.compare, return_timestamp=args.return_timestamp, features=args.features) # handle output output(result, path=args.output, encoding=args.encoding, append=args.append) pass","title":"compare_users_cli"},{"location":"maintenance/cli/#compare_tweets_cli","text":"CLI function equivalent to the TwitterAPI.compare_tweets function. This function is a wrapper around the TwitterAPI.compare_tweets function and handles the inputs and outputs to the console. Args: tweets (List): Tweet IDs compare (List): Comparison attributes for Twitter users. Must be from this list . Short form: -c . features (List): Features that should be contained within the feature vectors for the similarity comparison attribute. Must be from this list . Short form: -f env (str, optional): Path to .env file. Defaults to config_file_path . If the user wishes to use different secrets for authentification, he or she can pass in the path to another .env file. This file must also have the same form, as described in the read_secrets function section. return_timestamp (bool, optional): Wheather to return the Unix timestamp of the request. Defaults to false. output (str, optional): Export file path. This argument is also used in combination with the append argument to specify that the data should be added to an existing file. If both arguments were provided, data is appended. encoding (str, optional): Encoding of the data. Defaults to UTF-8. append (bool, optional): Wheather to append the data to an existing file or not. If the flag is provided (i.e., true), the file path needs to be specified with the output argument. First, the secrets are loaded from the environment file path. Then, the authentication to the TwitterAPI using the TwitterAPI class is performed. After that, the TwitterAPI instance calls the compare_tweets function with the specified arguments. The results are either printed to the console, exported to a file, or appended to a file. Source Code @subcommand( \"compare-tweets\", args=[ argument(\"tweets\", nargs=\"+\", default=[], help=\"The IDs of the Tweets.\"), argument(\"--compare\", \"-c\", nargs=\"+\", default=[], required=True, help=f\"The comparison attribute(s). Must be the following: {', '.join(get_args(TwitterAPI.LITERALS_COMPARE_TWEETS))}.\"), argument( \"--features\", \"-f\", nargs=\"+\", default=[], required=False, help=f\"Features that should be contained in the feature vector for similarity comparison. Must be from: {', '.join(get_args(TwitterAPI.SIMILARITY_FEATURES_COMPARE_TWEETS))}\" ), argument(\"--env\", \"-e\", type=str, default=config_file_path, required=False, help=f\"Path to .env file. Defaults to {config_file_path}.\"), argument(\"--return-timestamp\", type=bool, default=False, required=False, action=argparse.BooleanOptionalAction, help=\"Returns the UTC timestamp of the request.\"), argument( \"--output\", \"-o\", type=str, default=None, required=False, help=\"Store results in a JSON or CSV file. Specify output file path (including file name). File extension specifies file export (e.g., '.csv' for CSV file export and '.json' for JSON file export)\", ), argument(\"--encoding\", type=str, default=\"utf-8\", required=False, help=\"Encoding of the output file. Defaults to UTF-8.\"), argument(\"--append\", \"-a\", type=bool, default=False, required=False, action=argparse.BooleanOptionalAction, help=\"Add results to an existing JSON file. File needs to be specified in the --output flag.\"), ], ) def compare_tweets_cli(args): \"\"\"CLI function to compare multiple Tweets with the specified comparision attribute(s).\"\"\" # read secrets secrets = read_secrets(args.env) # establish connection to the API api = TwitterAPI(**secrets) # get results result = api.compare_tweets(tweet_ids=args.tweets, compare=args.compare, return_timestamp=args.return_timestamp, features=args.features) # handle output output(result, path=args.output, encoding=args.encoding, append=args.append) pass","title":"compare_tweets_cli"},{"location":"maintenance/implementation-details/","text":"Implementation Details This package was designed to perform more detailed and advanced queries to the Twitter API and allows a more detailed analysis of Twitter data. Four classes were implemented: TwitterAPI , TwitterDataFetcher , BaseDataProcessor , TwitterDataProcessor The figure below shows the architecture of the PySNA package: PySNA Architecture depicted in UML The TwitterAPI class uses instances of TwitterDataFetcher and TwitterDataProcessor . This class serves as an interface to the user and is the main class of the package. With this class, all functions of the package (except the ones from the utils.py module) can be used. It is built on top of the famous Tweepy Python package. Therefore, it inherits the Tweepy Client class and extends it with the functions provided by this package. During the implementation of this package, the Separation of Concerns design principle was realized. That means, that every class in this package serves a single, well-defined purpose and no interleaving of different procedures exist. Whereas the TwitterDataProcessor class serves the purpose of processing Twitter-related data and the TwitterDataFetcher class is used for fetching social data from the Twitter platform using its API, the TwitterAPI class offers an interface for user interaction. Each class also has a single responsibility. The design principle of making classes and encapuslation ensures that related data and procedures are kept together within one function and/or class. An overview of all attributes and methods of the implemented classes is provided in the following UML diagram: PySNA Class Overview depicted in UML Available content: TwitterAPI TwitterDataFetcher TwitterDataProcessor BaseDataProcessor Utility Functions CLI Functions Software Testing","title":"Implementation details"},{"location":"maintenance/implementation-details/#implementation-details","text":"This package was designed to perform more detailed and advanced queries to the Twitter API and allows a more detailed analysis of Twitter data. Four classes were implemented: TwitterAPI , TwitterDataFetcher , BaseDataProcessor , TwitterDataProcessor The figure below shows the architecture of the PySNA package: PySNA Architecture depicted in UML The TwitterAPI class uses instances of TwitterDataFetcher and TwitterDataProcessor . This class serves as an interface to the user and is the main class of the package. With this class, all functions of the package (except the ones from the utils.py module) can be used. It is built on top of the famous Tweepy Python package. Therefore, it inherits the Tweepy Client class and extends it with the functions provided by this package. During the implementation of this package, the Separation of Concerns design principle was realized. That means, that every class in this package serves a single, well-defined purpose and no interleaving of different procedures exist. Whereas the TwitterDataProcessor class serves the purpose of processing Twitter-related data and the TwitterDataFetcher class is used for fetching social data from the Twitter platform using its API, the TwitterAPI class offers an interface for user interaction. Each class also has a single responsibility. The design principle of making classes and encapuslation ensures that related data and procedures are kept together within one function and/or class. An overview of all attributes and methods of the implemented classes is provided in the following UML diagram: PySNA Class Overview depicted in UML Available content: TwitterAPI TwitterDataFetcher TwitterDataProcessor BaseDataProcessor Utility Functions CLI Functions Software Testing","title":"Implementation Details"},{"location":"maintenance/repository/","text":"Repository Information Directory Tree docs/ mkdocs.yml docs/ ... site/ ... examples/ append_to_csv.ipynb append_to_json.ipynb compare_tweets.ipynb compare_users.ipynb export_to_csv.ipynb export_to_json.ipynb tweet_info.ipynb user_info.ipynb resources/ compare_tweets.json tweet_info.json user_info.csv user_info.json pysna/ api.py cli.py fetch.py process.py utils.py __init__.py tests/ config.py test_api.py test_fetch.py test_process.py test_utils.py cassettes/ ... fixtures/ ... .pre-commit-config.yaml build_deploy.sh LICENSE README.md requirements.txt setup.py Details : The docs directory contains the documentation. The mkdocs package was used to build the documentation. The mkdocs.yaml specifies the navigation and structure or the documentation. the docs/docs/ directory contains the markdowns files for the documentation. The docs/site/ directory contains the HTML and JavaScript files that build the website. The website is hosted on GitHub Pages. The examples directory contains Jupyter Notebooks that shows how the package can be used and output examples. These files are mainly used to guide the user of the package and provide additional help. The examples/resources/ directory contains saved files that were generated during a function call in one of the notebooks. Users can view these examples to get an idea of how data is saved with the help of this package. The pysna directory contains all necessary files for the pacakge. __init__.py specifies the import statement shortcuts and is mandatory to define this directory as a Python package. api.py contains the TwitterAPI class. cli.py contains the CLI wrappers and functions for the TwitterAPI class. fetch.py contains the TwitterDataFetcher class. process.py contains the BaseDataProcessor and TwitterDataProcessor classes. utils.py contains the (internal) utility functions. The tests directory contains all unit tests for the package. The cassettes folder contains all cassettes made by the VCR.py library. The fixtures folder contains all byte encoded pickle fixtures. config.py defines the base test case and configuration of test cases. test_api.py contains all test cases for the TwitterAPI class. test_fetch.py contains all test cases for the TwitterDataFetcher class. test_process.py contains all test cases for the BaseDataProcessor and TwitterDataProcessor classes. test_utils.py contains the test cases for internal utility functions. The .pre-commit.yaml files defines pre-commit hooks. They turned out to be very useful since they ensured clean coding during the implementation. The following pre-commit hooks were defined: check-yaml : auto-formatting yaml files. end-of-file-fixer : adds an empty line to the end of the file. trailing-whitespace : removes trailing whitespace from any files (except markdown files for line breaks) requirements-txt-fixer : checks the used and specified requirements from the requirements.txt detect-private-key : throws an exception if any string was found that looks like a private key/secret. fix-encoding-pragma : remove the coding pragma in a python3-only codebase. black : ensures black code style. isort : automatically sorts imports. flake8 : ensures code style according to flake8 . build_deploy.sh is used to push the package to the Python Package Index (PyPI). When running the script with the --test flag, the package will be pushed to the testing stage of PyPI instead of the production stage. LICENSE : lincense specifications. MIT license. README.md : basic readme file saying what you can do with this package. requirements.txt : dependencies used for this package. setup.py : setup file for package distribution.","title":"Repository Information"},{"location":"maintenance/repository/#repository-information","text":"","title":"Repository Information"},{"location":"maintenance/repository/#directory-tree","text":"docs/ mkdocs.yml docs/ ... site/ ... examples/ append_to_csv.ipynb append_to_json.ipynb compare_tweets.ipynb compare_users.ipynb export_to_csv.ipynb export_to_json.ipynb tweet_info.ipynb user_info.ipynb resources/ compare_tweets.json tweet_info.json user_info.csv user_info.json pysna/ api.py cli.py fetch.py process.py utils.py __init__.py tests/ config.py test_api.py test_fetch.py test_process.py test_utils.py cassettes/ ... fixtures/ ... .pre-commit-config.yaml build_deploy.sh LICENSE README.md requirements.txt setup.py Details : The docs directory contains the documentation. The mkdocs package was used to build the documentation. The mkdocs.yaml specifies the navigation and structure or the documentation. the docs/docs/ directory contains the markdowns files for the documentation. The docs/site/ directory contains the HTML and JavaScript files that build the website. The website is hosted on GitHub Pages. The examples directory contains Jupyter Notebooks that shows how the package can be used and output examples. These files are mainly used to guide the user of the package and provide additional help. The examples/resources/ directory contains saved files that were generated during a function call in one of the notebooks. Users can view these examples to get an idea of how data is saved with the help of this package. The pysna directory contains all necessary files for the pacakge. __init__.py specifies the import statement shortcuts and is mandatory to define this directory as a Python package. api.py contains the TwitterAPI class. cli.py contains the CLI wrappers and functions for the TwitterAPI class. fetch.py contains the TwitterDataFetcher class. process.py contains the BaseDataProcessor and TwitterDataProcessor classes. utils.py contains the (internal) utility functions. The tests directory contains all unit tests for the package. The cassettes folder contains all cassettes made by the VCR.py library. The fixtures folder contains all byte encoded pickle fixtures. config.py defines the base test case and configuration of test cases. test_api.py contains all test cases for the TwitterAPI class. test_fetch.py contains all test cases for the TwitterDataFetcher class. test_process.py contains all test cases for the BaseDataProcessor and TwitterDataProcessor classes. test_utils.py contains the test cases for internal utility functions. The .pre-commit.yaml files defines pre-commit hooks. They turned out to be very useful since they ensured clean coding during the implementation. The following pre-commit hooks were defined: check-yaml : auto-formatting yaml files. end-of-file-fixer : adds an empty line to the end of the file. trailing-whitespace : removes trailing whitespace from any files (except markdown files for line breaks) requirements-txt-fixer : checks the used and specified requirements from the requirements.txt detect-private-key : throws an exception if any string was found that looks like a private key/secret. fix-encoding-pragma : remove the coding pragma in a python3-only codebase. black : ensures black code style. isort : automatically sorts imports. flake8 : ensures code style according to flake8 . build_deploy.sh is used to push the package to the Python Package Index (PyPI). When running the script with the --test flag, the package will be pushed to the testing stage of PyPI instead of the production stage. LICENSE : lincense specifications. MIT license. README.md : basic readme file saying what you can do with this package. requirements.txt : dependencies used for this package. setup.py : setup file for package distribution.","title":"Directory Tree"},{"location":"maintenance/testing/","text":"Software Testing During the implementation process, a mixture of manual and automated testing was performed. For automated testing, the library VCR.py is used. Thereby, cassettes (i.e., separate files) allow recording HTTP interactions (and their metadata) from external dependencies that use HTTP requests. They are stored under the tests/cassettes folder of the repository. When the test case is rerun, the cassettes are used to simulate an HTTP request and its responses caused by the client by recreating it from the prerecorded interactions without producing any traffic on the external services. NOTE : Whenever any function using a cassette or fixture for testing is modified in its behavior, it is likely that the cassette and fixture have to be recreated as they store the results of a previous version of the function. This is especially the case when a new (comparison) attribute is made available to any of the four main functions of the TwitterAPI class. After a function was implemented, it was tested manually first, and then the cassette was recoreded. A fixture with the expected results has been stored beforehand. During the implementation, regression testing was performed to ensure the correct functionality of the software component. The fixtures are saved under the tests/fixtures directory of the repository. All fixtures are byte encoded are stored in pickle files. Besides the VCR.py package, the unittest library was used to design test cases. Config Within the config.py , the secrets are loaded and a unittest.Testcase instance was created. This test case stores the secrets as well as class instances and forms the basis for other test cases. For testing with VCR.py, bearer tokens are filtered from the headers. Source Code tape = vcr.VCR(filter_headers=[\"Authorization\"]) class PySNATestCase(unittest.TestCase): def setUp(self): self.bearer_token = bearer_token self.consumer_key = consumer_key self.consumer_secret = consumer_secret self.access_token = access_token self.access_token_secret = access_token_secret self.rapidapi_key = rapidapi_key self.rapidapi_host = rapidapi_host self.api = TwitterAPI(self.bearer_token, self.consumer_key, self.consumer_secret, self.access_token, self.access_token_secret, self.rapidapi_key, self.rapidapi_host) self.fetcher = TwitterDataFetcher(self.bearer_token, self.consumer_key, self.consumer_secret, self.access_token, self.access_token_secret, self.rapidapi_key, self.rapidapi_host) self.data_processor = TwitterDataProcessor() Testing TwitterAPI Test cases for the TwitterAPI class test the four main functions of the class. Therefore, the corresponding function is called first so get the cassette response. Then, the fixture is loaded and the results are compared. The test cases can be found under the tests/test_api.py file of the repository. Example for testing with cassettes @tape.use_cassette(\"tests/cassettes/tweet_info.yaml\") def test_tweet_info(self): cassette_response = self.api.tweet_info(test_tweet_id_1, get_args(self.api.LITERALS_TWEET_INFO)) with open(\"tests/fixtures/tweet_info.pickle\", \"rb\") as handle: expected_response = pickle.load(handle) self.assertDictEqual(cassette_response, expected_response) Testing TwitterDataFetcher For the TwitterDataFetcher class, unit testing was performed more granularly. Each function was tested with the help of cassettes and fixtures. The results were compared but also the instances and return types of each function were tested. The test cases can be found under the tests/test_fetch.py file of the repository. Example for testing with cassettes @tape.use_cassette(\"tests/cassettes/manual_request.yaml\") def test_manual_request(self): url = f\"https://api.twitter.com/2/users/{test_user_id_1}\" cassette_response = self.fetcher._manual_request(url, \"GET\", additional_fields={\"user.fields\": [\"username\"]}) self.assertIsInstance(cassette_response, dict) self.assertEqual(cassette_response[\"data\"][\"username\"], test_username_1) with open(\"tests/fixtures/manual_request.pickle\", \"rb\") as handle: expected_response = pickle.load(handle) self.assertDictEqual(cassette_response, expected_response) Here, some functions were tested for different inputs and check for the exact same output (i.e., Twitter user ID vs. screen name). Testing TwitterDataProcessor Both classes, BaseDataProcessor and TwitterDataProcessor , were tested. Each function of the `BaseDataProcessor class was tested with predefined unit tests without using cassettes. Results as well as instances were checked. The test cases can be found under the tests/test_process.py file of the repository. Example for unit testing without cassettes test_sets = {test_user_id_1: set([1, 3, 5, 7]), test_user_id_2: set([3, 6, 7, 9]), test_user_id_3: set([0, 3, 7])} def test_intersection(self): # calc intersection results = self.data_processor.intersection(test_sets.values()) # assert instances self.assertIsInstance(results, list) assert all(isinstance(item, Number) for item in results) # assert results to be equal self.assertListEqual(results, [3, 7]) For the functions of the TwitterDataProcessor class, unit tests were also defined previously. Some functions, however, required recently fetcher Twitter user or tweet objects to be tested. Therefore, cassettes and fixtures were created like for the functions of the TwitterAPI class. Example for unit testing with cassettes @tape.use_cassette(\"tests/cassettes/user_obj.yaml\") def test_extract_followers(self): cassette_user = self.api.fetcher.get_user_object(test_user_id_1) results = self.data_processor.extract_followers(cassette_user) # ensure instances self.assertIsInstance(results, dict) # compare with fixture with open(\"tests/fixtures/extract_followers.pickle\", \"rb\") as handle: test_results = pickle.load(handle) self.assertDictEqual(results, test_results) Testing Utility Functions Only the internal utility functions _tuples_to_string and _string_to_tuple are tested. Cassettes and fixture were not used as they does not rely on external data from a third-party service. These test cases were designed beforehand. The results as well as the instances were tested. The test cases can be found under the tests/test_utils.py file of the repository.","title":"Software Testing"},{"location":"maintenance/testing/#software-testing","text":"During the implementation process, a mixture of manual and automated testing was performed. For automated testing, the library VCR.py is used. Thereby, cassettes (i.e., separate files) allow recording HTTP interactions (and their metadata) from external dependencies that use HTTP requests. They are stored under the tests/cassettes folder of the repository. When the test case is rerun, the cassettes are used to simulate an HTTP request and its responses caused by the client by recreating it from the prerecorded interactions without producing any traffic on the external services. NOTE : Whenever any function using a cassette or fixture for testing is modified in its behavior, it is likely that the cassette and fixture have to be recreated as they store the results of a previous version of the function. This is especially the case when a new (comparison) attribute is made available to any of the four main functions of the TwitterAPI class. After a function was implemented, it was tested manually first, and then the cassette was recoreded. A fixture with the expected results has been stored beforehand. During the implementation, regression testing was performed to ensure the correct functionality of the software component. The fixtures are saved under the tests/fixtures directory of the repository. All fixtures are byte encoded are stored in pickle files. Besides the VCR.py package, the unittest library was used to design test cases.","title":"Software Testing"},{"location":"maintenance/testing/#config","text":"Within the config.py , the secrets are loaded and a unittest.Testcase instance was created. This test case stores the secrets as well as class instances and forms the basis for other test cases. For testing with VCR.py, bearer tokens are filtered from the headers. Source Code tape = vcr.VCR(filter_headers=[\"Authorization\"]) class PySNATestCase(unittest.TestCase): def setUp(self): self.bearer_token = bearer_token self.consumer_key = consumer_key self.consumer_secret = consumer_secret self.access_token = access_token self.access_token_secret = access_token_secret self.rapidapi_key = rapidapi_key self.rapidapi_host = rapidapi_host self.api = TwitterAPI(self.bearer_token, self.consumer_key, self.consumer_secret, self.access_token, self.access_token_secret, self.rapidapi_key, self.rapidapi_host) self.fetcher = TwitterDataFetcher(self.bearer_token, self.consumer_key, self.consumer_secret, self.access_token, self.access_token_secret, self.rapidapi_key, self.rapidapi_host) self.data_processor = TwitterDataProcessor()","title":"Config"},{"location":"maintenance/testing/#testing-twitterapi","text":"Test cases for the TwitterAPI class test the four main functions of the class. Therefore, the corresponding function is called first so get the cassette response. Then, the fixture is loaded and the results are compared. The test cases can be found under the tests/test_api.py file of the repository. Example for testing with cassettes @tape.use_cassette(\"tests/cassettes/tweet_info.yaml\") def test_tweet_info(self): cassette_response = self.api.tweet_info(test_tweet_id_1, get_args(self.api.LITERALS_TWEET_INFO)) with open(\"tests/fixtures/tweet_info.pickle\", \"rb\") as handle: expected_response = pickle.load(handle) self.assertDictEqual(cassette_response, expected_response)","title":"Testing TwitterAPI"},{"location":"maintenance/testing/#testing-twitterdatafetcher","text":"For the TwitterDataFetcher class, unit testing was performed more granularly. Each function was tested with the help of cassettes and fixtures. The results were compared but also the instances and return types of each function were tested. The test cases can be found under the tests/test_fetch.py file of the repository. Example for testing with cassettes @tape.use_cassette(\"tests/cassettes/manual_request.yaml\") def test_manual_request(self): url = f\"https://api.twitter.com/2/users/{test_user_id_1}\" cassette_response = self.fetcher._manual_request(url, \"GET\", additional_fields={\"user.fields\": [\"username\"]}) self.assertIsInstance(cassette_response, dict) self.assertEqual(cassette_response[\"data\"][\"username\"], test_username_1) with open(\"tests/fixtures/manual_request.pickle\", \"rb\") as handle: expected_response = pickle.load(handle) self.assertDictEqual(cassette_response, expected_response) Here, some functions were tested for different inputs and check for the exact same output (i.e., Twitter user ID vs. screen name).","title":"Testing TwitterDataFetcher"},{"location":"maintenance/testing/#testing-twitterdataprocessor","text":"Both classes, BaseDataProcessor and TwitterDataProcessor , were tested. Each function of the `BaseDataProcessor class was tested with predefined unit tests without using cassettes. Results as well as instances were checked. The test cases can be found under the tests/test_process.py file of the repository. Example for unit testing without cassettes test_sets = {test_user_id_1: set([1, 3, 5, 7]), test_user_id_2: set([3, 6, 7, 9]), test_user_id_3: set([0, 3, 7])} def test_intersection(self): # calc intersection results = self.data_processor.intersection(test_sets.values()) # assert instances self.assertIsInstance(results, list) assert all(isinstance(item, Number) for item in results) # assert results to be equal self.assertListEqual(results, [3, 7]) For the functions of the TwitterDataProcessor class, unit tests were also defined previously. Some functions, however, required recently fetcher Twitter user or tweet objects to be tested. Therefore, cassettes and fixtures were created like for the functions of the TwitterAPI class. Example for unit testing with cassettes @tape.use_cassette(\"tests/cassettes/user_obj.yaml\") def test_extract_followers(self): cassette_user = self.api.fetcher.get_user_object(test_user_id_1) results = self.data_processor.extract_followers(cassette_user) # ensure instances self.assertIsInstance(results, dict) # compare with fixture with open(\"tests/fixtures/extract_followers.pickle\", \"rb\") as handle: test_results = pickle.load(handle) self.assertDictEqual(results, test_results)","title":"Testing TwitterDataProcessor"},{"location":"maintenance/testing/#testing-utility-functions","text":"Only the internal utility functions _tuples_to_string and _string_to_tuple are tested. Cassettes and fixture were not used as they does not rely on external data from a third-party service. These test cases were designed beforehand. The results as well as the instances were tested. The test cases can be found under the tests/test_utils.py file of the repository.","title":"Testing Utility Functions"},{"location":"maintenance/utils/","text":"Utility Functions Some utility functions were implemented to support export and import of collected data to different formats. With the help of these functions, a comparison over time is made possible as they can be used in combination with specific arguments from the main functions of the TwitterAPI class. Besides the class-linked methods from the TwitterAPI , TwitterDataFetcher , and TwitterDataProcessor classes, utility functions were developed which are part of the utils module of the package. These are, by not being part of a class, not only usable for Twitter data exclusively, but also for data from other social media platforms and are not linked to classes or the corresponding platforms, therefore. Methods for export to CSV and JSON files were designed as well as appending new observations to existing files. All functions can be imported by running: from pysna.utils import * or name the desired functions in the import statement. In the following, functions for internal usage as well as user functions are presented. Internal Utility Functions The following functions are used internally at different places in the code. They are not intended to be used directly by users. Often, they are designed to be helper functions for contributing developers. strf_datetime Converts datetime objects to string representation. Default format is %Y-%m-%d %H:%M:%S and will return a datetime string like 2023-03-10 09:16:12.662320 . Function: strf_datetime(date: datetime, format: str = \"%Y-%m-%d %H:%M:%S\") This function takes in a date format string. Any other format different to the default one can be passed in using the format argument. This function is used internally to convert a Unix timestamp to a readable format. This is the case, for the return_timestamp argument of the four main functions from the TwitterAPI class`. Source Code def strf_datetime(date: datetime, format: str = \"%Y-%m-%d %H:%M:%S\") -> str: \"\"\"Convert datetime object to string representation. Args: date (datetime): Input datetime object format (str, optional): Datetime string format. Defaults to \"%Y-%m-%d %H:%M:%S\". Returns: str: string representation of input datetime in given format. \"\"\" return date.strftime(format) tuple_to_string (private) This function serializes tuples-keys from dictionaries to string representation. A tuple-key wil obtain a leading __tuple__ string and decomposed in list representation. This function is private as it is not intended for external usage by the package user. The reason why this function was implemented is that an export to JSON format is not possible with tuples as keys. Some of the four main functions from the TwitterAPI class will generate tuples as dictionary keys (e.g., when a relationship of two Twitter users or twees is investigated). The JSON format does not support tuples for serialization and, thus, needs a list representation of Python tuples. This function will iterate recursively through the entire dictionary that is provided to the function and will exchange the dictionary tuple-keys to a string representation. Thus, even for nested dictionaries, this function will find and convert all tuple-keys inside the dictionary. Function: _tuple_to_string(obj: Any) For instance, a tuple-key like (\"WWU_Muenster\", \"goetheuni\") will be encoded to __tuple__[\"WWU_Muenster\", \"goetheuni\"] . Then, the JSONEncoder class from the json Python module can convert this key as string. This function is used within the export_to_json function to serialize tuples inside the data dictionary. In order to avoid a manipulation of the object passed in, a deep copy of the object is performed at the beginning before conversion. Source Code def _tuple_to_string(obj: Any) -> Any: \"\"\"Serialize tuple-keys to string representation. A tuple wil obtain a leading '__tuple__' string and decomposed in list representation. Args: obj (Any): Typically a dict, tuple, list, int, or string. Returns: Any: Input object with serialized tuples. Example: A tuple (\"WWU_Muenster\", \"goetheuni\") will be encoded to \"__tuple__[\"WWU_Muenster\", \"goetheuni\"]. \"\"\" # deep copy object to avoid manipulation during iteration obj_copy = copy.deepcopy(obj) # if the object is a dictionary if isinstance(obj, dict): # iterate over every key for key in obj: # set for later to avoid modification in later iterations when this var does not get overwritten serialized_key = None # if key is tuple if isinstance(key, tuple): # stringify the key serialized_key = f\"__tuple__{list(key)}\" # replace old key with encoded key obj_copy[serialized_key] = obj_copy.pop(key) # if the key was modified if serialized_key is not None: # do it again for the next nested dictionary obj_copy[serialized_key] = _tuple_to_string(obj[key]) # else, just do it for the next dictionary else: obj_copy[key] = _tuple_to_string(obj[key]) return obj_copy string_to_tuple (private) This function converts serialized tuples back to original representation. Serialized tuples need to have a leading __tuple__ string. This function is private as no external usage by the package user is intended. This function does the opposite of what the _tuple_to_string function does. Since any tuple-keys were decomposed into a string representation through the export_to_json function, these tuples need to be recovered when the data is to be imported again. Therefore, this function is used internally within the load_from_json function. This function iterates recursively through the entire JSON data that is being loaded and decodes any serialized tuple with a leading __tuple__ string to the corresponding Python tuple representation, so that serialized tuples are recovered. Function: _string_to_tuple(obj: Any) In order to avoid a manipulation of the object passed in, a deep copy of the object is performed at the beginning before conversion. Source Code def _string_to_tuple(obj: Any) -> Any: \"\"\"Convert serialized tuples back to original representation. Tuples need to have a leading \"__tuple__\" string. Args: obj (Any): Typically a dict, tuple, list, int, or string. Returns: Any: Input object with recovered tuples. Example: A encoded tuple \"__tuple__[\"WWU_Muenster\", \"goetheuni\"] will be decoded to (\"WWU_Muenster\", \"goetheuni\"). \"\"\" # deep copy object to avoid manipulation during iteration obj_copy = copy.deepcopy(obj) # if the object is a dictionary if isinstance(obj, dict): # iterate over every key for key in obj: # set for later to avoid modification in later iterations when this var does not get overwritten serialized_key = None # if key is a serialized tuple starting with the \"__tuple__\" affix if isinstance(key, str) and key.startswith(\"__tuple__\"): # decode it so tuple serialized_key = tuple(key.split(\"__tuple__\")[1].strip(\"[]\").replace(\"'\", \"\").split(\", \")) # if key is number in string representation if all(entry.isdigit() for entry in serialized_key): # convert to integer, recover ID serialized_key = tuple(map(int, serialized_key)) # replace old key with encoded key obj_copy[serialized_key] = obj_copy.pop(key) # if the key was modified if serialized_key is not None: # do it again for the next nested dictionary obj_copy[serialized_key] = _string_to_tuple(obj[key]) # else, just do it for the next dictionary else: obj_copy[key] = _string_to_tuple(obj[key]) # if another instance was found elif isinstance(obj, list): for item in obj: _string_to_tuple(item) return obj_copy User Utility Functions These functions are designed for external usage by the package user. They allow export to JSON or CSV formats as well as appending new observations to existing files. For the JSON format specifically, a function was designed to load and recover a saved Python dictionary from a JSON file. All user utility function can be imported by running from pysna import * as they are part of the import-all shortcut. export_to_json Export dictionary data to JSON file. Tuple-keys are encoded to strings. Function: export_to_json(data: dict, export_path: str, encoding: str = \"utf-8\", ensure_ascii: bool = False, *args) Args: data (dict): Data dictionary that should be exported. export_path (str): Export path including file name and extension. encoding (str, optional): Encoding of JSON file. Defaults to \"utf-8\". ensure_ascii (bool): Wheather to convert characters to ASCII. Defaults to False. Other encodings could be specified by overwriting the default for the encoding argument. The ensure_ascii argument is used for the json.dump function from the json Python module. Additional arguments can be passed to the json.dump function by the `*args argument of this function. In case a tuple was detected in the input data dictionary, an error will be raised during the serialization since JSON does not support tuple encoding. Therefore, the TypeError or json.JSONDecodeError are caught and the data dictionary will be preprocessed by the internal _tuple_to_string function. Then, all tuple-keys inside the data dictionary will be converted to a string representation and the export will be repeated with the serialized tuples. Any exported dictionary will be exported to a JSON file of the form: { \"data\": [ ... ] } Thus, the dictionary will be stored inside the list of the data key. This allows appending new entries to the same file (for more information, see the append_to_json function). Reference: https://docs.python.org/3/library/json.html Source Code def export_to_json(data: dict, export_path: str, encoding: str = \"utf-8\", ensure_ascii: bool = False, *args): \"\"\"Export dictionary data to JSON file. Tuple-keys are encoded to strings. Args: data (dict): Data dictionary export_path (str): Export path including file name and extension. encoding (str, optional): Encoding of JSON file. Defaults to \"utf-8\". ensure_ascii (bool): Wheather to convert characters to ASCII. Defaults to False. \"\"\" try: with open(export_path, \"w\", encoding=encoding) as jsonfile: # add 'data' key in order to append additional dicts to same file, if not already exist if \"data\" not in data: serialized_data = {\"data\": [data]} # dump to json json.dump(serialized_data, jsonfile, indent=4, ensure_ascii=ensure_ascii, *args) except IOError as e: raise e # usually when tuple cannot be serialized except TypeError or json.JSONDecodeError: # serialize tuples data = _tuple_to_string(data) # retry export_to_json(data=data, export_path=export_path, encoding=encoding, ensure_ascii=ensure_ascii) pass append_to_json Append a dictionary to an existing JSON file. Tuple-keys are encoded to strings. Function: append_to_json(input_dict: Dict[str, Any], filepath: str, encoding: str = \"utf-8\", **kwargs) Args: input_dict (Dict[str, Any]): Dictionary containing new data that should be added to file. filepath (str): Absolute or relative filepath including the file extension. Depending on the current working directory. encoding (str, optional): The encoding of the file. Defaults to \"utf-8\". The function takes in a data dictionary containing the data that should be added to an existing file. Tuple-keys will be encoded to strings using the _tuples_to_strings function. If any tuple-key inside the dictionary is detected during the serialization, the corresponding TypeError and/or json.JSONDecodeError will be caught and the _tuples_to_strings will be invoked. After that, the export will be repeated. The filepath of the existing JSON file must be provided including the file extension. Other encodings different to UTF-8 can also be specified. Keyword arguments can also be passed to the json.dumps function by the **kwargs argument. The provided input data dictionary will be appended to the data key of the JSON file. Hence, the existing file must be of the form: { \"data\": [ ... ] } Source Code def append_to_json(input_dict: Dict[str, Any], filepath: str, encoding: str = \"utf-8\", **kwargs): \"\"\"Append a dictionary to an existing JSON file. Tuple-keys are encoded to strings. Args: input_dict (Dict[str, Any]): Dictionary containing new data that should be added to file. filepath (str): Absolute or relative filepath including the file extension. Depending on the current working directory. encoding (str, optional): The encoding of the file. Defaults to \"utf-8\". NOTE: Existing JSON file needs a 'data' key. Raises: ValueError: If input dict and file do not have the same keys or columns, respectively. \"\"\" # load file from path with open(filepath, \"r\", encoding=encoding) as input_file: f = json.load(input_file, **kwargs) # existing file should have a \"data\"-key and a list to append to if \"data\" not in f.keys(): raise KeyError(\"The file to be extended must contain the key 'data'.\") else: try: # serialize tuples if any exist input_dict = _tuple_to_string(input_dict) # append new dict to file f[\"data\"].append(input_dict) with open(filepath, \"w\", encoding=encoding) as jsonfile: json.dump(f, jsonfile, indent=4, **kwargs) except IOError as e: raise e # usually when tuple cannot be serialized except TypeError or json.JSONDecodeError: # serialize tuples input_dict = _tuple_to_string(input_dict) # retry append_to_json(input_dict=input_dict, filepath=filepath, encoding=encoding, **kwargs) pass load_from_json Load Python dictionary from JSON file. Tuples are recovered. Function: load_from_json(filepath: str, encoding: str = \"utf-8\", **kwargs) Args: filepath (str): Path to JSON file. encoding (str, optional): Encoding of file. Defaults to \"utf-8\". The function allows to recover a JSON serialized dictionary containing tuple-keys. Therefore, the interncal _strings_to_tuples is used. The user will get a full Python dictionary like before the export to JSON of it. Source Code def load_from_json(filepath: str, encoding: str = \"utf-8\", **kwargs) -> dict: \"\"\"Load Python Dictionary from JSON file. Tuples are recovered. Args: filepath (str): Path to JSON file. encoding (str, optional): Encoding of file. Defaults to \"utf-8\". Returns: dict: Python Dictionary containing (deserialized) data from JSON file. \"\"\" # read from filepath with open(filepath, \"r\", encoding=encoding) as jsonfile: f = json.load(jsonfile, **kwargs) if \"data\" in f: entries = [_string_to_tuple(entry) for entry in f[\"data\"]] f = {\"data\": entries} else: # try to deserialize if any tuples were found in the file f = _string_to_tuple(f) return f export_to_csv Besides the JSON export, a CSV export option is provided by this function. Dictionary data is exported to CSV files using the Pandas package. Function: export_to_csv(data: dict, export_path: str, encoding: str = \"utf-8\", sep: str = \",\", **kwargs) Args: data (dict): Data dictionary (nested dictionaries are not allowed) export_path (str): Export path including file name and extension. encoding (str, optional): Encoding of CSV file. Defaults to 'utf-8'. sep (str, optional): Value separator for CSV file. Defaults to \",\". kwargs : Keyword arguments for pd.DataFrame.to_csv. See references below for further details. The function will raise a ValueError if a nested dictionary was provided. This function was designed to allow an export of a simple one-level dictionary to a more readable format compared to JSON. However, it is highly recommended to use the JSON export function instead. Reference: https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.to_csv.html Source Code def export_to_csv(data: dict, export_path: str, encoding: str = \"utf-8\", sep: str = \",\", **kwargs): \"\"\"Export dictionary data to CSV file. Args: data (dict): Data dictionary (nested dictionaries are not allowed) export_path (str): Export path including file name and extension. encoding (str, optional): Encoding of CSV file. Defaults to 'utf-8'. sep (str, optional): Value separator for CSV file. Defaults to \",\". kwargs: Keyword arguments for pd.DataFrame.to_csv. See references below for further details. Raises: ValueError: If nested dictionary was provided. IOError: If export fails due to bad input. References: https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.to_csv.html \"\"\" # catch nested dict if any(isinstance(data[key], dict) for key in data.keys()): raise ValueError(\"'data' dictionary must not contain nested dictionaries. Use JSON export instead.\") try: # convert to pandas dataframe from dict f = pd.DataFrame(data, index=[0]) # export data frame f.to_csv(export_path, encoding=encoding, sep=sep, index=False, **kwargs) except IOError as e: raise e append_to_csv Append a dictionary to an existing CSV file. Function: append_to_csv(data: dict, filepath: str, encoding: str = \"utf-8\", sep: str = \",\", *args) Args: data (dict): Dictionary containing new data that should be added to file. filepath (str): Absolute or relative filepath including the file extension. Depending on the current working directory. encoding (str, optional): Encoding of CSV file.. Defaults to 'utf-8'. sep (str, optional): Value separator for CSV file. Defaults to \",\". args : Keyword Arguments for reading and writing from/to CSV file from pandas. Pass in: *[read_kwargs, write_kwargs]. See references below for further details on possible read/write arguments. The args argument allows to specify additional read and write options. Therefore, the user can pass in a list containing keyword arguments for read and write. Thus, the args argument has to be of the form: [{\"doublequote\": False}, # read keywords arguments {\"prefix\": \"foo_\"}] # write keywords argument Read keyword arguments will be passed to the p andas.read_csv function whereas write keyword arguments will be passed to the pandas.DataFrame.to_csv function. The function will raise a ValueError if a nested dictionary was provided. This function was designed to allow an append of a simple one-level dictionary to an existing CSV file. However, it is highly recommended to use the JSON export function instead. References: Pandas to CSV Pandas read CSV Source Code def append_to_csv(data: dict, filepath: str, encoding: str = \"utf-8\", sep: str = \",\", *args): \"\"\"Append a dictionary to an existing CSV file. Args: data (dict): Dictionary containing new data that should be added to file. filepath (str): Absolute or relative filepath including the file extension. Depending on the current working directory. encoding (str, optional): Encoding of CSV file.. Defaults to 'utf-8'. sep (str, optional): Value separator for CSV file. Defaults to \",\". args: Keyword Arguments for reading and writing from/to CSV file from pandas. Pass in: *[read_kwargs, write_kwargs]. See references below for further details on possible read/write arguments. Raises: ValueError: If nested dictionary was provided. ValueError: If 'args' does not contain a dictionaries for read and write. IOError: If export fails due to bad input. References: - https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.to_csv.html - https://pandas.pydata.org/docs/reference/api/pandas.read_csv.html \"\"\" # catch nested dict if any(isinstance(data[key], dict) for key in data.keys()): raise ValueError(\"'data' dictionary must not contain nested dictionaries. Use JSON export instead.\") if args: if any(not isinstance(kwargs, dict) for kwargs in args): raise ValueError(\"'args' must be of type list containing dictionaries.\") try: # read existing file f = pd.read_csv(filepath, sep=sep, encoding=encoding) # convert data dict to df input_df = pd.DataFrame(data, index=[0]) # concat dfs f = pd.concat([f, input_df], axis=0) # export to CSV f.to_csv(filepath, sep=sep, encoding=encoding, index=False) except IOError as e: raise e A function for CSV import was not designed as this was already implemented by the csv Python package or the Pandas package. Thus, a comparable function seemed unreasonable and redundant.","title":"Utility Functions"},{"location":"maintenance/utils/#utility-functions","text":"Some utility functions were implemented to support export and import of collected data to different formats. With the help of these functions, a comparison over time is made possible as they can be used in combination with specific arguments from the main functions of the TwitterAPI class. Besides the class-linked methods from the TwitterAPI , TwitterDataFetcher , and TwitterDataProcessor classes, utility functions were developed which are part of the utils module of the package. These are, by not being part of a class, not only usable for Twitter data exclusively, but also for data from other social media platforms and are not linked to classes or the corresponding platforms, therefore. Methods for export to CSV and JSON files were designed as well as appending new observations to existing files. All functions can be imported by running: from pysna.utils import * or name the desired functions in the import statement. In the following, functions for internal usage as well as user functions are presented.","title":"Utility Functions"},{"location":"maintenance/utils/#internal-utility-functions","text":"The following functions are used internally at different places in the code. They are not intended to be used directly by users. Often, they are designed to be helper functions for contributing developers.","title":"Internal Utility Functions"},{"location":"maintenance/utils/#strf_datetime","text":"Converts datetime objects to string representation. Default format is %Y-%m-%d %H:%M:%S and will return a datetime string like 2023-03-10 09:16:12.662320 . Function: strf_datetime(date: datetime, format: str = \"%Y-%m-%d %H:%M:%S\") This function takes in a date format string. Any other format different to the default one can be passed in using the format argument. This function is used internally to convert a Unix timestamp to a readable format. This is the case, for the return_timestamp argument of the four main functions from the TwitterAPI class`. Source Code def strf_datetime(date: datetime, format: str = \"%Y-%m-%d %H:%M:%S\") -> str: \"\"\"Convert datetime object to string representation. Args: date (datetime): Input datetime object format (str, optional): Datetime string format. Defaults to \"%Y-%m-%d %H:%M:%S\". Returns: str: string representation of input datetime in given format. \"\"\" return date.strftime(format)","title":"strf_datetime"},{"location":"maintenance/utils/#tuple_to_string-private","text":"This function serializes tuples-keys from dictionaries to string representation. A tuple-key wil obtain a leading __tuple__ string and decomposed in list representation. This function is private as it is not intended for external usage by the package user. The reason why this function was implemented is that an export to JSON format is not possible with tuples as keys. Some of the four main functions from the TwitterAPI class will generate tuples as dictionary keys (e.g., when a relationship of two Twitter users or twees is investigated). The JSON format does not support tuples for serialization and, thus, needs a list representation of Python tuples. This function will iterate recursively through the entire dictionary that is provided to the function and will exchange the dictionary tuple-keys to a string representation. Thus, even for nested dictionaries, this function will find and convert all tuple-keys inside the dictionary. Function: _tuple_to_string(obj: Any) For instance, a tuple-key like (\"WWU_Muenster\", \"goetheuni\") will be encoded to __tuple__[\"WWU_Muenster\", \"goetheuni\"] . Then, the JSONEncoder class from the json Python module can convert this key as string. This function is used within the export_to_json function to serialize tuples inside the data dictionary. In order to avoid a manipulation of the object passed in, a deep copy of the object is performed at the beginning before conversion. Source Code def _tuple_to_string(obj: Any) -> Any: \"\"\"Serialize tuple-keys to string representation. A tuple wil obtain a leading '__tuple__' string and decomposed in list representation. Args: obj (Any): Typically a dict, tuple, list, int, or string. Returns: Any: Input object with serialized tuples. Example: A tuple (\"WWU_Muenster\", \"goetheuni\") will be encoded to \"__tuple__[\"WWU_Muenster\", \"goetheuni\"]. \"\"\" # deep copy object to avoid manipulation during iteration obj_copy = copy.deepcopy(obj) # if the object is a dictionary if isinstance(obj, dict): # iterate over every key for key in obj: # set for later to avoid modification in later iterations when this var does not get overwritten serialized_key = None # if key is tuple if isinstance(key, tuple): # stringify the key serialized_key = f\"__tuple__{list(key)}\" # replace old key with encoded key obj_copy[serialized_key] = obj_copy.pop(key) # if the key was modified if serialized_key is not None: # do it again for the next nested dictionary obj_copy[serialized_key] = _tuple_to_string(obj[key]) # else, just do it for the next dictionary else: obj_copy[key] = _tuple_to_string(obj[key]) return obj_copy","title":"tuple_to_string (private)"},{"location":"maintenance/utils/#string_to_tuple-private","text":"This function converts serialized tuples back to original representation. Serialized tuples need to have a leading __tuple__ string. This function is private as no external usage by the package user is intended. This function does the opposite of what the _tuple_to_string function does. Since any tuple-keys were decomposed into a string representation through the export_to_json function, these tuples need to be recovered when the data is to be imported again. Therefore, this function is used internally within the load_from_json function. This function iterates recursively through the entire JSON data that is being loaded and decodes any serialized tuple with a leading __tuple__ string to the corresponding Python tuple representation, so that serialized tuples are recovered. Function: _string_to_tuple(obj: Any) In order to avoid a manipulation of the object passed in, a deep copy of the object is performed at the beginning before conversion. Source Code def _string_to_tuple(obj: Any) -> Any: \"\"\"Convert serialized tuples back to original representation. Tuples need to have a leading \"__tuple__\" string. Args: obj (Any): Typically a dict, tuple, list, int, or string. Returns: Any: Input object with recovered tuples. Example: A encoded tuple \"__tuple__[\"WWU_Muenster\", \"goetheuni\"] will be decoded to (\"WWU_Muenster\", \"goetheuni\"). \"\"\" # deep copy object to avoid manipulation during iteration obj_copy = copy.deepcopy(obj) # if the object is a dictionary if isinstance(obj, dict): # iterate over every key for key in obj: # set for later to avoid modification in later iterations when this var does not get overwritten serialized_key = None # if key is a serialized tuple starting with the \"__tuple__\" affix if isinstance(key, str) and key.startswith(\"__tuple__\"): # decode it so tuple serialized_key = tuple(key.split(\"__tuple__\")[1].strip(\"[]\").replace(\"'\", \"\").split(\", \")) # if key is number in string representation if all(entry.isdigit() for entry in serialized_key): # convert to integer, recover ID serialized_key = tuple(map(int, serialized_key)) # replace old key with encoded key obj_copy[serialized_key] = obj_copy.pop(key) # if the key was modified if serialized_key is not None: # do it again for the next nested dictionary obj_copy[serialized_key] = _string_to_tuple(obj[key]) # else, just do it for the next dictionary else: obj_copy[key] = _string_to_tuple(obj[key]) # if another instance was found elif isinstance(obj, list): for item in obj: _string_to_tuple(item) return obj_copy","title":"string_to_tuple (private)"},{"location":"maintenance/utils/#user-utility-functions","text":"These functions are designed for external usage by the package user. They allow export to JSON or CSV formats as well as appending new observations to existing files. For the JSON format specifically, a function was designed to load and recover a saved Python dictionary from a JSON file. All user utility function can be imported by running from pysna import * as they are part of the import-all shortcut.","title":"User Utility Functions"},{"location":"maintenance/utils/#export_to_json","text":"Export dictionary data to JSON file. Tuple-keys are encoded to strings. Function: export_to_json(data: dict, export_path: str, encoding: str = \"utf-8\", ensure_ascii: bool = False, *args) Args: data (dict): Data dictionary that should be exported. export_path (str): Export path including file name and extension. encoding (str, optional): Encoding of JSON file. Defaults to \"utf-8\". ensure_ascii (bool): Wheather to convert characters to ASCII. Defaults to False. Other encodings could be specified by overwriting the default for the encoding argument. The ensure_ascii argument is used for the json.dump function from the json Python module. Additional arguments can be passed to the json.dump function by the `*args argument of this function. In case a tuple was detected in the input data dictionary, an error will be raised during the serialization since JSON does not support tuple encoding. Therefore, the TypeError or json.JSONDecodeError are caught and the data dictionary will be preprocessed by the internal _tuple_to_string function. Then, all tuple-keys inside the data dictionary will be converted to a string representation and the export will be repeated with the serialized tuples. Any exported dictionary will be exported to a JSON file of the form: { \"data\": [ ... ] } Thus, the dictionary will be stored inside the list of the data key. This allows appending new entries to the same file (for more information, see the append_to_json function). Reference: https://docs.python.org/3/library/json.html Source Code def export_to_json(data: dict, export_path: str, encoding: str = \"utf-8\", ensure_ascii: bool = False, *args): \"\"\"Export dictionary data to JSON file. Tuple-keys are encoded to strings. Args: data (dict): Data dictionary export_path (str): Export path including file name and extension. encoding (str, optional): Encoding of JSON file. Defaults to \"utf-8\". ensure_ascii (bool): Wheather to convert characters to ASCII. Defaults to False. \"\"\" try: with open(export_path, \"w\", encoding=encoding) as jsonfile: # add 'data' key in order to append additional dicts to same file, if not already exist if \"data\" not in data: serialized_data = {\"data\": [data]} # dump to json json.dump(serialized_data, jsonfile, indent=4, ensure_ascii=ensure_ascii, *args) except IOError as e: raise e # usually when tuple cannot be serialized except TypeError or json.JSONDecodeError: # serialize tuples data = _tuple_to_string(data) # retry export_to_json(data=data, export_path=export_path, encoding=encoding, ensure_ascii=ensure_ascii) pass","title":"export_to_json"},{"location":"maintenance/utils/#append_to_json","text":"Append a dictionary to an existing JSON file. Tuple-keys are encoded to strings. Function: append_to_json(input_dict: Dict[str, Any], filepath: str, encoding: str = \"utf-8\", **kwargs) Args: input_dict (Dict[str, Any]): Dictionary containing new data that should be added to file. filepath (str): Absolute or relative filepath including the file extension. Depending on the current working directory. encoding (str, optional): The encoding of the file. Defaults to \"utf-8\". The function takes in a data dictionary containing the data that should be added to an existing file. Tuple-keys will be encoded to strings using the _tuples_to_strings function. If any tuple-key inside the dictionary is detected during the serialization, the corresponding TypeError and/or json.JSONDecodeError will be caught and the _tuples_to_strings will be invoked. After that, the export will be repeated. The filepath of the existing JSON file must be provided including the file extension. Other encodings different to UTF-8 can also be specified. Keyword arguments can also be passed to the json.dumps function by the **kwargs argument. The provided input data dictionary will be appended to the data key of the JSON file. Hence, the existing file must be of the form: { \"data\": [ ... ] } Source Code def append_to_json(input_dict: Dict[str, Any], filepath: str, encoding: str = \"utf-8\", **kwargs): \"\"\"Append a dictionary to an existing JSON file. Tuple-keys are encoded to strings. Args: input_dict (Dict[str, Any]): Dictionary containing new data that should be added to file. filepath (str): Absolute or relative filepath including the file extension. Depending on the current working directory. encoding (str, optional): The encoding of the file. Defaults to \"utf-8\". NOTE: Existing JSON file needs a 'data' key. Raises: ValueError: If input dict and file do not have the same keys or columns, respectively. \"\"\" # load file from path with open(filepath, \"r\", encoding=encoding) as input_file: f = json.load(input_file, **kwargs) # existing file should have a \"data\"-key and a list to append to if \"data\" not in f.keys(): raise KeyError(\"The file to be extended must contain the key 'data'.\") else: try: # serialize tuples if any exist input_dict = _tuple_to_string(input_dict) # append new dict to file f[\"data\"].append(input_dict) with open(filepath, \"w\", encoding=encoding) as jsonfile: json.dump(f, jsonfile, indent=4, **kwargs) except IOError as e: raise e # usually when tuple cannot be serialized except TypeError or json.JSONDecodeError: # serialize tuples input_dict = _tuple_to_string(input_dict) # retry append_to_json(input_dict=input_dict, filepath=filepath, encoding=encoding, **kwargs) pass","title":"append_to_json"},{"location":"maintenance/utils/#load_from_json","text":"Load Python dictionary from JSON file. Tuples are recovered. Function: load_from_json(filepath: str, encoding: str = \"utf-8\", **kwargs) Args: filepath (str): Path to JSON file. encoding (str, optional): Encoding of file. Defaults to \"utf-8\". The function allows to recover a JSON serialized dictionary containing tuple-keys. Therefore, the interncal _strings_to_tuples is used. The user will get a full Python dictionary like before the export to JSON of it. Source Code def load_from_json(filepath: str, encoding: str = \"utf-8\", **kwargs) -> dict: \"\"\"Load Python Dictionary from JSON file. Tuples are recovered. Args: filepath (str): Path to JSON file. encoding (str, optional): Encoding of file. Defaults to \"utf-8\". Returns: dict: Python Dictionary containing (deserialized) data from JSON file. \"\"\" # read from filepath with open(filepath, \"r\", encoding=encoding) as jsonfile: f = json.load(jsonfile, **kwargs) if \"data\" in f: entries = [_string_to_tuple(entry) for entry in f[\"data\"]] f = {\"data\": entries} else: # try to deserialize if any tuples were found in the file f = _string_to_tuple(f) return f","title":"load_from_json"},{"location":"maintenance/utils/#export_to_csv","text":"Besides the JSON export, a CSV export option is provided by this function. Dictionary data is exported to CSV files using the Pandas package. Function: export_to_csv(data: dict, export_path: str, encoding: str = \"utf-8\", sep: str = \",\", **kwargs) Args: data (dict): Data dictionary (nested dictionaries are not allowed) export_path (str): Export path including file name and extension. encoding (str, optional): Encoding of CSV file. Defaults to 'utf-8'. sep (str, optional): Value separator for CSV file. Defaults to \",\". kwargs : Keyword arguments for pd.DataFrame.to_csv. See references below for further details. The function will raise a ValueError if a nested dictionary was provided. This function was designed to allow an export of a simple one-level dictionary to a more readable format compared to JSON. However, it is highly recommended to use the JSON export function instead. Reference: https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.to_csv.html Source Code def export_to_csv(data: dict, export_path: str, encoding: str = \"utf-8\", sep: str = \",\", **kwargs): \"\"\"Export dictionary data to CSV file. Args: data (dict): Data dictionary (nested dictionaries are not allowed) export_path (str): Export path including file name and extension. encoding (str, optional): Encoding of CSV file. Defaults to 'utf-8'. sep (str, optional): Value separator for CSV file. Defaults to \",\". kwargs: Keyword arguments for pd.DataFrame.to_csv. See references below for further details. Raises: ValueError: If nested dictionary was provided. IOError: If export fails due to bad input. References: https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.to_csv.html \"\"\" # catch nested dict if any(isinstance(data[key], dict) for key in data.keys()): raise ValueError(\"'data' dictionary must not contain nested dictionaries. Use JSON export instead.\") try: # convert to pandas dataframe from dict f = pd.DataFrame(data, index=[0]) # export data frame f.to_csv(export_path, encoding=encoding, sep=sep, index=False, **kwargs) except IOError as e: raise e","title":"export_to_csv"},{"location":"maintenance/utils/#append_to_csv","text":"Append a dictionary to an existing CSV file. Function: append_to_csv(data: dict, filepath: str, encoding: str = \"utf-8\", sep: str = \",\", *args) Args: data (dict): Dictionary containing new data that should be added to file. filepath (str): Absolute or relative filepath including the file extension. Depending on the current working directory. encoding (str, optional): Encoding of CSV file.. Defaults to 'utf-8'. sep (str, optional): Value separator for CSV file. Defaults to \",\". args : Keyword Arguments for reading and writing from/to CSV file from pandas. Pass in: *[read_kwargs, write_kwargs]. See references below for further details on possible read/write arguments. The args argument allows to specify additional read and write options. Therefore, the user can pass in a list containing keyword arguments for read and write. Thus, the args argument has to be of the form: [{\"doublequote\": False}, # read keywords arguments {\"prefix\": \"foo_\"}] # write keywords argument Read keyword arguments will be passed to the p andas.read_csv function whereas write keyword arguments will be passed to the pandas.DataFrame.to_csv function. The function will raise a ValueError if a nested dictionary was provided. This function was designed to allow an append of a simple one-level dictionary to an existing CSV file. However, it is highly recommended to use the JSON export function instead. References: Pandas to CSV Pandas read CSV Source Code def append_to_csv(data: dict, filepath: str, encoding: str = \"utf-8\", sep: str = \",\", *args): \"\"\"Append a dictionary to an existing CSV file. Args: data (dict): Dictionary containing new data that should be added to file. filepath (str): Absolute or relative filepath including the file extension. Depending on the current working directory. encoding (str, optional): Encoding of CSV file.. Defaults to 'utf-8'. sep (str, optional): Value separator for CSV file. Defaults to \",\". args: Keyword Arguments for reading and writing from/to CSV file from pandas. Pass in: *[read_kwargs, write_kwargs]. See references below for further details on possible read/write arguments. Raises: ValueError: If nested dictionary was provided. ValueError: If 'args' does not contain a dictionaries for read and write. IOError: If export fails due to bad input. References: - https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.to_csv.html - https://pandas.pydata.org/docs/reference/api/pandas.read_csv.html \"\"\" # catch nested dict if any(isinstance(data[key], dict) for key in data.keys()): raise ValueError(\"'data' dictionary must not contain nested dictionaries. Use JSON export instead.\") if args: if any(not isinstance(kwargs, dict) for kwargs in args): raise ValueError(\"'args' must be of type list containing dictionaries.\") try: # read existing file f = pd.read_csv(filepath, sep=sep, encoding=encoding) # convert data dict to df input_df = pd.DataFrame(data, index=[0]) # concat dfs f = pd.concat([f, input_df], axis=0) # export to CSV f.to_csv(filepath, sep=sep, encoding=encoding, index=False) except IOError as e: raise e A function for CSV import was not designed as this was already implemented by the csv Python package or the Pandas package. Thus, a comparable function seemed unreasonable and redundant.","title":"append_to_csv"},{"location":"user-guide/installation/","text":"Installation The easiest way to install the latest version from PyPI is by using pip : pip install pysna You can also use Git to clone the repository from GitHub to install the latest development version: git clone https://github.com/mathun3003/PySNA.git cd PySNA pip install . Alternatively, install directly from the GitHub repository: pip install git+https://github.com/mathun3003/PySNA.git","title":"Installation"},{"location":"user-guide/installation/#installation","text":"The easiest way to install the latest version from PyPI is by using pip : pip install pysna You can also use Git to clone the repository from GitHub to install the latest development version: git clone https://github.com/mathun3003/PySNA.git cd PySNA pip install . Alternatively, install directly from the GitHub repository: pip install git+https://github.com/mathun3003/PySNA.git","title":"Installation"},{"location":"user-guide/overview/","text":"Available content: TwitterAPI Utility Functions CLI Tool","title":"Overview"},{"location":"user-guide/quick-start/","text":"Quick Start Import the API class for the Twitter API by writing: from pysna import TwitterAPI or import utility functions, too, by writing: from pysna import * Then, create an API instance by running: api = TwitterAPI(\"BEARER_TOKEN\", \"CONSUMER_KEY\", \"CONSUMER_SECRET\", \"ACCESS_TOKEN\", \"ACCESS_TOKEN_SECRET\") and invoke a function: api.user_info(...) Find usage and output examples in the examples folder .","title":"Quick Start"},{"location":"user-guide/quick-start/#quick-start","text":"Import the API class for the Twitter API by writing: from pysna import TwitterAPI or import utility functions, too, by writing: from pysna import * Then, create an API instance by running: api = TwitterAPI(\"BEARER_TOKEN\", \"CONSUMER_KEY\", \"CONSUMER_SECRET\", \"ACCESS_TOKEN\", \"ACCESS_TOKEN_SECRET\") and invoke a function: api.user_info(...) Find usage and output examples in the examples folder .","title":"Quick Start"},{"location":"user-guide/overview/TwitterAPI/","text":"TwitterAPI This class provides a Twitter API interface in order to interact with the Twitter Search API v2. It is built on top of the tweepy.Client class. Thus, it supports all methods from the Tweepy client. Additional functions are added. The following functions are available: user_info compare_users tweet_info compare_tweets Initialization Header: TwitterAPI(bearer_token: Optional[Any] = None, consumer_key: Optional[Any] = None, consumer_secret: Optional[Any] = None, access_token: Optional[Any] = None, access_token_secret: Optional[Any] = None, x_rapidapi_key: Optional[Any] = None, x_rapidapi_host: Optional[Any] = None, wait_on_rate_limit: bool = True) Args: bearer_token : Twitter API OAuth 2.0 Bearer Token consumer_key : Twitter API OAuth 1.0a Consumer Key consumer_secret : Twitter API OAuth 1.0a Consumer Secret access_token : Twitter API OAuth 1.0a Access Token access_token_secret : Twitter API OAuth 1.0a Access Token Secret x_rapidapi_key : Access Token for the Botometer API from the RapidAPI platform x_rapidapi_host : Host for the Botometer API from the RapidAPI platform wait_on_rate_limit : Whether to wait when rate limit is reached. Defaults to True. user_info Function: TwitterAPI.user_info(user: str | int, attributes: List[LITERALS_USER_INFO] | str, return_timestamp: bool = False) Receive requested user information from Twitter User Object. This function takes in a Twitter user identifier (i.e., an ID or unique screen name). The attributes are passed in by a list object or by a single string. For a single provided attribute, only the corresponding value is returned. For multiple attributes, a dictionary with the key-value pairs of the requested attributes is returned. If the requested attribute for the objet is not available, None will be returned. Args: user (str | int): Twitter User either specified by corresponding ID or screen name. attributes (List[LITERALS_USER_INFO] | str): Attributes of the User object. These must be from this list: Detailed description of user information attributes . See the link for detailed description of the attributes. return_timestamp (bool): Add UTC Timestamp of the request to results. Defaults to False. References: https://developer.twitter.com/en/docs/twitter-api/v1/accounts-and-users/follow-search-get-users/api-reference/get-users-lookup https://developer.twitter.com/en/docs/twitter-api/v1/data-dictionary/object-model/user Example: # request user information from the University of M\u00fcnster results = api.user_info(\"WWU_Muenster\", [\"id\", \"created_at\", \"last_active\", \"followers_count\"]) print(results) will print: {'id': 24677217, 'created_at': 'Mon Mar 16 11:19:30 +0000 2009', 'last_active': 'Wed Feb 15 13:51:04 +0000 2023', 'followers_count': 20183} compare_users Function: TwitterAPI.compare_users(users: List[str | int], compare: str | List[LITERALS_COMPARE_USERS], return_timestamp: bool = False, features: List[str] | None = None) Compare two or more users with the specified comparison attribute(s). This function takes in multiple Twitter user identifiers (i.e., IDs or unique screen names). The comparison attributes are passed in by a list object or by a single string. For a single attribute, only the corresponding value is returned. For multiple attributes, a dictionary with the key-value pairs of the requested attributes is returned. Args: users (List[str | int]): User IDs or unique screen names. compare (str | List[LITERALS_COMPARE_USERS]): Comparison attribute(s) by which users are compared. These must be from this list: Detailed description of user comparison attributes . See the link for detailed description of the attributes. return_timestamp (bool, optional): Add UTC Timestamp of the request to results. Defaults to False. features (List[str], optional): Defined features of Twitter User Object on which similarity will be computed. Must be from: followers_count , friends_count , listed_count , favourites_count , statuses_count . Must be provided if similarity comparison attribute was passed in. Defaults to None. References: https://developer.twitter.com/en/docs/twitter-api/v1/data-dictionary/object-model/user https://developer.twitter.com/en/docs/twitter-api/v1/accounts-and-users/follow-search-get-users/api-reference/get-friendships-show Example: # compare number of tweets results = api.compare_users([\"WWU_Muenster\", \"goetheuni\", \"UniKonstanz\"], compare=\"tweets_count\", return_timestamp=True) print(results) will print: {'tweets_count': { 'WWU_Muenster': 11670, 'goetheuni': 7245, 'UniKonstanz': 9857, 'metrics': { 'max': 11670, 'min': 7245, 'mean': 9590.666666666666, 'median': 9857.0, 'std': 1816.288584510243, 'var': 3298904.222222222, 'range': 4425, 'IQR': 2212.5, 'mad': 1563.777777777778}}, 'utc_timestamp': '2023-02-12 18:05:33.152930'} tweet_info Function: tweet_info(tweet_id: str | int, attributes: List[LITERALS_TWEET_INFO] | str, return_timestamp: bool = False) Receive requested Tweet information from Tweet Object. This function takes in a tweet ID as string or integer representation. The attributes are passed in by a list object or by a single string. For a single provided attribute, only the corresponding value is returned. For multiple attributes, a dictionary with the key-value pairs of the requested attributes is returned. If the requested attribute for the objet is not available, None will be returned. Args: tweet_id (str | int): Tweet ID either in string or integer representation. attributes (List[LITERALS_TWEET_INFO] | str): Attribute(s) of the Tweet object. These must be from this list: Detailed description of Tweet information attributes . See the link for detailed description of the attributes. return_timestamp (bool, optional): Add UTC Timestamp of the request to results. Defaults to False. References: https://developer.twitter.com/en/docs/twitter-api/v1/tweets/post-and-engage/api-reference/get-statuses-lookup https://developer.twitter.com/en/docs/twitter-api/v1/data-dictionary/object-model/tweet https://developer.twitter.com/en/docs/twitter-api/annotations/overview Example: # request creation date, language, and sentiment attributes for specified Tweet results = api.tweet_info(1612443577447026689, [\"created_at\", \"lang\", \"sentiment\"], return_timestamp=True) print(results) will print: { 'created_at': 'Mon Jan 09 13:38:01 +0000 2023', 'lang': 'de', 'sentiment': 'neutral', 'utc_timestamp': '2023-02-12 18:02:52.622169' } compare_tweets Function: compare_tweets(tweet_ids: List[str | int], compare: str | List[LITERALS_COMPARE_TWEETS], return_timestamp: bool = False, features: List[str] | None = None) Compare two or more Tweets with the specified comparison attribute. This function takes in multiple tweet IDs as string or integer representation. The comparison attributes are passed in by a list object or by a single string. For a single attribute, only the corresponding value is returned. For multiple attributes, a dictionary with the key-value pairs of the requested attributes is returned. Args: tweet_ids (List[str | int]): Tweet IDs either in string or integer representation. At least two Tweet IDs are required. compare (str | List[LITERALS_COMPARE_TWEETS]): Comparison attribute(s) by which Tweets are compared. These must be from this list: Detailed description of Tweet comparison attributes . See the link for detailed description of the attributes. return_timestamp (bool optional): Add UTC Timestamp of the request to results. Defaults to False. features (List[str], optional): Defined features of Tweet Object on which similarity will be computed. Must be from: public_metrics (i.e., retweet_count , reply_count , like_count , quote_count , impression_count ). Must be provided if similarity comparison attribute was passed in. Defaults to None. References: https://developer.twitter.com/en/docs/twitter-api/v1/tweets/post-and-engage/api-reference/get-statuses-lookup https://developer.twitter.com/en/docs/twitter-api/v1/data-dictionary/object-model/tweet Example: # get common liking users of specified Tweets results = api.compare_tweets(tweet_ids=[1612443577447026689, 1611301422364082180, 1612823288723476480], compare=\"common_liking_users\") print(results) will print: [3862364523] For all functions, a comparison over time can be achieved by using the return_timestamp argument for each request, storing the data in a JSON or CSV file using the export_to_json and export_to_csv , respectively, and append new records to existing files with the append_to_json or append_to_csv utility functions. Example: # request results for Tweet comparison, return timestamp results = api.compare_tweets([1612443577447026689, 1611301422364082180, 1612823288723476480], compare=[\"common_liking_users\"], return_timestamp=True) # export to JSON file export_to_json(results, export_path=\"compare_tweets.json\") # some time later... # generate new results that should be appended in the next step new_results = api.compare_tweets([1612443577447026689, 1611301422364082180, 1612823288723476480], compare=[\"common_liking_users\"], return_timestamp=True) # append to an existing file. append_to_json(new_results, \"compare_tweets.json\") The compare_tweets.json could then look like this: { \"data\": [ { \"common_liking_users\": [3862364523], \"utc_timestamp\": \"2023-02-21 11:26:45.885444\" }, { \"common_liking_users\": [3862364523, 20965264523], \"utc_timestamp\": \"2023-02-22 12:31:23.765328\" } ] }","title":"TwitterAPI"},{"location":"user-guide/overview/TwitterAPI/#twitterapi","text":"This class provides a Twitter API interface in order to interact with the Twitter Search API v2. It is built on top of the tweepy.Client class. Thus, it supports all methods from the Tweepy client. Additional functions are added. The following functions are available: user_info compare_users tweet_info compare_tweets","title":"TwitterAPI"},{"location":"user-guide/overview/TwitterAPI/#initialization","text":"Header: TwitterAPI(bearer_token: Optional[Any] = None, consumer_key: Optional[Any] = None, consumer_secret: Optional[Any] = None, access_token: Optional[Any] = None, access_token_secret: Optional[Any] = None, x_rapidapi_key: Optional[Any] = None, x_rapidapi_host: Optional[Any] = None, wait_on_rate_limit: bool = True) Args: bearer_token : Twitter API OAuth 2.0 Bearer Token consumer_key : Twitter API OAuth 1.0a Consumer Key consumer_secret : Twitter API OAuth 1.0a Consumer Secret access_token : Twitter API OAuth 1.0a Access Token access_token_secret : Twitter API OAuth 1.0a Access Token Secret x_rapidapi_key : Access Token for the Botometer API from the RapidAPI platform x_rapidapi_host : Host for the Botometer API from the RapidAPI platform wait_on_rate_limit : Whether to wait when rate limit is reached. Defaults to True.","title":"Initialization"},{"location":"user-guide/overview/TwitterAPI/#user_info","text":"Function: TwitterAPI.user_info(user: str | int, attributes: List[LITERALS_USER_INFO] | str, return_timestamp: bool = False) Receive requested user information from Twitter User Object. This function takes in a Twitter user identifier (i.e., an ID or unique screen name). The attributes are passed in by a list object or by a single string. For a single provided attribute, only the corresponding value is returned. For multiple attributes, a dictionary with the key-value pairs of the requested attributes is returned. If the requested attribute for the objet is not available, None will be returned. Args: user (str | int): Twitter User either specified by corresponding ID or screen name. attributes (List[LITERALS_USER_INFO] | str): Attributes of the User object. These must be from this list: Detailed description of user information attributes . See the link for detailed description of the attributes. return_timestamp (bool): Add UTC Timestamp of the request to results. Defaults to False. References: https://developer.twitter.com/en/docs/twitter-api/v1/accounts-and-users/follow-search-get-users/api-reference/get-users-lookup https://developer.twitter.com/en/docs/twitter-api/v1/data-dictionary/object-model/user Example: # request user information from the University of M\u00fcnster results = api.user_info(\"WWU_Muenster\", [\"id\", \"created_at\", \"last_active\", \"followers_count\"]) print(results) will print: {'id': 24677217, 'created_at': 'Mon Mar 16 11:19:30 +0000 2009', 'last_active': 'Wed Feb 15 13:51:04 +0000 2023', 'followers_count': 20183}","title":"user_info"},{"location":"user-guide/overview/TwitterAPI/#compare_users","text":"Function: TwitterAPI.compare_users(users: List[str | int], compare: str | List[LITERALS_COMPARE_USERS], return_timestamp: bool = False, features: List[str] | None = None) Compare two or more users with the specified comparison attribute(s). This function takes in multiple Twitter user identifiers (i.e., IDs or unique screen names). The comparison attributes are passed in by a list object or by a single string. For a single attribute, only the corresponding value is returned. For multiple attributes, a dictionary with the key-value pairs of the requested attributes is returned. Args: users (List[str | int]): User IDs or unique screen names. compare (str | List[LITERALS_COMPARE_USERS]): Comparison attribute(s) by which users are compared. These must be from this list: Detailed description of user comparison attributes . See the link for detailed description of the attributes. return_timestamp (bool, optional): Add UTC Timestamp of the request to results. Defaults to False. features (List[str], optional): Defined features of Twitter User Object on which similarity will be computed. Must be from: followers_count , friends_count , listed_count , favourites_count , statuses_count . Must be provided if similarity comparison attribute was passed in. Defaults to None. References: https://developer.twitter.com/en/docs/twitter-api/v1/data-dictionary/object-model/user https://developer.twitter.com/en/docs/twitter-api/v1/accounts-and-users/follow-search-get-users/api-reference/get-friendships-show Example: # compare number of tweets results = api.compare_users([\"WWU_Muenster\", \"goetheuni\", \"UniKonstanz\"], compare=\"tweets_count\", return_timestamp=True) print(results) will print: {'tweets_count': { 'WWU_Muenster': 11670, 'goetheuni': 7245, 'UniKonstanz': 9857, 'metrics': { 'max': 11670, 'min': 7245, 'mean': 9590.666666666666, 'median': 9857.0, 'std': 1816.288584510243, 'var': 3298904.222222222, 'range': 4425, 'IQR': 2212.5, 'mad': 1563.777777777778}}, 'utc_timestamp': '2023-02-12 18:05:33.152930'}","title":"compare_users"},{"location":"user-guide/overview/TwitterAPI/#tweet_info","text":"Function: tweet_info(tweet_id: str | int, attributes: List[LITERALS_TWEET_INFO] | str, return_timestamp: bool = False) Receive requested Tweet information from Tweet Object. This function takes in a tweet ID as string or integer representation. The attributes are passed in by a list object or by a single string. For a single provided attribute, only the corresponding value is returned. For multiple attributes, a dictionary with the key-value pairs of the requested attributes is returned. If the requested attribute for the objet is not available, None will be returned. Args: tweet_id (str | int): Tweet ID either in string or integer representation. attributes (List[LITERALS_TWEET_INFO] | str): Attribute(s) of the Tweet object. These must be from this list: Detailed description of Tweet information attributes . See the link for detailed description of the attributes. return_timestamp (bool, optional): Add UTC Timestamp of the request to results. Defaults to False. References: https://developer.twitter.com/en/docs/twitter-api/v1/tweets/post-and-engage/api-reference/get-statuses-lookup https://developer.twitter.com/en/docs/twitter-api/v1/data-dictionary/object-model/tweet https://developer.twitter.com/en/docs/twitter-api/annotations/overview Example: # request creation date, language, and sentiment attributes for specified Tweet results = api.tweet_info(1612443577447026689, [\"created_at\", \"lang\", \"sentiment\"], return_timestamp=True) print(results) will print: { 'created_at': 'Mon Jan 09 13:38:01 +0000 2023', 'lang': 'de', 'sentiment': 'neutral', 'utc_timestamp': '2023-02-12 18:02:52.622169' }","title":"tweet_info"},{"location":"user-guide/overview/TwitterAPI/#compare_tweets","text":"Function: compare_tweets(tweet_ids: List[str | int], compare: str | List[LITERALS_COMPARE_TWEETS], return_timestamp: bool = False, features: List[str] | None = None) Compare two or more Tweets with the specified comparison attribute. This function takes in multiple tweet IDs as string or integer representation. The comparison attributes are passed in by a list object or by a single string. For a single attribute, only the corresponding value is returned. For multiple attributes, a dictionary with the key-value pairs of the requested attributes is returned. Args: tweet_ids (List[str | int]): Tweet IDs either in string or integer representation. At least two Tweet IDs are required. compare (str | List[LITERALS_COMPARE_TWEETS]): Comparison attribute(s) by which Tweets are compared. These must be from this list: Detailed description of Tweet comparison attributes . See the link for detailed description of the attributes. return_timestamp (bool optional): Add UTC Timestamp of the request to results. Defaults to False. features (List[str], optional): Defined features of Tweet Object on which similarity will be computed. Must be from: public_metrics (i.e., retweet_count , reply_count , like_count , quote_count , impression_count ). Must be provided if similarity comparison attribute was passed in. Defaults to None. References: https://developer.twitter.com/en/docs/twitter-api/v1/tweets/post-and-engage/api-reference/get-statuses-lookup https://developer.twitter.com/en/docs/twitter-api/v1/data-dictionary/object-model/tweet Example: # get common liking users of specified Tweets results = api.compare_tweets(tweet_ids=[1612443577447026689, 1611301422364082180, 1612823288723476480], compare=\"common_liking_users\") print(results) will print: [3862364523] For all functions, a comparison over time can be achieved by using the return_timestamp argument for each request, storing the data in a JSON or CSV file using the export_to_json and export_to_csv , respectively, and append new records to existing files with the append_to_json or append_to_csv utility functions. Example: # request results for Tweet comparison, return timestamp results = api.compare_tweets([1612443577447026689, 1611301422364082180, 1612823288723476480], compare=[\"common_liking_users\"], return_timestamp=True) # export to JSON file export_to_json(results, export_path=\"compare_tweets.json\") # some time later... # generate new results that should be appended in the next step new_results = api.compare_tweets([1612443577447026689, 1611301422364082180, 1612823288723476480], compare=[\"common_liking_users\"], return_timestamp=True) # append to an existing file. append_to_json(new_results, \"compare_tweets.json\") The compare_tweets.json could then look like this: { \"data\": [ { \"common_liking_users\": [3862364523], \"utc_timestamp\": \"2023-02-21 11:26:45.885444\" }, { \"common_liking_users\": [3862364523, 20965264523], \"utc_timestamp\": \"2023-02-22 12:31:23.765328\" } ] }","title":"compare_tweets"},{"location":"user-guide/overview/Utilities/","text":"Utility Functions Utility functions are defined to read and write to specific files. The files can be imported via from pysna.utils import export_to_json, export_to_csv, append_to_json, append_to_csv, load_from_json or are included in the import-all-statement: from pysna import * Export to JSON Function: export_to_json(data: dict, export_path: str, encoding: str = 'utf-8', ensure_ascii: bool = False, *args) Export dictionary data to JSON file. Function will add a data key for the JSON file and store the provided dictionary inside the data field. Args: data (dict): Data dictionary. export_path (str): Export path including file name and extension. encoding (str, optional): Encoding of JSON file. Defaults to UTF-8. args (optional): Further arguments to be passed to json.dump() . References: https://docs.python.org/3/library/json.html NOTE: When trying to export a dictionary containing tuples as keys, the function will try to serialize them by converting tuples to strings. Then, a tuple like (\"WWU_Muenster\", \"goetheuni\") will be encoded to: \"__tuples__['WWU_Muenster', 'goetheuni']\" . For recovering the original dictionary after JSON export, use the load_from_json function. Example: # request results for Tweet comparison, return timestamp results = api.compare_tweets([1612443577447026689, 1611301422364082180, 1612823288723476480], compare=[\"common_liking_users\"], return_timestamp=True) # export to JSON file export_to_json(results, export_path=\"compare_tweets.json\") The exported compare_tweets.json file will the look like: { \"data\": [ { \"common_liking_users\": [ 3862364523 ], \"utc_timestamp\": \"2023-01-31 09:22:11.996652\" } } Append to JSON Function: append_to_json(input_dict: Dict[str, Any], filepath: str, encoding: str = \"utf-8\", **kwargs) Append a dictionary to an existing JSON file. Existing JSON file needs a 'data' key. Args: input_dict : Dictionary containing new data that should be added to file. filepath : Absolute or relative filepath including the file extension. Depending on the current working directory. encoding : The encoding of the file. Defaults to UTF-8. kwargs : Additional keyword arguments to be passed to json.dump() and json.load() References: https://docs.python.org/3/library/json.html Note: When trying to append a dictionary containing tuples as keys, the function will try to serialize them by converting tuples to strings. For recovering the original dictionary after JSON export, use the load_from_json function. Example: # generate new results that should be appended in the next step new_results = api.compare_tweets([1612443577447026689, 1611301422364082180, 1612823288723476480], compare=[\"common_liking_users\"], return_timestamp=True) # append to an existing file. append_to_json(new_results, \"compare_tweets.json\") The extended compare_tweets.json file will be supplemented with one further entry within the data field. An example output could look like: { \"data\": [ { \"common_liking_users\": [ 3862364523 ], \"utc_timestamp\": \"2023-01-31 09:22:11.996652\" }, { \"common_liking_users\": [ 3862364523 ], \"utc_timestamp\": \"2023-01-31 09:23:05.848485\" } ] } Load from JSON Function: load_from_json(filepath: str, encoding: str = \"utf-8\", **kwargs) -> dict Load Python Dictionary from JSON file. Tuples are recovered. Args: filepath (str): Path to JSON file. encoding (str, optional): Encoding of file. Defaults to UTF-8. kwargs (optional): Keyword arguments to be passed to json.load() . Returns: Python Dictionary containing (deserialized) data from JSON file. References: https://docs.python.org/3/library/json.html NOTE : Tuples that have been encoded by the export_to_json function with a leading __tuples__ string will be recovered to original tuple representation. For instance, a encoded tuple __tuple__ [\"WWU_Muenster\", \"goetheuni\"] will be returned as (\"WWU_Muenster\", \"goetheuni\") . Example: Suppose an example.json file containing one entry with a serialized tuple key: { \"data\": [ { \"__tuple__ ['WWU_Muenster', 'goetheuni']\": 0.578077 } ] } By calling: from pysna.utils import load_from_json data = load_from_json(\"example.json\") print(data) the tuple will be recovered and a conventional Python Dictionary will be returned: {(\"WWU_Muenster\", \"goetheuni\"): 0.578077} Export to CSV Function: export_to_csv(data: dict, export_path: str, encoding: str = \"utf-8\", sep: str = \",\", **kwargs) Export dictionary data to CSV file. Will raise an exception if data dictionary contains nested dictionaries. Args: data (dict): Data dictionary export_path (str): Exportpath including file name and extension. encoding (str, optional): Encoding of CSV file. Defaults to UTF-8. sep (str, optional): Value separator for CSV file. Defaults to ',' . kwargs (optional): Keyword arguments for pandas.DataFrame.to_csv . References: https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.to_csv.html Example: # request results for user information, return timestamp results = api.user_info(\"WWU_Muenster\", [\"id\", \"location\", \"friends_count\", \"followers_count\", \"last_active\", \"statuses_count\"], return_timestamp=True) # export to CSV file export_to_csv(results, export_path=\"user_info.csv\") Append to CSV Function: append_to_csv(data: dict, filepath: str, encoding: str = \"utf-8\", sep: str = \",\", *args) Append a dictionary to an existing CSV file. Will raise an exception if data dictionary contains nested dictionaries. Args: data (dict): Dictionary containing new data that should be added to file. filepath (str): Absolute or relative filepath including the file extension. Depending on the current working directory. encoding (str, optional): Encoding of CSV file. Defaults to UTF-8. sep (str, optional): Value separator for CSV file. Defaults to \",\". args : Keyword Arguments for reading and writing from/to CSV file from pandas. Pass in: *[read_kwargs, write_kwargs] , whereas both are dictionaries (i.e., provide a list of two dictionaries). References: https://pandas.pydata.org/docs/reference/api/pandas.read_csv.html https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.to_csv.html Example: # request results for user information, return timestamp results = api.user_info(\"WWU_Muenster\", [\"id\", \"location\", \"friends_count\", \"followers_count\", \"last_active\", \"statuses_count\"], return_timestamp=True) # export to CSV file append_to_csv(results, filepath=\"user_info.csv\") Notes Only JSON and CSV file formats are supported, yet.","title":"Utility Functions"},{"location":"user-guide/overview/Utilities/#utility-functions","text":"Utility functions are defined to read and write to specific files. The files can be imported via from pysna.utils import export_to_json, export_to_csv, append_to_json, append_to_csv, load_from_json or are included in the import-all-statement: from pysna import *","title":"Utility Functions"},{"location":"user-guide/overview/Utilities/#export-to-json","text":"Function: export_to_json(data: dict, export_path: str, encoding: str = 'utf-8', ensure_ascii: bool = False, *args) Export dictionary data to JSON file. Function will add a data key for the JSON file and store the provided dictionary inside the data field. Args: data (dict): Data dictionary. export_path (str): Export path including file name and extension. encoding (str, optional): Encoding of JSON file. Defaults to UTF-8. args (optional): Further arguments to be passed to json.dump() . References: https://docs.python.org/3/library/json.html NOTE: When trying to export a dictionary containing tuples as keys, the function will try to serialize them by converting tuples to strings. Then, a tuple like (\"WWU_Muenster\", \"goetheuni\") will be encoded to: \"__tuples__['WWU_Muenster', 'goetheuni']\" . For recovering the original dictionary after JSON export, use the load_from_json function. Example: # request results for Tweet comparison, return timestamp results = api.compare_tweets([1612443577447026689, 1611301422364082180, 1612823288723476480], compare=[\"common_liking_users\"], return_timestamp=True) # export to JSON file export_to_json(results, export_path=\"compare_tweets.json\") The exported compare_tweets.json file will the look like: { \"data\": [ { \"common_liking_users\": [ 3862364523 ], \"utc_timestamp\": \"2023-01-31 09:22:11.996652\" } }","title":"Export to JSON"},{"location":"user-guide/overview/Utilities/#append-to-json","text":"Function: append_to_json(input_dict: Dict[str, Any], filepath: str, encoding: str = \"utf-8\", **kwargs) Append a dictionary to an existing JSON file. Existing JSON file needs a 'data' key. Args: input_dict : Dictionary containing new data that should be added to file. filepath : Absolute or relative filepath including the file extension. Depending on the current working directory. encoding : The encoding of the file. Defaults to UTF-8. kwargs : Additional keyword arguments to be passed to json.dump() and json.load() References: https://docs.python.org/3/library/json.html Note: When trying to append a dictionary containing tuples as keys, the function will try to serialize them by converting tuples to strings. For recovering the original dictionary after JSON export, use the load_from_json function. Example: # generate new results that should be appended in the next step new_results = api.compare_tweets([1612443577447026689, 1611301422364082180, 1612823288723476480], compare=[\"common_liking_users\"], return_timestamp=True) # append to an existing file. append_to_json(new_results, \"compare_tweets.json\") The extended compare_tweets.json file will be supplemented with one further entry within the data field. An example output could look like: { \"data\": [ { \"common_liking_users\": [ 3862364523 ], \"utc_timestamp\": \"2023-01-31 09:22:11.996652\" }, { \"common_liking_users\": [ 3862364523 ], \"utc_timestamp\": \"2023-01-31 09:23:05.848485\" } ] }","title":"Append to JSON"},{"location":"user-guide/overview/Utilities/#load-from-json","text":"Function: load_from_json(filepath: str, encoding: str = \"utf-8\", **kwargs) -> dict Load Python Dictionary from JSON file. Tuples are recovered. Args: filepath (str): Path to JSON file. encoding (str, optional): Encoding of file. Defaults to UTF-8. kwargs (optional): Keyword arguments to be passed to json.load() . Returns: Python Dictionary containing (deserialized) data from JSON file. References: https://docs.python.org/3/library/json.html NOTE : Tuples that have been encoded by the export_to_json function with a leading __tuples__ string will be recovered to original tuple representation. For instance, a encoded tuple __tuple__ [\"WWU_Muenster\", \"goetheuni\"] will be returned as (\"WWU_Muenster\", \"goetheuni\") . Example: Suppose an example.json file containing one entry with a serialized tuple key: { \"data\": [ { \"__tuple__ ['WWU_Muenster', 'goetheuni']\": 0.578077 } ] } By calling: from pysna.utils import load_from_json data = load_from_json(\"example.json\") print(data) the tuple will be recovered and a conventional Python Dictionary will be returned: {(\"WWU_Muenster\", \"goetheuni\"): 0.578077}","title":"Load from JSON"},{"location":"user-guide/overview/Utilities/#export-to-csv","text":"Function: export_to_csv(data: dict, export_path: str, encoding: str = \"utf-8\", sep: str = \",\", **kwargs) Export dictionary data to CSV file. Will raise an exception if data dictionary contains nested dictionaries. Args: data (dict): Data dictionary export_path (str): Exportpath including file name and extension. encoding (str, optional): Encoding of CSV file. Defaults to UTF-8. sep (str, optional): Value separator for CSV file. Defaults to ',' . kwargs (optional): Keyword arguments for pandas.DataFrame.to_csv . References: https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.to_csv.html Example: # request results for user information, return timestamp results = api.user_info(\"WWU_Muenster\", [\"id\", \"location\", \"friends_count\", \"followers_count\", \"last_active\", \"statuses_count\"], return_timestamp=True) # export to CSV file export_to_csv(results, export_path=\"user_info.csv\")","title":"Export to CSV"},{"location":"user-guide/overview/Utilities/#append-to-csv","text":"Function: append_to_csv(data: dict, filepath: str, encoding: str = \"utf-8\", sep: str = \",\", *args) Append a dictionary to an existing CSV file. Will raise an exception if data dictionary contains nested dictionaries. Args: data (dict): Dictionary containing new data that should be added to file. filepath (str): Absolute or relative filepath including the file extension. Depending on the current working directory. encoding (str, optional): Encoding of CSV file. Defaults to UTF-8. sep (str, optional): Value separator for CSV file. Defaults to \",\". args : Keyword Arguments for reading and writing from/to CSV file from pandas. Pass in: *[read_kwargs, write_kwargs] , whereas both are dictionaries (i.e., provide a list of two dictionaries). References: https://pandas.pydata.org/docs/reference/api/pandas.read_csv.html https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.to_csv.html Example: # request results for user information, return timestamp results = api.user_info(\"WWU_Muenster\", [\"id\", \"location\", \"friends_count\", \"followers_count\", \"last_active\", \"statuses_count\"], return_timestamp=True) # export to CSV file append_to_csv(results, filepath=\"user_info.csv\")","title":"Append to CSV"},{"location":"user-guide/overview/Utilities/#notes","text":"Only JSON and CSV file formats are supported, yet.","title":"Notes"},{"location":"user-guide/overview/cli/","text":"Command-line Interface Tool The main functions from the TwitterAPI class are also available on the CLI. To see the usage instructions and help, run: pysna -h If you wish to see the usage instructions for a function, run: pysna <function> --help For example, if you want to request a comparison of two users, you can run: pysna compare-users \"WWU_Muenster\" \"goetheuni\" -c \"tweets_count\" \"common_followers\" -o \"results.json\" --return-timestamp This will perform a comparison on the \"WWU_Muenster\" and \"goetheuni\" Twitter Accounts based their number of composed Tweets and their common followers. The results are exported to the results.json file. Also, the timestamp of the request will be returned. NOTE : Every request needs valid credentials for the official Twitter API. Thus, pass in a .env file to every function call by using the --env flag or use the set-secrets function to set the API secrets for upcoming requests (recommended). Functions set-secrets In order to set the API secrets to run every command from any working directory, it is recommended to use this function. The function will copy the given .env file to the ~/.pysna/config/ directory and will create a config file containing your API secrets. This file will be read every time a request is made. If you wish to overwrite the existing config file containing the secrets, rerun this function with a new .env file. If you wish to use other secrets for authentification sporadically, you can use the --env flag of every function to use different secrets than specified in the config file. NOTE : The provided .env file must have the format: BEARER_TOKEN=... CONSUMER_KEY=... CONSUMER_SECRET=... ACCESS_TOKEN=... ACCESS_TOKEN_SECRET=... X_RAPIDAPI_KEY=... X_RAPIDAPI_HOST=... BEARER_TOKEN : Twitter API OAuth 2.0 Bearer Token CONSUMER_KEY : Twitter API OAuth 1.0a Consumer Key CONSUMER_SECRET : Twitter API OAuth 1.0a Consumer Secret ACCESS_TOKEN : Twitter API OAuth 1.0a Access Token ACCESS_TOKEN_SECRET : Twitter API OAuth 1.0a Access Token Secret X_RAPIDAPI_KEY : Access Token for the Botometer API from the RapidAPI platform X_RAPIDAPI_HOST : Host for the Botometer API from the RapidAPI platform Only .env files are supported, yet. Example: pysna set-secrets local.env or, if you want to use different secrets for a request, use the --env flag: pysna compare-users [...] --env ./local.env user-info Command: pysna user-info <user> <attributes> [--return-timestamp] [--output] [--append] [--encoding] [--env] Args: user (required): Twitter User ID or unique screen name attributes (required): pass in desired attributes separated by space. For a list of attributes, see here . return-timestamp (optional): return UTC timestamp of the query. output (optional): writes the output to a file. Pass in the file path and file name including the extension. If empty, output is printed to the CLI. Currently, CSV and JSON exports are supported. (e.g., write output.json for JSON export.). Flag short form: -o . append (optional): appends the output to an existing file. Pass in the path to the existing file with the output flag. encoding (optional): specify file encoding. Defaults to UTF-8. env (positional): specify path to environment file. Defaults to ~/.pysna/config/secrets.env (i.e., the config file path set via the set-secrets function). Flag short form: -e . compare-users Command: pysna compare-users <users> -c <compare> [--features] [--return-timestamp] [--output] [--append] [--encoding] [--env] Args: users (required): IDs or unique screen names of Twitter users. Pass in the users separated by space. compare (required): Comparison attributes Must be from the following: relationship , followers_count , followees_count , tweets_count , favourites_count , common_followers , distinct_followers , common_followees , distinct_followees , commonly_liked_tweets , distinctly_liked_tweets , similarity , created_at , protected , verified . For an overview of what the comparison attributes do, see here . Provide the comparison attributes separated by space after the -c flag. features (positional): Define the components of the feature vector for the similarity comparison attribute. Must be passed in if the aforementioned comparison attribute was provided. Features must be from: followers_count , friends_count , listed_count , favourites_count , statuses_count . return-timestamp (optional): return UTC timestamp of the query. output (optional): writes the output to a file. Pass in the file path and file name including the extension. If empty, output is printed to the CLI. Currently, CSV and JSON exports are supported. (e.g., write output.json for JSON export.). Flag short form: -o . append (optional): appends the output to an existing file. Pass in the path to the existing file with the output flag. encoding (optional): specify file encoding. Defaults to UTF-8. env (positional): specify path to environment file. Defaults to ~/.pysna/config/secrets.env (i.e., the config file path set via the set-secrets function). Flag short form: -e . tweet-info Command: pysna tweet-info <tweet> <attributes> [--return-timestamp] [--output] [--append] [--encoding] [--env] Args: tweet (required): Unique Tweet ID. attributes (required): pass in desired attributes separated by space. For a list of attributes, see here . return-timestamp (optional): return UTC timestamp of the query. output (optional): writes the output to a file. Pass in the file path and file name including the extension. If empty, output is printed to the CLI. Currently, CSV and JSON exports are supported. (e.g., write output.json for JSON export.) Flag short form: -o . append (optional): appends the output to an existing file. Pass in the path to the existing file with the output flag. encoding (optional): specify file encoding. Defaults to UTF-8. env (positional): specify path to environment file. Defaults to ~/.pysna/config/secrets.env (i.e., the config file path set via the set-secrets function). Flag short form: -e . compare-tweets Command: pysna compare-tweets <tweets> -c <compare> [--features] [--return-timestamp] [--output] [--append] [--encoding] [--env] Args: tweets (required): Unique Tweet IDs separated by space. compare (required): Comparison attributes Must be from the following: view_count , like_count , retweet_count , quote_count , reply_count , common_quoting_users , distinct_quoting_users , common_liking_users , distinct_liking_users , common_retweeters , distinct_retweeters , similarity , created_at . For an overview of what the comparison attributes do, see here . Provide the comparison attributes separated by space after the -c flag. features (positional): Define the components of the feature vector for the similarity comparison attribute. Must be passed in if the aforementioned comparison attribute was provided. Features must be from: retweet_count , favorite_count . return-timestamp (optional): return UTC timestamp of the query. output (optional): writes the output to a file. Pass in the file path and file name including the extension. If empty, output is printed to the CLI. Currently, CSV and JSON exports are supported. (e.g., write output.json for JSON export.). Flag short form: -o . append (optional): appends the output to an existing file. Pass in the path to the existing file with the output flag. encoding (optional): specify file encoding. Defaults to UTF-8. env (positional): specify path to environment file. Defaults to ~/.pysna/config/secrets.env (i.e., the config file path set via the set-secrets function). Flag short form: -e .","title":"CLI Tool"},{"location":"user-guide/overview/cli/#command-line-interface-tool","text":"The main functions from the TwitterAPI class are also available on the CLI. To see the usage instructions and help, run: pysna -h If you wish to see the usage instructions for a function, run: pysna <function> --help For example, if you want to request a comparison of two users, you can run: pysna compare-users \"WWU_Muenster\" \"goetheuni\" -c \"tweets_count\" \"common_followers\" -o \"results.json\" --return-timestamp This will perform a comparison on the \"WWU_Muenster\" and \"goetheuni\" Twitter Accounts based their number of composed Tweets and their common followers. The results are exported to the results.json file. Also, the timestamp of the request will be returned. NOTE : Every request needs valid credentials for the official Twitter API. Thus, pass in a .env file to every function call by using the --env flag or use the set-secrets function to set the API secrets for upcoming requests (recommended).","title":"Command-line Interface Tool"},{"location":"user-guide/overview/cli/#functions","text":"","title":"Functions"},{"location":"user-guide/overview/cli/#set-secrets","text":"In order to set the API secrets to run every command from any working directory, it is recommended to use this function. The function will copy the given .env file to the ~/.pysna/config/ directory and will create a config file containing your API secrets. This file will be read every time a request is made. If you wish to overwrite the existing config file containing the secrets, rerun this function with a new .env file. If you wish to use other secrets for authentification sporadically, you can use the --env flag of every function to use different secrets than specified in the config file. NOTE : The provided .env file must have the format: BEARER_TOKEN=... CONSUMER_KEY=... CONSUMER_SECRET=... ACCESS_TOKEN=... ACCESS_TOKEN_SECRET=... X_RAPIDAPI_KEY=... X_RAPIDAPI_HOST=... BEARER_TOKEN : Twitter API OAuth 2.0 Bearer Token CONSUMER_KEY : Twitter API OAuth 1.0a Consumer Key CONSUMER_SECRET : Twitter API OAuth 1.0a Consumer Secret ACCESS_TOKEN : Twitter API OAuth 1.0a Access Token ACCESS_TOKEN_SECRET : Twitter API OAuth 1.0a Access Token Secret X_RAPIDAPI_KEY : Access Token for the Botometer API from the RapidAPI platform X_RAPIDAPI_HOST : Host for the Botometer API from the RapidAPI platform Only .env files are supported, yet. Example: pysna set-secrets local.env or, if you want to use different secrets for a request, use the --env flag: pysna compare-users [...] --env ./local.env","title":"set-secrets"},{"location":"user-guide/overview/cli/#user-info","text":"Command: pysna user-info <user> <attributes> [--return-timestamp] [--output] [--append] [--encoding] [--env] Args: user (required): Twitter User ID or unique screen name attributes (required): pass in desired attributes separated by space. For a list of attributes, see here . return-timestamp (optional): return UTC timestamp of the query. output (optional): writes the output to a file. Pass in the file path and file name including the extension. If empty, output is printed to the CLI. Currently, CSV and JSON exports are supported. (e.g., write output.json for JSON export.). Flag short form: -o . append (optional): appends the output to an existing file. Pass in the path to the existing file with the output flag. encoding (optional): specify file encoding. Defaults to UTF-8. env (positional): specify path to environment file. Defaults to ~/.pysna/config/secrets.env (i.e., the config file path set via the set-secrets function). Flag short form: -e .","title":"user-info"},{"location":"user-guide/overview/cli/#compare-users","text":"Command: pysna compare-users <users> -c <compare> [--features] [--return-timestamp] [--output] [--append] [--encoding] [--env] Args: users (required): IDs or unique screen names of Twitter users. Pass in the users separated by space. compare (required): Comparison attributes Must be from the following: relationship , followers_count , followees_count , tweets_count , favourites_count , common_followers , distinct_followers , common_followees , distinct_followees , commonly_liked_tweets , distinctly_liked_tweets , similarity , created_at , protected , verified . For an overview of what the comparison attributes do, see here . Provide the comparison attributes separated by space after the -c flag. features (positional): Define the components of the feature vector for the similarity comparison attribute. Must be passed in if the aforementioned comparison attribute was provided. Features must be from: followers_count , friends_count , listed_count , favourites_count , statuses_count . return-timestamp (optional): return UTC timestamp of the query. output (optional): writes the output to a file. Pass in the file path and file name including the extension. If empty, output is printed to the CLI. Currently, CSV and JSON exports are supported. (e.g., write output.json for JSON export.). Flag short form: -o . append (optional): appends the output to an existing file. Pass in the path to the existing file with the output flag. encoding (optional): specify file encoding. Defaults to UTF-8. env (positional): specify path to environment file. Defaults to ~/.pysna/config/secrets.env (i.e., the config file path set via the set-secrets function). Flag short form: -e .","title":"compare-users"},{"location":"user-guide/overview/cli/#tweet-info","text":"Command: pysna tweet-info <tweet> <attributes> [--return-timestamp] [--output] [--append] [--encoding] [--env] Args: tweet (required): Unique Tweet ID. attributes (required): pass in desired attributes separated by space. For a list of attributes, see here . return-timestamp (optional): return UTC timestamp of the query. output (optional): writes the output to a file. Pass in the file path and file name including the extension. If empty, output is printed to the CLI. Currently, CSV and JSON exports are supported. (e.g., write output.json for JSON export.) Flag short form: -o . append (optional): appends the output to an existing file. Pass in the path to the existing file with the output flag. encoding (optional): specify file encoding. Defaults to UTF-8. env (positional): specify path to environment file. Defaults to ~/.pysna/config/secrets.env (i.e., the config file path set via the set-secrets function). Flag short form: -e .","title":"tweet-info"},{"location":"user-guide/overview/cli/#compare-tweets","text":"Command: pysna compare-tweets <tweets> -c <compare> [--features] [--return-timestamp] [--output] [--append] [--encoding] [--env] Args: tweets (required): Unique Tweet IDs separated by space. compare (required): Comparison attributes Must be from the following: view_count , like_count , retweet_count , quote_count , reply_count , common_quoting_users , distinct_quoting_users , common_liking_users , distinct_liking_users , common_retweeters , distinct_retweeters , similarity , created_at . For an overview of what the comparison attributes do, see here . Provide the comparison attributes separated by space after the -c flag. features (positional): Define the components of the feature vector for the similarity comparison attribute. Must be passed in if the aforementioned comparison attribute was provided. Features must be from: retweet_count , favorite_count . return-timestamp (optional): return UTC timestamp of the query. output (optional): writes the output to a file. Pass in the file path and file name including the extension. If empty, output is printed to the CLI. Currently, CSV and JSON exports are supported. (e.g., write output.json for JSON export.). Flag short form: -o . append (optional): appends the output to an existing file. Pass in the path to the existing file with the output flag. encoding (optional): specify file encoding. Defaults to UTF-8. env (positional): specify path to environment file. Defaults to ~/.pysna/config/secrets.env (i.e., the config file path set via the set-secrets function). Flag short form: -e .","title":"compare-tweets"},{"location":"user-guide/overview/literals-compare-tweets/","text":"Detailed Description of the Attributes for the compare_tweets function: view_count : Compares the number of views the specified Tweets currently have. Will return additional statistical metrics on the numbers of views. like_count : Compares the number of likes the specified Tweets currently have. Will return additional statistical metrics on the numbers of likes. retweet_count : Compares the number of Retweets the specified Tweets currently have. Will return additional statistical metrics on the numbers of Retweets. quote_count : Compares the number of quotes the specified Tweets currently have. Will return additional statistical metrics on the numbers of quotes. reply_count : Compares the number of replies the specified Tweets currently have. Will return additional statistical metrics on the numbers of replies. common_quoting_users : Returns the set of quoting Twitter users all specified Tweets have in common. distinct_quoting_users : Returns the sets of distinct quoting Twitter users all specified Tweets have (i.e., the difference between the quoting Twitter users of all Tweets is calculated). common_liking_users : Returns the set of liking Twitter users all specified Tweets have in common. distinct_liking_users : Returns the sets of distinct liking Twitter users all specified Tweets have (i.e., the difference between the liking Twitter users of all Tweets is calculated). common_retweeters : Returns the set of retweeters all specified Tweets have in common. distinct_retweeters : Returns the sets of distinct retweeters all specified Tweets have (i.e., the difference between the retweeters of all Tweets is calculated). similarity : Computes the euclidean distance between two feature vectors. Each feature vector contains numerical attributes from each Tweet. The features that should be contained in the feature vector have to be provided in the features argument of the function. Available features are: retweet_count : The number of times a tweet was retweeted. reply_count : Number of replies a Tweet has. like_count : Number of likes a Tweet has. quote_count : Number of quotes a Tweet has. impression_count : Number of views a Tweet has. If more than two Tweets were provided, all possible pairs of combinations will be returned containing a distance. The smaller the distance, the more similar the Tweets are. Output will be sorted in ascending order, thus, most similar Tweets are on top. Each entry in the output contains a pair of two Tweets. created_at : Compares the specified Tweets on their creation dates. Additional Will return additional statistical metrics on the dates.","title":"Literals compare tweets"},{"location":"user-guide/overview/literals-compare-users/","text":"Detailed Description of the Attributes for the compare_users function: relationship : Returns detailed information about the relationship between a pair two arbitrary users. If more than two users were provided, all possible pairs of relationships will be returned. followers_count : Compares the number of followers the specified accounts currently have. Will return additional statistical metrics on the numbers of followers. followees_count : Compares the number of friends (AKA their \u201cfollowings\u201d or \"followees\") the specified accounts currently have. Will return additional statistical metrics on the numbers of friends. tweets_count : Compares the number of composed tweets the specified accounts currently have. Will return additional statistical metrics on the numbers of tweets. favourites_count : Compares the number of liked tweets the specified accounts currently have. Will return additional statistical metrics on the numbers of liked tweets. British spelling used in the field name for historical reasons. common_followers : Returns the set of followers all specified accounts have in common. distinct_followers : Returns the sets of distinct followers all specified accounts have (i.e., the difference between the followers of all accounts is calculated). common_followees : Returns the set of friends (AKA their \u201cfollowings\u201d or \"followees\") all specified accounts have in common. distinct_followees : Returns the sets of distinct friends (AKA their \u201cfollowings\u201d or \"followees\") all specified accounts have (i.e., the difference between the friends of all accounts is calculated). commonly_liked_tweets : Returns the set of liked tweets all specified accounts have in common. distinctly_liked_tweets : Returns the sets of distinct liked tweets all specified accounts have (i.e., the difference between all liked tweets of all accounts is calculated). similarity : Computes the euclidean distance between two feature vectors. Each feature vector contains numerical attributes from each user. The features that should be contained in the feature vector have to be provided in the features argument of the function. Available features are: followers_count : The number of followers this account currently has. friends_count : The number of users this account is following (AKA their \u201cfollowings\u201d or \"followees\"). listed_count : The number of public lists that this user is a member of. favourites_count : The number of Tweets this user has liked in the account\u2019s lifetime. British spelling used in the field name for historical reasons. statuses_count : The number of Tweets (including retweets) issued by the user. If more than two users were provided, all possible pairs of combinations will be returned containing a distance. The smaller the distance, the more similar the users are. Output will be sorted in ascending order, thus, most similar users are on top. Each entry in the output contains a pair of two users. created_at : Compares the specified accounts on their creation dates. Additional Will return additional statistical metrics on the dates. protected : Compares users on their protected attribute. When true, indicates that this user has chosen to protect their Tweets. verified : Compares users on their verified attribute. When true, indicates that the user has a verified account.","title":"Literals compare users"},{"location":"user-guide/overview/literals-tweet-info/","text":"Detailed Description of the Attributes for the tweet_info function: id : The integer representation of the unique identifier for this Tweet. id_str : The string representation of the unique identifier for this Tweet. full_text : The actual UTF-8 full text of the status update. If the tweet is a retweet (marked by a leading 'RT' within the text), the text will be truncated to 140 characters. The full text of the original tweet is contained within the display_text_range : The range of the tweet text characters provided as array containing the indexes of first and last character. retweeted_status field. Hence, add the retweeted_status fields to the attributes list and see under retweeted_status -> full_text fields for the full text of the retweet. truncated : Indicates whether the value of the text parameter was truncated, for example, as a result of a retweet exceeding the original Tweet text length limit of 140 characters. Truncated text will end in ellipsis, like this ... Since Twitter now rejects long Tweets vs truncating them, the large majority of Tweets will have this set to false . Note that while native retweets may have their toplevel text property shortened, the original text will be available under the retweeted_status object and the truncated parameter will be set to the value of the original status (in most cases, false ). created_at : UTC time when this Tweet was created. entities : Entities which have been parsed out of the text of the Tweet (hashtags, URLs, user mentions, media, symbols, polls). tweet_annotations : Context annotations and named entities of the Tweet object. Context annotations are derived from the analysis of a Tweet\u2019s text and will include a domain and entity pairing which can be used to discover Tweets on topics that may have been previously difficult to surface. Named Entities are comprised of people, places, products, and organizations. Entities are delivered as part of the entity payload section. They are programmatically assigned based on what is explicitly mentioned (named-entity recognition) in the Tweet text. See the official website for further details. source : Utility used to post the Tweet, as an HTML-formatted string. Tweets from the Twitter website have a source value of web . retweeters : Twitter user IDs from retweeters. in_reply_to_status_id : If the represented Tweet is a reply, this field will contain the integer representation of the original Tweet\u2019s ID. (Nullable) in_reply_to_status_id_str : If the represented Tweet is a reply, this field will contain the string representation of the original Tweet\u2019s ID. (Nullable) in_reply_to_user_id : If the represented Tweet is a reply, this field will contain the integer representation of the original Tweet\u2019s author ID. This will not necessarily always be the user directly mentioned in the Tweet. (Nullable) in_reply_to_user_id_str : If the represented Tweet is a reply, this field will contain the string representation of the original Tweet\u2019s author ID. This will not necessarily always be the user directly mentioned in the Tweet. (Nullable) in_reply_to_screen_name : If the represented Tweet is a reply, this field will contain the screen name of the original Tweet\u2019s author. (Nullable) user : The user who posted this Tweet. See User data dictionary for complete list of attributes. contributors : The contributors of the Tweet. coordinates : Represents the geographic location of this Tweet as reported by the user or client application. The inner coordinates array is formatted as geoJSON (longitude first, then latitude). (Nullable) place : When present, indicates that the tweet is associated (but not necessarily originating from) a place. (Nullable) is_quote_status : Indicates whether this is a Quoted Tweet. public_metrics : Public metrics for this Tweet containing (impressions_count (=views), quote_count, reply_count, retweet_count, favorite_count (=likes)) quoting_users : Twitter User IDs from users who quoted this Tweet. liking_users : Twitter User IDs from users who liked this Tweet. favorited : Indicates whether this Tweet has been liked by the authenticating user. (Nullable) retweeted : Indicates whether this Tweet has been Retweeted by the authenticating user. retweeted_status : Users can amplify the broadcast of Tweets authored by other users by retweeting . Retweets can be distinguished from typical Tweets by the existence of a retweeted_status attribute. This attribute contains a representation of the original Tweet that was retweeted. Note that retweets of retweets do not show representations of the intermediary retweet, but only the original Tweet. possibly_sensitive : This field indicates content may be recognized as sensitive. The Tweet author can select within their own account preferences and choose \u201cMark media you tweet as having material that may be sensitive\u201d so each Tweet created after has this flag set. This may also be judged and labeled by an internal Twitter support agent. (Nullable) lang : When present, indicates a BCP 47 language identifier corresponding to the machine-detected language of the Tweet text, or und if no language could be detected. See more documentation HERE . sentiment : The sentiment of the Tweet, either positive, neutral, or negative. Polarity scores are returned additionally. The sentiment is detected by using VADER . It is recommended to analyze only english Tweets. In case a Tweet of a different language is analyzed, results will still be returned but might not be accurate.","title":"Literals tweet info"},{"location":"user-guide/overview/literals-user-info/","text":"Detailed Description of the Attributes for the user_info function: id : The integer representation of the unique identifier for this User. id_str : The string representation of the unique identifier for this User. name : The name of the user, as they\u2019ve defined it. screen_name : The screen name, handle, or alias that this user identifies themselves with. followers : IDs, names, and screen names of the user's followers. followees : IDs, names, and screen names of the user's followees. location : The user-defined location for this account\u2019s profile. (Nullable) description : The user-defined UTF-8 string describing their account. (Nullable) url : A URL provided by the user in association with their profile. (Nullable) entities : Entities of the user object. protected : When true, indicates that this user has chosen to protect their Tweets. followers_count : The number of followers this account currently has. friends_count : The number of users this account is following (AKA their \u201cfollowings\u201d or \"followees\"). listed_count : The number of public lists that this user is a member of. created_at : The UTC datetime that the user account was created on Twitter. latest_activity : Latest acitivity according to the users timeline. If the latest activity is a retweet (marked by a leading 'RT' in the text), the text will be truncated to 140 characters. The full text of the original tweet is in the retweeted_status field of the JSON response. Hence, see under the latest_activity -> retweeted_status -> full_text field. last_active : Datetime of the latest activity according to the users timeline. liked_tweets : List of IDs of the liked tweets by the user. composed_tweets : List of IDs of the composed tweet by the user. favourites_count : The number of Tweets this user has liked in the account\u2019s lifetime. British spelling used in the field name for historical reasons. verified : When true, indicates that the user has a verified account. statuses_count : The number of Tweets (including retweets) issued by the user. status : Latest tweet object according to the user's timeline. contributors_enabled : Whether contributors are enabled for this account. profile_image_url_https : A HTTPS-based URL pointing to the user\u2019s profile image. profile_banner_url : The HTTPS-based URL pointing to the standard web representation of the user\u2019s uploaded profile banner. default_profile : When true, indicates that the user has not altered the theme or background of their user profile. default_profile_image : When true, indicates that the user has not uploaded their own profile image and a default image is used instead. withheld_in_countries : When present, indicates a list of uppercase two-letter country codes this content is withheld from. bot_scores : Estimation for bot-like behavior from the Botometer API .","title":"Literals user info"}]}
